<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>transformer-description</title>
      <link href="/2024/07/25/transformer-description/"/>
      <url>/2024/07/25/transformer-description/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="transformer-attention-is-all-you-need">Transformer: Attention IsAll You Need</h1><h2 id="网络结构">网络结构</h2><p>整体架构，Transformer由两部分构成，Encoder和Decoder。都包含了6个<strong><em><span style="color: #ff7700">block</span></em></strong></p><p><img src="https://imgur.la/images/2024/07/25/PPKV7RHB.png" alt="PPKV7RHB" border="0"></p><h2 id="section"></h2><h2 id="第一步获取输入句子的每一个单词的表示向量-xx-由单词的-embeddingembedding就是从原始数据提取出来的feature-和单词位置的-embedding-相加得到">第一步：获取输入句子的每一个单词的表示向量<span class="math inline">\(X\)</span><strong><span style="color: rgb(18, 18, 18)"><span style="background-color: rgb(255, 255, 255)">，</span></span></strong><span class="math inline">\(X\)</span> 由单词的Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的Embedding 相加得到。</h2><p><img src="https://imgur.la/images/2024/07/25/YR9I534X.png" alt="YR9I534X" border="0"></p><center><p>Transformer的输入表示</p></center><ul><li><p>词Embedding方法：<a href="https://arxiv.org/abs/1301.3781">Word2Vec</a> 这种方式在 2018年之前比较主流，但是随着 BERT、GPT2.0的出现，这种方式已经不算效果最好的方法了), <a href="https://aclanthology.org/D14-1162/">Glove</a>等算法预训练得到，也可以在 Transformer 中训练得到。</p></li><li><p>位置Embedding方法：Transformer与RNN不同，Transformer是统观全局将所有单词的位置信息也进行编码作为输入，而RNN则是使用单词的顺序信息，通过使用前一步的输出作为当前步的输入来获取位置信息。</p><p>通常，位置 Embedding 用 <strong>PE</strong><strong>PE</strong> 的维度与单词 Embedding 是一样的。PE可以通过训练得到，也可以使用某种公式计算得到。在 Transformer中采用了后者，计算公式如下：</p></li></ul><p><span class="math display">\[PE_{(pos,2i)}=sin(pos/10000^{2i/d}) \\PE_{(pos,2i+1)}=cos(pos/10000^{2i/d})\]</span></p><p>其中，pos 表示单词在句子中的位置，<span class="math inline">\(d\)</span> 表示 <strong>PE</strong> 的维度 (与词Embedding 一样)，<span class="math inline">\(2i\)</span>表示偶数的维度，<span class="math inline">\(2i+1\)</span> 表示奇数维度(即 <span class="math inline">\(2i≤d, 2i+1≤d\)</span>)。使用这种公式计算<strong>PE</strong> 有以下的好处：</p><ul><li><p>使 PE能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第21 位的 Embedding。</p></li><li><p>可以让模型容易地计算出相对位置，对于固定长度的间距 <span class="math inline">\(k\)</span>，<strong>PE(pos+k)</strong>可以用 <strong>PE(pos)</strong>计算得到。因为 <span class="math inline">\(Sin(A+B) = Sin(A)Cos(B) +Cos(A)Sin(B),\\ Cos(A+B) = Cos(A)Cos(B) -Sin(A)Sin(B)\)</span>。</p></li></ul><p><span style="color: rgb(18, 18, 18)"><span style="background-color: rgb(255, 255, 255)">将单词的词 Embedding 和位置Embedding 相加，就可以得到单词的表示向量 </span></span><strong><span style="color: rgb(18, 18, 18)"><span style="background-color: rgb(255, 255, 255)">x</span></span></strong><span style="color: rgb(18, 18, 18)"><span style="background-color: rgb(255, 255, 255)">，</span></span><strong><span style="color: rgb(18, 18, 18)"><span style="background-color: rgb(255, 255, 255)">x </span></span></strong><span style="color: rgb(18, 18, 18)"><span style="background-color: rgb(255, 255, 255)">就是 Transformer的输入。</span></span></p><h2 id="第二步将得到的单词表示向量矩阵-如上图所示每一行是一个单词的表示-x-传入-encoder-中经过-6-个-encoder-block-后可以得到句子所有单词的编码信息矩阵-c">第二步：将得到的单词表示向量矩阵(如上图所示，每一行是一个单词的表示 x) 传入 Encoder 中，经过 6 个Encoder block 后可以得到句子所有单词的编码信息矩阵 C</h2><p>如下图。单词向量矩阵用 <span class="math inline">\(X_{n×d}\)</span>表示， <span class="math inline">\(n\)</span> 是句子中单词个数，<span class="math inline">\(d\)</span> 是表示向量的维度 (论文中 <span class="math inline">\(d=512\)</span>)。每一个 Encoder block输出的矩阵维度与输入完全一致。</p><p><img src="https://imgur.la/images/2024/07/25/BWNSQFCB.png" alt="BWNSQFCB" border="0" style="zoom:50%;"></p><center><p>Transformer Encoder 编码句子信息</p></center><h3 id="encoder-block">Encoder  block</h3><p>        <img src="https://imgur.la/images/2024/07/25/architecture.png" alt="architecture" border="0" style="zoom:50%;"></p><p>左侧为 <strong>Encoder block</strong>，右侧为 <strong>Decoderblock</strong>。<strong>Multi-HeadAttention</strong>，是由多个 <strong>Self-Attention</strong>组成的，可以看到 <strong>Encoder block</strong> 包含一个<strong>Multi-Head Attention</strong>，而 <strong>Decoder block</strong>包含两个 <strong>Multi-Head Attention</strong> (其中有一个用到Masked)。<strong>Add &amp; Norm</strong> 层，Add 表示残差连接 (ResidualConnection) 用于防止网络退化，<strong>Norm</strong> 表示 LayerNormalization，用于对每一层的激活值进行归一化。</p><h4 id="self-attention-结构"><strong>Self-Attention 结构</strong></h4><p><img src="https://imgur.la/images/2024/07/25/3FSEDRY4.png" alt="3FSEDRY4" border="0" style="zoom:50%;"></p><h5 id="qquerykkeyvvalue"><strong>Q(Query)、K(Key)、V(Value)</strong></h5><p>计算自注意力的第一步是从编码器的每个输入向量（在本例中为每个单词的嵌入）创建三个向量。因此，对于每个单词，我们创建一个<strong>Q</strong>、<strong>K</strong>、<strong>V</strong>。<strong>WQ</strong>、<strong>WK</strong>、<strong>WV</strong>三个矩阵是在训练的时候创建的。</p><p><img src="https://imgur.la/images/2024/07/25/V97S38NR.png" alt="V97S38NR" border="0" style="zoom:50%;"></p><h5 id="self-attention计算输出即计算self-attention的分数"><strong>Self-Attention计算输出(即计算self-Attention的分数）</strong></h5><p>假设我们正在计算这个例子中第一个词“我”的自我注意力。我们需要根据这个单词对输入句子的每个单词进行评分。分数决定了当我们在某个位置对单词进行编码时，对输入句子的其他部分的关注程度。</p><p><span class="math display">\[Attention（Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}}V)\]</span></p><p>公式中计算矩阵 <span class="math inline">\(Q\)</span> 和 <span class="math inline">\(K\)</span>每一行向量的内积，为了防止内积过大，因此除以 <span class="math inline">\(d_k\)</span> 的平方根。<span class="math inline">\(Q\)</span> 乘以 <span class="math inline">\(K\)</span> 的转置后，得到的矩阵行列数都为 <span class="math inline">\(n\)</span>，<span class="math inline">\(n\)</span>为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为 <span class="math inline">\(Q\)</span> 乘以<span class="math inline">\(K^T\)</span>，1234 表示的是句子中的单词。</p><p><img src="https://imgur.la/images/2024/07/25/BY3PA8IB.png" alt="BY3PA8IB" border="0"></p><p>得到 <span class="math inline">\(QK^T\)</span>之后，使用 Softmax计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax是对矩阵的每一行进行 Softmax，即每一行的和都变为 1. softmax分数决定了每个单词在此位置的表达量。显然，这个位置的单词将具有最高的softmax分数，但有时关注与当前单词相关的另一个单词是有用的。</p><p><img src="https://imgur.la/images/2024/07/25/YAAYRBU9.png" alt="YAAYRBU9" border="0"></p><p>得到 Softmax 矩阵之后可以和 <span class="math inline">\(V\)</span>相乘，得到最终的输出 <span class="math inline">\(Z\)</span>.</p><p><img src="https://imgur.la/images/2024/07/25/IEIZUYE5.png" alt="IEIZUYE5" border="0"></p><p>Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention系数，最终单词 1 的输出 <span class="math inline">\(V_i\)</span> 等于所有单词 i 的值 <span class="math inline">\(V_i\)</span> 根据 attention系数的比例加在一起得到，如下图所示：</p><p><img src="https://imgur.la/images/2024/07/25/B6LPEG5N.png" alt="B6LPEG5N" border="0"></p><h4 id="multi-head-attention">Multi-Head Attention</h4><p>Multi-Head Attention 包含多个 Self-Attention 层，首先将输入 <span class="math inline">\(X\)</span> 分别传递到 <span class="math inline">\(h\)</span> 个不同的 Self-Attention 中，计算得到<span class="math inline">\(h\)</span> 个输出矩阵 <span class="math inline">\(Z\)</span>。下图是 <span class="math inline">\(h=8\)</span> 时候的情况，此时会得到 8 个输出矩阵<span class="math inline">\(Z\)</span>.</p><p><img src="https://imgur.la/images/2024/07/25/ILJMUQNZ.png" alt="ILJMUQNZ" border="0" style="zoom:50%;"></p><p>得到 8 个输出矩阵 <span class="math inline">\(Z1\)</span> 到 <span class="math inline">\(Z8\)</span> 之后，Multi-Head Attention将它们拼接在一起 (Concat)，然后传入一个<strong>Linear</strong> 层，得到Multi-Head Attention 最终的输出 <span class="math inline">\(Z\)</span>。Multi-Head Attention 输出的矩阵 <span class="math inline">\(Z\)</span> 与其输入的矩阵 <span class="math inline">\(X\)</span> 的维度是一样的。</p><p><img src="https://imgur.la/images/2024/07/25/T7JWQ9UY.png" alt="T7JWQ9UY" border="0"></p><h5 id="add-norm">Add &amp; Norm</h5><p>Add &amp; Norm 层由 Add 和 Norm 两部分组成,</p><p><span class="math display">\[LayerNorm(X+MultiHeadAttention(X)) \\LayerNorm(X+FeedForward(X))\]</span></p><p>其中 <span class="math inline">\(X\)</span> 表示 Multi-Head Attention或者 Feed Forward 的输入，MultiHeadAttention(<span class="math inline">\(X\)</span>) 和 FeedForward(<span class="math inline">\(X\)</span>) 表示输出 (输出与输入 <span class="math inline">\(X\)</span>维度是一样的，所以可以相加)。<strong>Add</strong> 指 <span class="math inline">\(X\)</span>+MultiHeadAttention(<span class="math inline">\(X\)</span>)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分：</p><p><img src="https://imgur.la/images/2024/07/25/E5CVMKZS.png" alt="E5CVMKZS" border="0"></p><p><strong>Norm</strong> 指 Layer Normalization，通常用于 RNN结构，Layer Normalization会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。</p><h5 id="feed-forward">Feed forward</h5><p><strong>Feed Forward</strong>是一个两层的全连接层，第一层的激活函数为Relu，第二层不使用激活函数，对应的公式如下: <span class="math display">\[FFN(x)=max(0,xW_1+b_1)W_2+b_2\]</span></p><p>通过 <strong>Multi-Head Attention, Add &amp; Norm, Feedforward</strong> 三个部分可以构成一个<strong>Encoder Block,EncoderBlock</strong> 通过接收输入 <span class="math inline">\(X_{(n×d)}\)</span>,并通过 <strong>EncoderBlock</strong> 得到输出 <span class="math inline">\(O_{(n×d)}\)</span>，通过多个 <strong>Encoderblock</strong> 叠加就可以组成 <strong>Encoder</strong>。经过<strong>Encoder</strong> 部分我们可以得到信息编码矩阵 <span class="math inline">\(C\)</span>。</p><h2 id="第三步将-encoder-输出的编码信息矩阵-c传递到-decoder-中decoder-依次会根据当前翻译过的单词-1-i-翻译下一个单词-i1如下图所示在使用的过程中翻译到单词-i1-的时候需要通过-mask-操作遮盖住-i1-之后的单词"><strong><span style="color: rgb(18, 18, 18)"><span style="background-color: rgb(255, 255, 255)">第三步</span></span></strong><span style="color: rgb(18, 18, 18)"><span style="background-color: rgb(255, 255, 255)">：将 Encoder输出的编码信息矩阵 </span></span><strong><span style="color: rgb(18, 18, 18)"><span style="background-color: rgb(255, 255, 255)">C</span></span></strong><span style="color: rgb(18, 18, 18)"><span style="background-color: rgb(255, 255, 255)">传递到 Decoder 中，Decoder依次会根据当前翻译过的单词 1~ i 翻译下一个单词i+1，如下图所示。在使用的过程中，翻译到单词 i+1的时候需要通过 </span></span><strong><span style="color: rgb(18, 18, 18)"><span style="background-color: rgb(255, 255, 255)">Mask</span></span></strong><span style="color: rgb(18, 18, 18)"><span style="background-color: rgb(255, 255, 255)">操作遮盖住 i+1之后的单词。</span></span></h2><h3 id="decoder-block">Decoder  block</h3><p><img src="https://imgur.la/images/2024/07/25/architecture.png" alt="architecture" border="0" style="zoom:50%;"></p><center><p>Decoder block 包含两个 Multi-Head Attention 层。</p></center><p>第一个 Multi-Head Attention 层采用了 Masked操作。(在Transformer模型中，使用Masked可以避免在预测当前位置是使用到后续序列中的信息。在预测当前位置时使用未来位置的信息会导致信息泄露和模型过拟合的原因是因为这会造成未来信息的“泄漏”。在自注意力机制中，每个位置都可以与其他位置进行关联，因此如果模型在预测当前位置时使用了未来位置的信息，那么模型实际上是“知道”了未来的信息，这就相当于信息泄漏。信息泄漏会导致模型在训练时过度依赖未来位置的信息，从而导致模型过拟合训练数据，失去了对真实数据的泛化能力。过拟合的模型在未见过的数据上表现不佳，因为它们过度依赖于训练数据中的特定模式和关联，而无法很好地推广到新的数据。因此，在Transformer中，为了避免信息泄漏和过拟合，需要使用masked来确保模型只能使用当前位置之前的信息，这样可以更好地捕捉序列中的依赖关系和上下文信息，同时保持模型的泛化能力)</p><h4 id="第一个multi-head-attentionmasked">第一个Multi-HeadAttention（Masked）：</h4><p>Decoder block 的第一个 Multi-Head Attention 采用了 Masked操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1个单词之后的信息。以 "我有一只猫" 翻译成 "I have a cat" 为例，在 Decoder的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入"<Begin>" 预测出第一个单词为 "I"，然后根据输入 "<Begin> I"预测下一个单词 "have"。</Begin></Begin></p><p><img src="https://imgur.la/images/2024/07/25/3C7JYEUH.png" alt="3C7JYEUH" border="0"></p><p>Decoder 可以在训练的过程中使用 Teacher Forcing并且并行化训练，即将正确的单词序列 (&lt;Begin&gt; I have a cat)和对应输出 (I have a cat &lt;end&gt;) 传递到 Decoder。那么在预测第 i个输出时，就要将第 i+1 之后的单词掩盖住，注意 Mask 操作是在Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示"&lt;Begin&gt; I have a cat &lt;end&gt;"。</p><p><strong>第一步：</strong>是 Decoder 的输入矩阵和<strong>Mask</strong> 矩阵，输入矩阵包含 "<Begin> I have a cat" (0, 1,2, 3, 4) 五个单词的表示向量，<strong>Mask</strong> 是一个 5×5 的矩阵。在<strong>Mask</strong> 可以发现单词 0 只能使用单词 0 的信息，而单词 1可以使用单词 0, 1 的信息，即只能使用之前的信息。</Begin></p><p><img src="https://imgur.la/images/2024/07/25/IYX8T5F3.png" alt="IYX8T5F3" border="0"></p><center><p>输入矩阵与 Mask 矩阵</p></center><p><strong>第二步：</strong>接下来的操作和之前的 Self-Attention一样，通过输入矩阵 <span class="math inline">\(X\)</span> 计算得到 <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, <span class="math inline">\(V\)</span>矩阵。然后计算 <span class="math inline">\(Q\)</span> 和 <span class="math inline">\(K^T\)</span> 的乘积 <span class="math inline">\(QK^T\)</span> 。</p><p><img src="https://imgur.la/images/2024/07/25/83YBXRLY.png" alt="83YBXRLY" border="0"></p><center><p>Q乘以K的转置</p></center><p><strong>第三步：</strong>在得到 <span class="math inline">\(QK^T\)</span> 之后需要进行 Softmax，计算 attentionscore，我们在 Softmax之前需要使用<strong>Mask</strong>矩阵遮挡住每一个单词之后的信息，遮挡操作如下：</p><p><img src="https://imgur.la/images/2024/07/25/YU3MAKSS.png" alt="YU3MAKSS" border="0"></p><center><p>Softmax 之前 Mask</p></center><p>得到 <strong>Mask</strong> <span class="math inline">\(QK^T\)</span>之后在 <strong>Mask</strong> <span class="math inline">\(QK^T\)</span>上进行 Softmax，每一行的和都为1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p><p><strong>第四步：</strong>使用 <strong>Mask</strong> <span class="math inline">\(QK^T\)</span> 与矩阵 <span class="math inline">\(V\)</span> 相乘，得到输出 <span class="math inline">\(Z\)</span>，则单词 1 的输出向量 <span class="math inline">\(Z_1\)</span> 是只包含单词 1 信息的。</p><p><img src="https://imgur.la/images/2024/07/25/2JJ24Y9A.png" alt="2JJ24Y9A" border="0"></p><center><p>Mask 之后的输出</p></center><p><strong>第五步：</strong>通过上述步骤就可以得到一个 MaskSelf-Attention 的输出矩阵 <strong>Zi</strong>，然后和 Encoder 类似，通过Multi-Head Attention 拼接多个输出<strong>Zi</strong> 然后计算得到第一个Multi-Head Attention的输出<strong>Z</strong>，<strong>Z</strong>与输入<strong>X</strong>维度一样。</p><h4 id="第二个-multi-head-attention">第二个 Multi-Head Attention</h4><p>第二个 Multi-Head Attention 层的<strong>K, V</strong>矩阵使用 Encoder的<strong>编码信息矩阵C</strong>进行计算，而<strong>Q</strong>使用上一个Decoder block 的输出计算。根据 Encoder 的输出 <strong>C</strong>计算得到<strong>K, V</strong>，根据上一个 Decoder block 的输出<strong>Z</strong> 计算 <strong>Q</strong> (如果是第一个 Decoder block则使用输入矩阵 <strong>X</strong>进行计算)，后续的计算方法与之前描述的一致。这样做的好处是在 Decoder的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需<strong>Mask</strong>)。</p><p><img src="https://imgur.la/images/2024/07/25/8ARNRACC.png" alt="8ARNRACC" border="0"></p><h4 id="最后的-softmax-层">最后的 Softmax 层</h4><p>计算下一个翻译单词的概率。</p><p>Decoder block 最后的部分是利用 Softmax预测下一个单词，在之前的网络层我们得到一个最终的输出<strong>Z</strong>，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0的信息，如下：</p><p><img src="https://imgur.la/images/2024/07/25/H7ZPFVIX.png" alt="H7ZPFVIX" border="0"></p><hr><blockquote><p>Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J].Advances in neural information processing systems, 2017, 30.</p><p>知乎：Transformer模型详解 https://zhuanlan.zhihu.com/p/338817680</p><p>李宏毅：Transformerhttps://youtu.be/ugWDIIOHtPA?si=S9Iut52lR-5wD-kp</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Sequence Model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>symbols</title>
      <link href="/2024/06/20/symbols/"/>
      <url>/2024/06/20/symbols/</url>
      
        <content type="html"><![CDATA[<h1 id="greek-alphabet-希腊字母">Greek Alphabet (希腊字母)</h1><table><colgroup><col style="width: 30%"><col style="width: 29%"><col style="width: 20%"><col style="width: 20%"></colgroup><thead><tr class="header"><th style="text-align: center;">Letter</th><th style="text-align: center;">LaTex</th><th style="text-align: center;">Letter</th><th style="text-align: center;">LaTex</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\alpha\)</span></td><td style="text-align: center;">\alpha</td><td style="text-align: center;"><span class="math inline">\(A\)</span></td><td style="text-align: center;">\Alpha</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\beta\)</span></td><td style="text-align: center;">\beta</td><td style="text-align: center;"><span class="math inline">\(B\)</span></td><td style="text-align: center;">\Beta</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\gamma\)</span></td><td style="text-align: center;">\gamma</td><td style="text-align: center;"><span class="math inline">\(\Gamma\)</span></td><td style="text-align: center;">\Gamma</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\delta\)</span></td><td style="text-align: center;">\delta</td><td style="text-align: center;"><span class="math inline">\(\Delta\)</span></td><td style="text-align: center;">\Delta</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\epsilon\)</span> <span class="math inline">\(\varepsilon\)</span></td><td style="text-align: center;">\epsilon / \varepilon</td><td style="text-align: center;"><span class="math inline">\(E\)</span></td><td style="text-align: center;">\Epsilon</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\eta\)</span></td><td style="text-align: center;">\eta</td><td style="text-align: center;"><span class="math inline">\(H\)</span></td><td style="text-align: center;">\Eta</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\zeta\)</span></td><td style="text-align: center;">\zeta</td><td style="text-align: center;"><span class="math inline">\(Z\)</span></td><td style="text-align: center;">\Zeta</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\theta\)</span> <span class="math inline">\(\vartheta\)</span></td><td style="text-align: center;">\theta</td><td style="text-align: center;"><span class="math inline">\(\Theta\)</span></td><td style="text-align: center;">\Theta</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\iota\)</span></td><td style="text-align: center;">\iota</td><td style="text-align: center;"><span class="math inline">\(I\)</span></td><td style="text-align: center;">\Iota</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\kappa\)</span></td><td style="text-align: center;">\kappa</td><td style="text-align: center;"><span class="math inline">\(K\)</span></td><td style="text-align: center;">\Kappa</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\lambda\)</span></td><td style="text-align: center;">\lambda</td><td style="text-align: center;"><span class="math inline">\(\Lambda\)</span></td><td style="text-align: center;">\Lambda</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\mu\)</span></td><td style="text-align: center;">\mu</td><td style="text-align: center;"><span class="math inline">\(M\)</span></td><td style="text-align: center;">\Mu</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\nu\)</span></td><td style="text-align: center;">\nu</td><td style="text-align: center;"><span class="math inline">\(N\)</span></td><td style="text-align: center;">\Nu</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\omicron\)</span></td><td style="text-align: center;">\omicron</td><td style="text-align: center;"><span class="math inline">\(O\)</span></td><td style="text-align: center;">\Omicron</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\xi\)</span></td><td style="text-align: center;">\xi</td><td style="text-align: center;"><span class="math inline">\(\Xi\)</span></td><td style="text-align: center;">\Xi</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\pi\)</span></td><td style="text-align: center;">\pi</td><td style="text-align: center;"><span class="math inline">\(\Pi\)</span></td><td style="text-align: center;">\Pi</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\rho\)</span> <span class="math inline">\(\varrho\)</span></td><td style="text-align: center;">\rho</td><td style="text-align: center;"><span class="math inline">\(P\)</span></td><td style="text-align: center;">\Rho</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\sigma\)</span></td><td style="text-align: center;">\sigma</td><td style="text-align: center;"><span class="math inline">\(\Sigma\)</span></td><td style="text-align: center;">\Sigma</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\tau\)</span></td><td style="text-align: center;">\tau</td><td style="text-align: center;"><span class="math inline">\(T\)</span></td><td style="text-align: center;">\Tau</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\upsilon\)</span></td><td style="text-align: center;">\upsilon</td><td style="text-align: center;"><span class="math inline">\(\Upsilon\)</span></td><td style="text-align: center;">\Upsilon</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\phi\)</span> <span class="math inline">\(\varphi\)</span></td><td style="text-align: center;">\phi / \varphi</td><td style="text-align: center;"><span class="math inline">\(\Phi\)</span> <span class="math inline">\(\varPhi\)</span></td><td style="text-align: center;">\Phi / \varPhi</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\chi\)</span></td><td style="text-align: center;">\chi</td><td style="text-align: center;"><span class="math inline">\(X\)</span></td><td style="text-align: center;">\Chi</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\psi\)</span></td><td style="text-align: center;">\psi</td><td style="text-align: center;"><span class="math inline">\(\Psi\)</span></td><td style="text-align: center;">\Psi</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\omega\)</span></td><td style="text-align: center;">\omega</td><td style="text-align: center;"><span class="math inline">\(\Omega\)</span></td><td style="text-align: center;">\Omega</td></tr></tbody></table><h2 id="symbolic-meaning">Symbolic Meaning</h2><h3 id="set">Set</h3><table><thead><tr class="header"><th style="text-align: center;">symbol</th><th style="text-align: left;">meaning</th><th style="text-align: left;">LaTex</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\mathbb{R}\)</span></td><td style="text-align: left;"><strong><em>real numbers set</em></strong>实数集</td><td style="text-align: left;">\mathbb{R}</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\mathbb{Q}\)</span></td><td style="text-align: left;"><strong><em>rational numbersset</em></strong> 有理数集合</td><td style="text-align: left;">\mathbb{Q}</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\mathbb{Z}\)</span></td><td style="text-align: left;"><strong><em>integers set</em></strong>整数集合</td><td style="text-align: left;">\mathbb{Z}</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\mathbb{N}\)</span></td><td style="text-align: left;"><strong><em>natural numbers</em></strong>自然数集合</td><td style="text-align: left;">\mathbb{N}</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\mathbb{C}\)</span></td><td style="text-align: left;"><strong><em>plural set</em></strong>虚数集合</td><td style="text-align: left;">\mathbb{C}</td></tr></tbody></table><h3 id="variables">Variables</h3><table><thead><tr class="header"><th style="text-align: center;">symbol</th><th>meaning</th><th>LaTex</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"></td><td></td><td></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\top\)</span></td><td>矩阵的转置</td><td>\top</td></tr><tr class="odd"><td style="text-align: center;"></td><td></td><td></td></tr><tr class="even"><td style="text-align: center;"></td><td></td><td></td></tr><tr class="odd"><td style="text-align: center;"></td><td></td><td></td></tr><tr class="even"><td style="text-align: center;"></td><td></td><td></td></tr><tr class="odd"><td style="text-align: center;"></td><td></td><td></td></tr><tr class="even"><td style="text-align: center;"></td><td></td><td></td></tr><tr class="odd"><td style="text-align: center;"></td><td></td><td></td></tr></tbody></table><h3 id="mathematical">Mathematical</h3><table><colgroup><col style="width: 14%"><col style="width: 12%"><col style="width: 12%"><col style="width: 10%"><col style="width: 16%"><col style="width: 14%"><col style="width: 12%"><col style="width: 10%"></colgroup><thead><tr class="header"><th style="text-align: center;">Symbol</th><th style="text-align: center;">LaTex</th><th style="text-align: center;">Symbol</th><th style="text-align: center;">LaTex</th><th style="text-align: center;">Symbol</th><th style="text-align: center;">LaTex</th><th style="text-align: center;">Symbol</th><th style="text-align: center;">LaTex</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\pm\)</span></td><td style="text-align: center;">\pm</td><td style="text-align: center;"><span class="math inline">\(\mp\)</span></td><td style="text-align: center;">\mp</td><td style="text-align: center;"><span class="math inline">\(\times\)</span></td><td style="text-align: center;">\times</td><td style="text-align: center;"><span class="math inline">\(\div\)</span></td><td style="text-align: center;">\div</td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\cdot\)</span></td><td style="text-align: center;">\cdot</td><td style="text-align: center;"><span class="math inline">\(\ast\)</span></td><td style="text-align: center;">\ast</td><td style="text-align: center;"><span class="math inline">\(\star\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr></tbody></table><h2 id="abbreviation">Abbreviation</h2><table style="width:100%;"><colgroup><col style="width: 15%"><col style="width: 50%"><col style="width: 33%"></colgroup><thead><tr class="header"><th>abbreviation</th><th>full name</th><th>CN</th></tr></thead><tbody><tr class="odd"><td>w.r.t</td><td>with respect to</td><td>关于、谈到、涉及</td></tr><tr class="even"><td>i.i.d</td><td>independent and identically distributed</td><td>独立同分布</td></tr><tr class="odd"><td>PDF</td><td>Probability Density Function</td><td>概率密度函数</td></tr><tr class="even"><td>w/；w/o</td><td>with；without</td><td>消融实验表中的对比实验设置</td></tr><tr class="odd"><td>subject to</td><td></td><td>限制条件（优化中常用）</td></tr></tbody></table>]]></content>
      
      
      
        <tags>
            
            <tag> LaTex </tag>
            
            <tag> MarkDown </tag>
            
            <tag> Deep Learning Knowledge </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mamba</title>
      <link href="/2024/06/19/Mamba/"/>
      <url>/2024/06/19/Mamba/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="mamba">Mamba</h1><!--本文是根据Mamba论文原作者撰写的Blog进行阅读的重点解读--><p>The Transformer architecture has been a major component in thesuccess of Large Language Models (LLMs). It has been used for nearly allLLMs that are being used today, from open-source models like Mistral toclosed-source models like ChatGPT.</p><p>To further improve LLMs, new architectures are developed that mighteven outperform the Transformer architecture. One of these methods is<em>Mamba</em>, a <em>State Space Model</em>.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6bc1ca-a387-47ed-a9a6-077af838b359_1148x892.png" style="zoom:50%;"></p><p>Mamba was proposed in the paper <a href="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time SequenceModeling with Selective State Spaces</a>.<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-1-141228095">1</a>You can find its official implementation and model checkpoints in its <a href="https://github.com/state-spaces/mamba">repository</a>.</p><p>In this post, I will introduce the field of State Space Models in thecontext of language modeling and explore concepts one by one to developan intuition about the field. Then, we will cover how Mamba mightchallenge the Transformers architecture.</p><p>As a visual guide, expect many visualizations to develop an intuitionabout Mamba and State Space Models!</p><h2 id="part-1-the-problem-with-transformers">Part 1: The Problem withTransformers</h2><p>To illustrate why Mamba is such an interesting architecture, let’s doa short re-cap of transformers first and explore one of itsdisadvantages.</p><p>A Transformer sees any textual input as a <em>sequence</em> thatconsists of <em>tokens</em>.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b8c299a-c0c0-46fe-86cf-b22e08a91b32_1776x544.png" style="zoom:50%;"></p><p>A major benefit of Transformers is that whatever input it receives,it can look back at any of the earlier tokens in the sequence to deriveits representation.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c01c75-1105-4aeb-a608-f00c85bbe5f7_1776x532.png" style="zoom:50%;"></p><h3 id="the-core-components-of-transformers">The Core Components ofTransformers</h3><p>Remember that a Transformer consists of two structures, a set ofencoder blocks for representing text and a set of decoder blocks forgenerating text. Together, these structures can be used for severaltasks, including translation.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62a21d60-2e84-4c19-a6fb-d2eff501af1c_1776x952.png" style="zoom:50%;"></p><p>We can adopt this structure to create generative models by using onlydecoders. This Transformer-based model, <em>Generative Pre-trainedTransformers</em> (GPT), uses decoder blocks to complete some inputtext.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58e51959-d4ef-4fa9-a1c7-dab0e5ca4dc0_1776x1012.png" style="zoom: 50%;"></p><p>Let’s take a look at how that works!</p><h3 id="a-blessing-with-training">A Blessing with Training…</h3><p>A single decoder block consists of two main components, maskedself-attention followed by a feed-forward neural network.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5af9c5-5266-4c2b-b583-b20a19f19fcc_1776x464.png" style="zoom:50%;"></p><p>Self-attention is a major reason why these models work so well. Itenables an uncompressed view of the entire sequence with fasttraining.</p><p>So how does it work?</p><p>It creates a matrix comparing each token with every token that camebefore. The weights in the matrix are determined by how relevant thetoken pairs are to one another.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F167cfe80-2863-47c8-a969-cb2eeedbd353_1776x860.png" style="zoom:50%;"></p><p>During training, this matrix is created in one go. The attentionbetween “<em>My</em>” and “<em>name</em>” does not need to be calculatedfirst before we calculate the attention between “<em>name</em>” and“<em>is</em>”.</p><p>It enables <strong>parallelization</strong>, which speeds up trainingtremendously!</p><h3 id="and-the-curse-with-inference">And the Curse with Inference!</h3><p>There is a flaw, however. When generating the next token, we need tore-calculate the attention for the <em>entire sequence</em>, even if wealready generated some tokens.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb66f1965-fc44-4a61-b9c6-912c8120ecad_2420x580.png" style="zoom:50%;"></p><p>Generating tokens for a sequence of length <em>L</em> needs roughly<em>L²</em> computations which can be costly if the sequence lengthincreases.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405074ed-aa8c-44b4-88a5-bae1dad0412e_2072x392.png" style="zoom:50%;"></p><p>This need to recalculate the entire sequence is a major bottleneck ofthe Transformer architecture.</p><p>Let’s look at how a “classic” technique, Recurrent Neural Networks,solves this problem of slow inference.</p><hr><blockquote><p><strong>为什么Transformer的推理速度会很慢？</strong></p><p>因为Transformer在推理当前的状态时，需要计算包含之前所有状态的信息，也就是说，当计算到第N个状态时，需要同时计算包含前N个状态的信息以得出当前状态的取值，这也就时为什么以Transformer为基础的模型在输入时会有长度限制的前提条件。而相比较于Transformer，RNN模型有着比较快的推理速度因为它只用重点关注前面重点的状态，根据此来推理出当前状态，故而其计算速度快，但由于只关注之前的部分状态，也导致了其推理的准确度相比较于Transformer略有不及。</p><p>例如：当输入的Batch Size=b, Sequence length=N,那么一个具有 <span class="math inline">\(l\)</span> 层的Transformer模型的计算量为 <span class="math inline">\(l*(24bNd^2+4bN^2d)\)</span>, <span class="math inline">\(d\)</span>为词向量的维度或者是隐藏层的维度，详细的计算过程见<a href="https://blog.csdn.net/v_JULY_v/article/details/133619540">原文</a>。</p></blockquote><h3 id="are-rnns-a-solution">Are RNNs a Solution?</h3><p>Recurrent Neural Networks (RNN) is a sequence-based network. It takestwo inputs at each time step in a sequence, namely the input at timestep <strong><em>t</em></strong> and a hidden state of the previous timestep <strong><em>t-1</em></strong>, to generate the next hidden stateand predict the output.</p><p>RNNs have a looping mechanism that allows them to pass informationfrom a previous step to the next. We can “unfold” this visualization tomake it more explicit.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddc71706-8da8-4c28-921b-675e9164c7ab_2404x872.png" style="zoom:50%;"></p><p>When generating the output, the RNN only needs to consider theprevious hidden state and current input. It prevents recalculating allprevious hidden states which is what a Transformer would do.</p><p>In other words, RNNs can do inference fast as it scales linearly withthe sequence length! In theory, it can even have an <em>infinite contextlength</em>.</p><p>To illustrate, let’s apply the RNN to the input text we have usedbefore.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2484541-f7b1-4950-b04f-5a3177596fbb_2228x808.png" style="zoom:50%;"></p><p>Each hidden state is the aggregation of all previous hidden statesand is typically a compressed view.</p><p>There is a problem, however…</p><p>Notice that the last hidden state, when producing the name“<em>Maarten</em>” does not contain information about the word“<em>Hello</em>” anymore. RNNs tend to forget information over timesince they only consider one previous state.</p><p>This sequential nature of RNNs creates another problem. Trainingcannot be done in parallel since it needs to go through each step at atime sequentially.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819fcf9b-ea31-4954-8496-4b66c5b46dc2_2236x828.png" style="zoom:50%;"></p><p>The problem with RNNs, compared to Transformers, is completely theopposite! Its inference is incredibly fast but it is notparallelizable.</p><blockquote><p><strong>RNN存在的问题</strong></p><ol type="1"><li><p>虽然在RNN中，每个状态都是先前所有隐藏状态的聚合，然而随着时间的推移，RNN会忘记掉一部分信息。</p></li><li><p>RNN没办法并行训练，即推理速度快但训练慢，而且因为RNN的结构，也无法进行卷积运算。</p></li></ol><p>由于RNN的每个时间 <span class="math inline">\(t\)</span>的输出需要依赖前一个时间 <span class="math inline">\(t-1\)</span> 的输出，导致其循环操作无法并行训练。</p><p>RNN中通过循环结构来实现权重共享，而CNN中通过卷积操作实现局部链接和全局共享。</p></blockquote><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7af6befe-e99d-4350-b555-21ce543cae53_2072x580.png" style="zoom:50%;"></p><p>Can we somehow find an architecture that does parallelize traininglike Transformers whilst still performing inference that scales linearlywith sequence length?</p><p>Yes! This is what Mamba offers but before diving into itsarchitecture, let’s explore the world of State Space Models first.</p><h2 id="part-2-the-state-space-model-ssm">Part 2: The <strong>StateSpace Model (SSM)</strong></h2><p>A State Space Model (SSM), like the Transformer and RNN, processessequences of information, like text but also signals. In this section,we will go through the basics of SSMs and how they relate to textualdata.</p><h3 id="what-is-a-state-space">What is a State Space?</h3><p>A State Space contains the minimum number of variables that fullydescribe a system. It is a way to mathematically represent a problem bydefining a system's possible states.</p><p>Let’s simplify this a bit. Imagine we are navigating through a maze.The “<em>state space</em>” is the map of all possible locations(states). Each point represents a unique position in the maze withspecific details, like how far you are from the exit.</p><p>The “<em>state space representation</em>” is a simplified descriptionof this map. It shows where you are (current state), where you can gonext (possible future states), and what changes take you to the nextstate (going right or left).</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6480800-2449-456a-87a7-27c8a4e9e718_2520x1388.png" style="zoom:50%;"></p><p>Although State Space Models use equations and matrices to track thisbehavior, it is simply a way to track where you are, where you can go,and how you can get there.</p><p>The variables that describe a state, in our example the X and Ycoordinates, as well as the distance to the exit, can be represented as“<em>state vectors</em>”.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c79eba-2559-4d9d-8999-bee33666f2e3_2364x736.png" style="zoom:50%;"></p><p>Sounds familiar? That is because embeddings or vectors in languagemodels are also frequently used to describe the “state” of an inputsequence. For instance, a vector of your current position (state vector)could look a bit like this:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ff8812a-64d2-4fc6-8e54-eb86222333b0_1496x444.png" style="zoom:50%;"></p><p>In terms of neural networks, the “state” of a system is typically itshidden state and in the context of Large Language Models, one of themost important aspects of generating a new token.</p><p>What is a State Space Model?</p><p>SSMs are models used to describe these state representations and makepredictions of what their next state could be depending on someinput.</p><p>Traditionally, at time <strong><em>t</em></strong>, SSMs:</p><ul><li><p>map an input sequence <strong><em>x(t)</em></strong> — (e.g.,moved left and down in the maze)</p></li><li><p>to a latent state representation <strong><em>h(t)</em></strong> —(e.g., distance to exit and x/y coordinates)</p></li><li><p>and derive a predicted output sequence<strong><em>y(t)</em></strong> — (e.g., move left again to reach theexit sooner)</p><blockquote><p><strong>A:</strong> 当前状态如何影响下一状态</p><p><strong>B:</strong> 输入序列 <strong>x</strong>如何影响当前的状态</p><p><strong>C:</strong> 当前的状态如何影响当前的输出</p><p><strong>D:</strong> 输入序列 <strong>x</strong>如何影响当前的输出</p></blockquote></li></ul><p>However, instead of using <em>discrete</em> <em>sequences</em> (likemoving left once) it takes as input a <em>continuous</em><em>sequence</em> and predicts the output sequence.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5788c3e4-8794-4492-af87-3a45f7a6aa70_1992x624.png" style="zoom:50%;"></p><p>SSMs assume that dynamic systems, such as an object moving in 3Dspace, can be predicted from its state at time<strong><em>t</em></strong> through two equations.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32401c6d-39b6-4619-a75e-6b33d3268bca_2520x388.png" style="zoom:50%;"></p><blockquote><p>这里的 <span class="math inline">\(h&#39;(t)\)</span>是SSM一阶微分方程中的导数，指的是连续状态下的当前状态是由上一状态得出的。</p><p>写成离散形态的即为 <span class="math inline">\(h_t=Ah_{t-1}+Bx_t\)</span>.</p><p>通过求解这两个方程，可以根据观察到的数据“输入序列 x 和先前的状态 h(t)，即可实现对未来状态 y(t) 进行预测。</p><p><img src="https://imgur.la/images/2024/06/21/Mamba-Con_dis.png" alt="Mamba Con dis" border="0" style="zoom:50%;"></p></blockquote><p>By solving these equations, we assume that we can uncover thestatistical principles to predict the state of a system based onobserved data (input sequence and previous state).</p><p>==Its goal is to find this state representation<strong><em>h(t)</em></strong> such that we can go from an input to anoutput sequence.==</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5c7ae-3dbe-44d8-8b13-7f4dcc14a29b_2008x624.png" style="zoom:50%;"></p><p>These two equations are the core of the State Space Model.</p><p>The two equations will be referenced throughout this guide. To makethem a bit more intuitive, they are <strong>color-coded</strong> so youcan quickly reference them.</p><p>The <strong>state equation</strong> describes how the state changes(through <em>matrix A</em>) based on how the input influences the state(through <em>matrix B</em>).</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0876819d-8a46-4187-9826-14391bfd47b9_1796x624.png" style="zoom:50%;"></p><p>As we saw before, <strong><em>h(t)</em></strong> refers to our latentstate representation at any given time <strong><em>t</em></strong>, and<strong><em>x(t)</em></strong> refers to some input.</p><p>The <strong>output equation</strong> describes how the state istranslated to the output (through <em>matrix C</em>) and how the inputinfluences the output (through <em>matrix D</em>).</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2e87708-9676-4a1c-b32c-d703026f64d9_1796x624.png" style="zoom:50%;"></p><blockquote><p><strong>NOTE</strong>: Matrices <em>A</em>, <em>B</em>, <em>C</em>,and <em>D</em> are also commonly refered to as <em>parameters</em> sincethey are learnable.</p></blockquote><p>Visualizing these two equations gives us the followingarchitecture:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc358439e-c507-49f1-ac2e-5dedaccc2a8b_1728x364.png" style="zoom:50%;"></p><p>Let’s go through the general technique step-by-step to understand howthese matrices influence the learning process.</p><p>Assume we have some input signal <strong><em>x(t)</em></strong>, thissignal first gets multiplied by <em>matrix B</em> which describes howthe inputs influence the system.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd6f8dae-2281-47af-8ba3-06bbdc594d1c_1956x360.png" style="zoom: 67%;"></p><p>The updated state (akin to the hidden state of a neural network) is alatent space that contains the core “knowledge” of the environment. Wemultiply the state with <em>matrix A</em> which describes how all theinternal states are connected as they represent the underlying dynamicsof the system.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cedc98a-d200-4fe4-b311-6d68dcaa50af_1956x572.png" style="zoom:50%;"></p><blockquote><p><strong>A</strong>矩阵决定了整个状态的演化，即如何从一个状态演化到下一个状态。其本质为存储着之前所有历史信息的浓缩精华。</p></blockquote><p>As you might have noticed, <em>matrix A</em> is applied beforecreating the state representations and is updated after the staterepresentation has been updated.</p><p>Then, we use <em>matrix C</em> to describe how the state can betranslated to an output.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8599487f-1023-4069-be7a-8056e63b0574_1956x572.png" style="zoom:50%;"></p><p>Finally, we can make use of <em>matrix D</em> to provide a directsignal from the input to the output. This is also often referred to as a<em>skip-connection</em>.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf79721f-5cef-44da-98c5-f63a7839ebc3_1956x756.png" style="zoom:50%;"></p><p>Since <em>matrix D</em> is similar to a skip-connection, the SSM isoften regarded as the following without the skip-connection.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca1d511-7d31-42a0-9220-2fb85b256efd_1956x864.png" style="zoom:50%;"></p><p>Going back to our simplified perspective, we can now focus onmatrices <em>A</em>, <em>B</em>, and <em>C</em> as the core of theSSM.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e52f4f0-d7ad-453d-a741-6dfa4a998964_1728x352.png" style="zoom:50%;"></p><p>We can update the original equations (and add some pretty colors) tosignify the purpose of each matrix as we did before.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55df8ede-3a16-4473-8ea9-872fe199d3a1_1904x676.png" style="zoom:50%;"></p><p>Together, these two equations aim to predict the state of a systemfrom observed data. Since the input is expected to be continuous, themain representation of the SSM is a <strong>continuous-timerepresentation</strong>.</p><h3 id="from-ssm-to-s4">From SSM to S4</h3><h4 id="from-a-continuous-to-a-discrete-signal">From a Continuous to aDiscrete Signal</h4><p>Finding the state representation <strong><em>h(t)</em></strong> isanalytically challenging if you have a continuous signal. Moreover,since we generally have a discrete input (like a textual sequence), wewant to discretize the model.</p><p>To do so, we make use of the <em>Zero-order hold technique.</em> Itworks as follows.</p><ol type="1"><li>First, every time we receive a discrete signal, we hold its valueuntil we receive a new discrete signal. This process creates acontinuous signal the SSM can use:</li></ol><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a2ffb18-2e66-4135-9888-98e9ab88d0d8_1488x472.png" style="zoom:50%;"></p><ol start="2" type="1"><li><p>How long we hold the value is represented by a new learnableparameter, called the <em>step size</em> <span class="math inline">\(\Delta\)</span> . It represents the resolution ofthe input.</p></li><li><p>Now that we have a continuous signal for our input, we cangenerate a continuous output and only sample the values according to thetime steps of the input.</p></li></ol><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F042ff699-81af-4479-b99f-92e4997c4c81_1488x476.png" style="zoom:50%;"></p><p>These sampled values are our discretized output!</p><p>Mathematically, we can apply the <a href="https://en.wikipedia.org/wiki/Zero-order_hold">Zero-order hold</a>as follows:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6df4b59-6f76-4f13-a201-7b69e59df164_6200x1176.png" style="zoom:50%;"></p><p>Together, they allow us to go from a continuous SSM to a discrete SSMrepresented by a formulation that instead of a<em>function-to-function</em>, <span class="math inline">\(x(t)\rightarrow y(t)\)</span>, is now a<em>sequence-to-sequence</em>, <span class="math inline">\(x_k\rightarrow y_k\)</span>:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc29cfbbb-ae41-4dc2-b899-9e0a81cba34d_1980x1012.png" style="zoom:50%;"></p><p>Here, matrices <strong><em>A</em></strong> and<strong><em>B</em></strong> now represent discretized parameters of themodel.</p><p>We use <strong><em>k</em></strong> instead of<strong><em>t</em></strong> to represent discretized timesteps and tomake it a bit more clear when we refer to a continuous versus a discreteSSM.</p><blockquote><p><strong>NOTE:</strong> We are still saving the continuous form of<em>Matrix A</em> and not the discretized version during training.During training, the continuous representation is discretized.</p></blockquote><p>Now that we have a formulation of a discrete representation, let’sexplore how we can actually <em>compute</em> the model.</p><h4 id="the-recurrent-representation">The Recurrent Representation</h4><p>Our discretized SSM allows us to formulate the problem in specifictimesteps instead of continuous signals. A recurrent approach, as we sawbefore with RNNs is quite useful here.</p><p>If we consider discrete timesteps instead of a continuous signal, wecan reformulate the problem with timesteps:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b70ba4-b068-44d1-8641-9b224d103c51_1980x548.png" style="zoom:50%;"></p><p>At each timestep, we calculate how the current input (<strong><span class="math inline">\(Bx_k\)</span></strong>) influences the previousstate (<strong><span class="math inline">\(Ah_{k-1}\)</span></strong>)and then calculate the predicted output (<strong><span class="math inline">\(Ch_k\)</span></strong>).</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb4d0412-87fb-4507-bedb-4793588bd465_2116x788.png" style="zoom:50%;"></p><p>This representation might already seem a bit familiar! We canapproach it the same way we did with the RNN as we saw before.</p><blockquote><p>对于 <span class="math inline">\(y_2\)</span> 展开计算则有 <span class="math display">\[\begin{align}y_2&amp;=Ch_2 \\&amp;=C(\bar{A}h_1+\bar{B}x_2) \\&amp;=C(\bar{A}(\bar{A}h_0+Bx_1)+\bar{B}x_2) \\&amp;=C(\bar{A}(\bar{A} \cdot \bar{B}x_0 +Bx_1)+\bar{B}x_2) \\&amp;=C\cdot \bar{A}^2\cdot \bar{B}x_0+C\cdot \bar{A}\cdot\bar{B}x_1+C\cdot \bar{B}x_2\end{align}\]</span> 由此可推得 <span class="math inline">\(y_k=C\cdot \bar{A}^k\cdot \bar{B}x_0+C\cdot \bar{A}^{k-1} \cdot \bar{B}x_1+\cdots+C\cdot\bar{A} \cdot \bar{B}x_k\)</span></p></blockquote><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91ca51f7-9b9b-4f17-bccb-32a5a96f3339_2184x868.png" style="zoom:50%;"></p><p>Which we can unfold (or unroll) as such:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1084e8a-a70d-450b-beb0-f18117ade5ed_2184x876.png" style="zoom:50%;"></p><p>Notice how we can use this discretized version using the underlyingmethodology of an RNN.</p><p>This technique gives us both the advantages and disadvantages of anRNN, namely fast inference and slow training.</p><h4 id="the-convolution-representation">The ConvolutionRepresentation</h4><p>Another representation that we can use for SSMs is that ofconvolutions. Remember from classic image recognition tasks where weapplied filters (<em>kernels</em>) to derive aggregate features:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47f05950-bfad-4013-b854-679c9a47ada9_3216x2144.png" style="zoom:50%;"></p><p>Since we are dealing with text and not images, we need a1-dimensional perspective instead:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb943872f-de72-43e8-b2f1-cb8213f120a3_3216x1296.png" style="zoom:50%;"></p><p>The kernel that we use to represent this “filter” is derived from theSSM formulation:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05049821-2352-4c04-8fb2-07fe15c20a9c_2620x824.png" style="zoom:50%;"></p><blockquote><p><strong>为什么可以写成卷积的形式？</strong></p><p>如上图，其中 <span class="math inline">\(K\)</span>为卷积核，其中此时的 <strong>A、B、C</strong> 都是常数，可以看出卷积核<span class="math inline">\(K\)</span>是通过固定数值矩阵得到的，只需确定 <strong>A、B、C</strong>即可并行运算。</p></blockquote><p>Let’s explore how this kernel works in practice. Like convolution, wecan use our SSM kernel to go over each set of tokens and calculate theoutput:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9007d03b-c1c9-4b37-8c83-f27bfe8318f4_2620x1080.png" style="zoom:50%;"></p><p>This also illustrates the effect padding might have on the output. Ichanged the order of padding to improve the visualization but we oftenapply it at the end of a sentence.</p><p>In the next step, the kernel is moved once over to perform the nextstep in the calculation:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ed71fb-f237-4173-bb23-bd1bf02ff123_2620x1080.png" style="zoom:50%;"></p><p>In the final step, we can see the full effect of the kernel:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4387b79-5e92-4fe2-8b30-2abf112f5a73_2620x1080.png" style="zoom:50%;"></p><p>A major benefit of representing the SSM as a convolution is that itcan be trained in parallel like Convolutional Neural Networks (CNNs).However, due to the fixed kernel size, their inference is not as fastand unbounded as RNNs.</p><h4 id="the-three-representations">The Three Representations</h4><p>These three representations, <em>continuous</em>, <em>recurrent</em>,and <em>convolutional</em> all have different sets of advantages anddisadvantages:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F682187d6-f402-44aa-8097-8a2e5b6179a7_2072x744.png" style="zoom:50%;"></p><p>Interestingly, we now have efficient inference with the recurrent SSMand parallelizable training with the convolutional SSM.</p><blockquote><ol type="1"><li>在训练的时候怎么使用CNN进行训练？</li></ol><p><span class="math display">\[y=\bar{K}x\]</span></p><p>​ 能够通过使用卷积的方法使其进行并行计算。</p><ol start="2" type="1"><li>在推理的时候怎么进行推理？ <span class="math display">\[\left\{ \begin{array}{lcl}h_k=\bar{A}h_{k-1}+\bar{B}x_k \\y_k=Ch_k+Dx_k\end{array}\right.\]</span></li></ol></blockquote><p>With these representations, there is a neat trick that we can use,namely choose a representation depending on the task. During training,we use the convolutional representation which can be parallelized andduring inference, we use the efficient recurrent representation:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c43c82d-9735-4d55-97bb-8ad6f504909e_1960x1008.png" style="zoom:50%;"></p><p>This model is referred to as the <a href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/05546b0e38ab9175cd905eebcc6ebb76-Abstract.html">LinearState-Space Layer (LSSL)</a>.<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-2-141228095">2</a></p><p>These representations share an important property, namely that of<strong><em>Linear Time Invariance</em></strong> (LTI). LTI states thatthe SSMs parameters, <em>A</em>, <em>B</em>, and <em>C</em>, are fixedfor all timesteps. This means that matrices <em>A</em>, <em>B</em>, and<em>C</em> are the same for every token the SSM generates.</p><p>In other words, regardless of what sequence you give the SSM, thevalues of <em>A</em>, <em>B</em>, and <em>C</em> remain the same. Wehave a static representation that is not content-aware.</p><p>Before we explore how Mamba addresses this issue, let’s explore thefinal piece of the puzzle, <em>matrix A</em>.</p><h4 id="the-importance-of-matrix-a">The Importance of Matrix<em>A</em></h4><blockquote><p><strong>A</strong>矩阵决定了整个状态的演化，即如何从一个状态演化到下一个状态。其本质为存储着之前所有历史信息的浓缩精华，<strong>A</strong> 决定了系统的动态特性，但 <strong>A</strong>可能存在一个问题，即 <strong>A</strong>只记住了之前的部分状态，像RNN那样忘记了部分状态，从而会导致SSM在性能上与RNN相似。</p><p><strong>A</strong> 设计的关键是如何在有限的空间中保留更多的记忆！</p></blockquote><p>Arguably one of the most important aspects of the SSM formulation is<em>matrix A</em>. As we saw before with the recurrent representation,it captures information about the <em>previous</em> state to build the<em>new</em> state.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07542fb1-d4b6-421e-8b2a-a3f0a7790939_2028x876.png" style="zoom:50%;"></p><p>In essence, <em>matrix</em> <em>A</em> produces the hidden state:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47635355-6b9a-4981-af3a-7ee6a12b87b3_1412x468.png" style="zoom:50%;"></p><p>Creating <em>matrix A</em> can therefore be the difference betweenremembering only a few previous tokens and capturing every token we haveseen thus far. Especially in the context of the Recurrent representationsince it only <em>looks back</em> <em>at the previous state</em>.</p><p>So how can we create <em>matrix A</em> in a way that retains a largememory (context size)?</p><p>We use Hungry Hungry Hippo! Or <a href="https://proceedings.neurips.cc/paper/2020/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html">HiPPO</a><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-3-141228095">3</a>for <strong>Hi</strong>gh-order <strong>P</strong>olynomial<strong>P</strong>rojection <strong>O</strong>perators. HiPPO attemptsto compress all input signals it has seen thus far into a vector ofcoefficients.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07985a64-fc26-4ee8-9ec2-c488e4cb709a_1492x488.png" style="zoom:50%;"></p><p>It uses <em>matrix A</em> to build a state representation thatcaptures recent tokens well and decays older tokens. Its formula can berepresented as follows:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bc7c768-7f0c-4983-a21e-70a4f587e6aa_2520x628.png" style="zoom:50%;"></p><p>Assuming we have a square <em>matrix <strong>A</strong></em>, thisgives us:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8f5de9-6448-43d5-9878-c8cd1d938b7c_1436x708.png" style="zoom:50%;"></p><p>Building <em>matrix A</em> using HiPPO was shown to be much betterthan initializing it as a random matrix. As a result, it more accuratelyreconstructs <em>newer</em> signals (recent tokens) compared to<em>older</em> signals (initial tokens).</p><p>The idea behind the HiPPO Matrix is that it produces a hidden statethat memorizes its history.</p><p>Mathematically, it does so by tracking the coefficients of a <a href="https://proceedings.neurips.cc/paper/2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html">Legendrepolynomial</a> which allows it to approximate all of the previoushistory.<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-4-141228095">4</a></p><p>HiPPO was then applied to the recurrent and convolutionrepresentations that we saw before to handle long-range dependencies.The result was <a href="https://arxiv.org/abs/2111.00396">StructuredState Space for Sequences (S4)</a>, a class of SSMs that can efficientlyhandle long sequences.<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-5-141228095">5</a></p><p>It consists of three parts:</p><ul><li><p>State Space Models</p></li><li><p>HiPPO for handling <strong>long-rangedependencies</strong></p></li><li><p>Discretization for creating <strong>recurrent</strong> and<strong>convolution</strong> representations</p></li></ul><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb055ec5-f8c7-4862-ab88-f4fb38abf042_1892x844.png" style="zoom:50%;"></p><p>This class of SSMs has several benefits depending on therepresentation you choose (recurrent vs. convolution). It can alsohandle long sequences of text and store memory efficiently by buildingupon the HiPPO matrix.</p><blockquote><p><strong>NOTE</strong>: If you want to dive into more of the technicaldetails on how to calculate the HiPPO matrix and build a S4 modelyourself, I would HIGHLY advise going through the <a href="https://srush.github.io/annotated-s4/">Annotated S4</a>.</p><p>其中在S4中对 <strong>A</strong> 进行了改进：</p><p><strong>Theorem 1.</strong> All <em>HiPPO</em> matrices have a<em>Normal Plus Low-Rank</em> (NPLR) representation <span class="math display">\[A=VAV^*-PQ^\top=V(\Lambda-(V^*P)(V^*Q))V^*\]</span> for unitary <span class="math inline">\(V\in\mathbb{C}^{N\times N}\)</span>, diagonal <span class="math inline">\(\Lambda\)</span>， and low-rank factorization<span class="math inline">\(P,Q\in\mathbb{R}^{N\times r}\)</span>. Thesematrices <em>HiPPO-LegS, LegT, LagT</em> all satisfy <span class="math inline">\(r=1 \ or \ r=2\)</span>.</p></blockquote><hr><h3 id="introduction-of-s4">Introduction of S4</h3><h4 id="improving-transformer-struggles-with-handling-very-long-sequences">Improvingtransformer struggles with handling very long sequences</h4><p>序列数据一般都是离散的数据 比如文本、图、DNA</p><ol type="1"><li>但现实生活中还有很多连续的数据，比如音频、视频，对于音视频这种信号而言，其一个重要特点就是有极长的contextwindow</li><li>而在transformer长context上往往会失败，或者注意力机制在有着超长上下文长度的任务上并不擅长，而S4擅长这类任务。</li></ol><h4 id="the-definition-and-derivation-of-hippostate-compresses-the-history-of-input">Thedefinition and derivation of HiPPO：State compresses the history ofinput</h4><p>我们已经知道一个RNN网络更擅长于处理这种序列数据，但是RNN网络最大的缺点是由于它的hiddenstate记忆能力有限，导致其会忘记掉一些以前的特征。</p><p>==关键：怎样改善RNN记忆有限的问题？==</p><p>假设我们在 <span class="math inline">\(t_0\)</span>时刻有接收到袁术输入信号 <span class="math inline">\(u(t)\)</span>之前的部分：</p><ol type="1"><li><p>我们希望使用一个记忆单元去压缩之前这段的输入部分来学习特征，我们使用一个多项式去近似这段之前的输入<span class="math display">\[x(t_0)=\begin{bmatrix} 0.1 \\-1.1 \\3.7 \\2.5\end{bmatrix}\]</span></p></li><li><p>当我们在接收更多的signal的时候，我们仍然希望这个记忆单元能够对已经接收到的所有的signal进行压缩，因此需要自动更新多项式的各项系数，如下图所示</p><p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_1.png" alt="Mamba HiPPO 1" border="0" style="zoom:50%;"></p></li><li><p>此时，HiPPO的关键问题就变成了一个优化问题</p><ul><li><p>如何能够找到这些最优的近似？</p></li><li><p>如何快速的更新这些多项式的参数？</p></li></ul><p>因此，我们需要定义一个指标去判断一个近似的好坏程度。这个指标可以解读为</p><p>（1）在距离当下时刻最近的拟合函数 <span class="math inline">\(x\)</span> 与原数据 <span class="math inline">\(u\)</span>保持最大的相似度，距离当下时刻远的保留其大致状态；</p><p>（2）也可以是只关注距离当下时刻最近的拟合函数 <span class="math inline">\(x\)</span> 与原数据 <span class="math inline">\(u\)</span>保持最大的相似度，而不在乎距离当下时刻远的状态；</p><p>（3）还可以是关注整体的拟合程度，使其总体与原数据保持最大........</p><p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_2.png" alt="Mamba HiPPO 2" border="0" style="zoom:50%;"></p><p>(Note: 添加推导过程)</p></li><li><p>HiPPO的定义，两个矩阵 (<strong>A</strong>表示之前的状态怎样影响当前的状态，<strong>B</strong>当前的输入怎样影响当前的状态)与两个信号( <span class="math inline">\(x(t)\)</span> 当前状态之前接收到的signal与<span class="math inline">\(u(t)\)</span> 当前接收到的signal)</p><p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_3.png" alt="Mamba HiPPO 3" border="0" style="zoom:50%;"></p><blockquote><p>需要澄清一点的是作者在这里使用了State spaceModel中状态的定义，与本文前面部分的定义有些许不同. 但事实上这里的 <span class="math inline">\(x(t)\)</span> 就等同于上文中的 <span class="math inline">\(h(t)\)</span> 这里的 <span class="math inline">\(u(t)\)</span> 就等同于上文中的 <span class="math inline">\(x(t)\)</span>。我们更换一下这个表示即可转变为上文中的形式 <span class="math display">\[x&#39;(t)=Ax(t)+Bu(t) \Longrightarrow h&#39;(t)=Ah(t)+Bx(t)\]</span> 其中 <strong>A</strong> 就是HiPPO matrix.</p></blockquote></li><li><p>HiPPO就相当于把一个高维的复杂函数映射压缩成一个简单的函数，这样既保留了基本信息，又节省了空间。如下图所示，<span class="math inline">\(u(t)\)</span> 是原始输入的signal，<span class="math inline">\(x(t)\)</span> 是经过压缩后产生到的signal，即对应上文中的 <span class="math inline">\(h(t)\)</span>.</p><p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_4.png" alt="Mamba HiPPO 4" border="0" style="zoom:50%;"></p></li><li><p>我们对上图进行还原处理即可得到</p><p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_5.png" alt="Mamba HiPPO 5" border="0" style="zoom:50%;"></p><p>说明：图中 <span class="math inline">\(u\)</span>序列是一个原始的长度为10000(横轴的长度为10000)，表示原本的信号总共有10000个1维信号，想要完全表示的话每个信号一个memoryunit，总共需要10000个memory unit。 但我们现在使用一个64个memory unit的信号压缩器(图中的蓝色线，其中这里为了画图清晰只画出了4条曲线，实际上应该是64条，即相当于使用了64个不同的hiddenstate去表示已经接收到的signal，对应的 <strong>A</strong> 矩阵的大小为<span class="math inline">\(\mathbb{R}^{N\times N}\)</span>)去压缩这个10000个memory unit的信号。图像中红色的线是使用压缩后的信号(蓝线：<span class="math inline">\(x\)</span> 序列)还原得到的原始信号序列 (黑线：<span class="math inline">\(u\)</span>序列)，可以看出其中在距离当下时刻最近的还原最准确，距离当下最远的时刻只保留了大致的状态。图中绿色的线是采用了EDM(ExponentialDecaying Measure) 作为评价指标来衡量逼近多项式 <span class="math inline">\(x\)</span> 的好坏。</p></li><li><p>事实上，HiPPO可以推广到任何指标上：</p><p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_6.png" alt="Mamba HiPPO 6" border="0" style="zoom:50%;"></p><p>无论是更关注于当前时刻的状态还是关注于总体的状态，都存在对应的 HiPPOoperator能计算下一时刻的状态。</p></li></ol><h4 id="high-order-hippo-from-state-state-transition-to-state-result-transition">High-orderHiPPO: from state-state transition to state-result transition</h4><p>通过上文，我们已经实现了HiPPO在低阶 (10000个Memoryunit)上的工作流程，然而维度是不断扩大的，我们的目标是将其拓展到高阶上的表示，阶数越高其对应的场景与LLM更加相似。</p><ol type="1"><li><p>一个简单的思路就是继续堆叠HiPPO算子 (即 <span class="math inline">\(x\)</span>的维度数)，但这会面临一个问题，不断地堆叠会造成维度爆炸，并且也失去了压缩的意义。</p><p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_7.png" alt="Mamba HiPPO 7" border="0" style="zoom:50%;"></p></li><li><p>这里采用了将映射的 state <span class="math inline">\(x\)</span>进行线性组合 <span class="math inline">\(Cx\)</span>，最终得到输出 <span class="math inline">\(y\)</span> （图中红色的线），而 <span class="math inline">\(D\)</span> 是一个skip connection, 是绕开状态 <span class="math inline">\(x\)</span>，直接从输入 <span class="math inline">\(u\)</span>到输出 <span class="math inline">\(y\)</span> 的连接。</p><p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_8.png" alt="Mamba HiPPO 8" border="0" style="zoom:50%;"></p><blockquote><p>同样的，这里的 <span class="math inline">\(x\)</span>就等同于上文中的 <span class="math inline">\(h(t)\)</span> 这里的 <span class="math inline">\(u\)</span> 就等同于上文中的 <span class="math inline">\(x\)</span>。我们更换一下这个表示即可转变为上文中的形式 <span class="math display">\[y=Cx+Du\Longrightarrow y_t=Ch(t)+Dx(t)\]</span></p></blockquote></li><li><p>事实上，以上的两个步骤就组成了Kalman在1960年提出的State SpaceMachine：</p></li></ol><p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_9.png" alt="Mamba HiPPO 9" border="0"><span class="math display">\[   \left\{ \begin{array}{rcl}  x&#39;=Ax+Bu \\   y=Cx+Du   \end{array}\right.   \ \  \Longrightarrow \ \   \left\{ \begin{array}{rcl} h&#39;(t)=Ah(t)+Bx(t) \\   y(t) = Ch(t)+Dx(t)   \end{array}\right.   \]</span></p><h4 id="the-definition-and-properties-of-s4">The definition andproperties of S4</h4><p><strong>S4， 即Structured State Spaces。</strong> 在一个State SpaceModel中，我们使用HiPPO operate 实现状态的变换，其中的 <span class="math inline">\(A\)</span> 矩阵我们使用 HiPPO Metrix。</p><p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_10.png" alt="Mamba HiPPO 10" border="0" style="zoom:50%;"></p><p>S4具有以下特点，能够应对连续序列，离散序列等场景。</p><p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_11.png" alt="Mamba HiPPO 11" border="0" style="zoom:50%;"></p><p>在代码实现中，SSM将这些表示作为深度学习pipeline中其中的一层，并且其中的矩阵<span class="math inline">\(A,B,C,D\)</span>是需要优化的变量，通过数据训练得到的。通常有 <span class="math inline">\(d\)</span>层这样的SSM层并行存在，每个对应一个隐藏维度。</p><blockquote><p>To preserve the sequence history, HiPPO projects the history on abasis of <strong>orthogonal polynomials</strong>, which translates tohaving SSMs whose <strong>A,B</strong> matrices are initialized to somematrices.</p><p>This recurrent form of SSMs allows efficient inference (i.e.,generation) : to generate the output of the next time-step, one onlyneeds the state of the current time-step, not the entire inputhistory.</p></blockquote><p>此时的S4, 在训练的时候可以通过卷积的形式进行训练 <span class="math inline">\(y=\bar{K}u\)</span>,这使得S4具有极快的推理速度。</p><blockquote><p>此时仍存在些许疑问，根据上文我们可以知道 <span class="math inline">\(\bar{K}=(C\bar{B},C\bar{A}\bar{B},...,C\bar{A}^k\bar{B},\dots)\)</span>。好像卷积核是随着序列长度不断增长的，这意味着卷积核<span class="math inline">\(\bar{K}\)</span>是无无限长的，但卷积核显然不能是无线长的，因此需要重新设计一个卷积核以适应无限长的输入序列。</p><p><img src="https://imgur.la/images/2024/06/21/Mamba-S4_12.png" alt="Mamba S4 12" border="0" style="zoom:50%;"></p><p>其中，我们不直接计算这个卷积核 <span class="math inline">\(\bar{K}\)</span> ，而是采用 truncated generatingfunction的方式去使用FFT的方法逼近卷积核 <span class="math inline">\(\bar{K}\)</span>.</p><p>此外，文中说的 <span class="math inline">\(\bar{K}\)</span> 是一个<strong>Low-Rank</strong> 的矩阵，是因为 <strong>A</strong> 矩阵是一个<strong>Low-Rank</strong> 的下三角矩阵，而 <span class="math inline">\(\bar{K}=(C\bar{B},C\bar{A}\bar{B},...,C\bar{A}^k\bar{B},\dots)\)</span>因此 <span class="math inline">\(\bar{K}\)</span> 也是一个<strong>Low-Rank</strong> 的矩阵。</p></blockquote><p>但此时的S4似乎仍存在一个问题，<strong>S4中的<span class="math inline">\(A,B,C\)</span>矩阵是始终保持不变的（这里的保持不变是说在inference中始终都使用这三个矩阵），这也就意味着对于不同时刻的输入<span class="math inline">\(x\)</span>我们都使用相同的方式处理，但实际上对于一个序列输入，有些部分重要，有些部分并不重要，对所有的部分都做相同处理，这无法实现对输入的针对性处理</strong></p><p>因为我们在前文中一直说明的SSM模型是线性非时变系统 (Lineartime-invariant system) 的状态空间模型，实际中的SSM可能是一个线性时变系统(Linear time-invariant system),那如何将线性非时变系统的SSM转化称线性时变系统的SSM呢？</p><p><img src="https://imgur.la/images/2024/06/21/Mamba-S4_13.png" alt="Mamba S4 13" border="0" style="zoom:50%;"></p><p>显然，我们只需要将其中的 <span class="math inline">\(A,B,C\)</span>矩阵变成可以跟着状态改变而改变的即可。但如果直接改变，又会引发一系列蝴蝶效应，虽然改变后的SSM模型能够"专注于"输入序列中对当前任务更重要的部分，但由于<span class="math inline">\(\bar{K}=(C\bar{B},C\bar{A}\bar{B},...,C\bar{A}^k\bar{B},\dots)\)</span>而导致其无法实现并行计算了。</p><p>在前文的介绍中，S4可以先计算卷积核 <span class="math inline">\(\bar{K}\)</span> ，保存，然后直接与输入 <span class="math inline">\(x\)</span> 相乘即可得到输出 <span class="math inline">\(y\)</span> ，然而，此时的 <span class="math inline">\(A,B,C\)</span>是会随着输入序列的状态变化的，因而导致了我们无法提前计算卷积核 <span class="math inline">\(\bar{K}\)</span>这也就意味着无法进行卷积运算，并行计算被PASS。</p><p>此时，解决此问题的关键就变成了<strong>如何找到一种无需卷积的并行运算？</strong>这正是Mamba要做的内容....</p><hr><h2 id="part-3-mamba---a-selective-ssm">Part 3: Mamba - A SelectiveSSM</h2><p>We finally have covered all the fundamentals necessary to understandwhat makes Mamba special. State Space Models can be used to modeltextual sequences but still have a set of disadvantages we want toprevent.</p><p>In this section, we will go through Mamba’s two maincontributions:</p><ol type="1"><li><p>A <strong>selective scan algorithm</strong>, which allows themodel to filter (ir)relevant information</p></li><li><p>A <strong>hardware-aware algorithm</strong> that allows forefficient storage of (intermediate) results through <em>parallelscan</em>, <em>kernel fusion</em>, and <em>recomputation</em>.</p></li></ol><p>Together they create the <em>selective SSM</em> or <em>S6</em> modelswhich can be used, like self-attention, to create <em>Mambablocks</em>.</p><p>Before exploring the two main contributions, let’s first explore whythey are necessary.</p><blockquote><p>先回答前一章节留下的问题。</p><p>Mamba解决这一问题的办法是设计了一种简单的选择机制，通过"<strong>参数化SSM的输入</strong>"，让模型对信息有选择性的处理，以便重点关注或忽略特定的输入。因此，模型一方面能够实现对关键信息的长时间记忆，另一方面可以过滤掉与任务无关的序列信息。</p><p>就如前文中所提到的，Mamba每次参考前面所有内容的一个概括，越往后写对前面内容概括得越狠，丢掉细节、保留大意。</p><p>而Transformer每次写之前都要从头到尾重新看一遍前文；RNN则是写着忘着，只关注前面的一句。</p><table><colgroup><col style="width: 6%"><col style="width: 25%"><col style="width: 35%"><col style="width: 32%"></colgroup><thead><tr class="header"><th>模型</th><th><strong>对信息的压缩程度</strong></th><th><strong>训练的效率</strong></th><th><strong>推理的效率</strong></th></tr></thead><tbody><tr class="odd"><td>Transformer</td><td>transformer对每个历史记录都不压缩</td><td>训练消耗算力大</td><td>推理消耗算力大</td></tr><tr class="even"><td>RNN</td><td>随着时间的推移，RNN 往往会忘记某一部分信息</td><td>RNN没法并行训练</td><td>推理时只看一个时间步 故推理高效(<em>相当于推理快但训练慢</em>)</td></tr><tr class="odd"><td>CNN</td><td></td><td>训练效率高，可并行「<em>因为能够绕过状态计算，并实现仅包含(B, L,D)的卷积核</em>」</td><td></td></tr><tr class="even"><td>SSM</td><td>SSM压缩每一个历史记录</td><td></td><td>矩阵不因输入不同而不同，无法针对输入做针对性推理</td></tr><tr class="odd"><td>Mamba</td><td>选择性的关注必须关注的、过滤掉可以忽略的</td><td>mamba每次参考前面所有内容的一个概括，兼备训练、推理的效率</td><td></td></tr></tbody></table><p>总之，序列模型的效率与效果的权衡点在于它们对状态的压缩程度：</p><ul><li>高效的模型必须有一个小的状态(比如RNN或S4)</li><li>而有效的模型必须有一个包含来自上下文的所有必要信息的状态(比如transformer)</li></ul></blockquote><h3 id="what-problem-does-it-attempt-to-solve">What Problem does itattempt to Solve?</h3><p>State Space Models, and even the S4 (Structured State Space Model),perform poorly on certain tasks that are vital in language modeling andgeneration, namely <em>the ability to focus on or ignore particularinputs</em>.</p><p>We can illustrate this with two synthetic tasks, namely<strong>selective copying</strong> and <strong>inductionheads</strong>.</p><p>In the <strong>selective copying</strong> task, the goal of the SSMis to copy parts of the input and output them in order:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f1d1fb-7603-4f86-a2e4-88e0496a1f08_2120x464.png" style="zoom:50%;"></p><p>However, a (recurrent/convolutional) SSM performs poorly in this tasksince it is <strong><em>Linear Time Invariant</em>.</strong> As we sawbefore, the matrices <em>A</em>, <em>B</em>, and <em>C</em> are the samefor every token the SSM generates.</p><p>As a result, an SSM cannot perform <em>content-aware reasoning</em>since it treats each token equally as a result of the fixed A, B, and Cmatrices. This is a problem as we want the SSM to reason about the input(prompt).</p><p>The second task an SSM performs poorly on is <strong>inductionheads</strong> where the goal is to reproduce patterns found in theinput:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27775742-18d2-4f2c-b792-5706976e75e3_2120x744.png" style="zoom:50%;"></p><p>In the above example, we are essentially performing one-shotprompting where we attempt to “teach” the model to provide an“<strong><em>A:</em></strong>” response after every“<strong><em>Q:</em></strong>”. However, since SSMs are time-invariantit cannot select which previous tokens to recall from its history.</p><p>Let’s illustrate this by focusing on <em>matrix B</em>. Regardless ofwhat the input <strong><em>x</em></strong> is, <em>matrix B</em> remainsexactly the same and is therefore independent of<strong><em>x</em></strong>:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ee2bd7a-b99b-4871-8396-69cb7dd13cf5_1480x808.png" style="zoom:50%;"></p><p>Likewise, <em>A</em> and <em>C</em> also remain fixed regardless ofthe input. This demonstrates the <em>static</em> nature of the SSMs wehave seen thus far.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fea51ca-8458-4216-bf1c-c880a23504b4_1412x484.png" style="zoom:50%;"></p><p>In comparison, these tasks are relatively easy for Transformers sincethey <em>dynamically</em> change their attention based on the inputsequence. They can selectively “look” or “attend” at different parts ofthe sequence.</p><p>The poor performance of SSMs on these tasks illustrates theunderlying problem with time-invariant SSMs, the static nature ofmatrices <em>A</em>, <em>B</em>, and <em>C</em> results in problems with<em>content-awareness</em>.</p><h3 id="selectively-retain-information">Selectively RetainInformation</h3><p>The recurrent representation of an SSM creates a small state that isquite efficient as it compresses the entire history. However, comparedto a Transformer model which does no compression of the history (throughthe attention matrix), it is much less powerful.</p><p>Mamba aims to have the best of both worlds. A small state that is aspowerful as the state of a Transformer:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84b8a71a-6310-416b-8622-e9166593171e_1540x464.png" style="zoom:50%;"></p><p>As teased above, it does so by compressing data selectively into thestate. When you have an input sentence, there is often information, likestop words, that does not have much meaning.</p><p>To selectively compress information, we need the parameters to bedependent on the input. To do so, let’s first explore the dimensions ofthe input and output in an SSM during training:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9376222e-b232-4458-a72c-bd741d8a031c_1436x500.png" style="zoom:50%;"></p><p>In a Structured State Space Model (S4), the matrices <em>A</em>,<em>B</em>, and <em>C</em> are independent of the input since theirdimensions <strong><em>N</em></strong> and <strong><em>D</em></strong>are static and do not change.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e93b701-df74-4954-be5c-c0d83779d3df_1412x532.png" style="zoom:50%;"></p><p>Instead, Mamba makes matrices <em>B</em> and <em>C,</em> and even the<em>step size</em> <strong>∆</strong><em>,</em> dependent on the inputby incorporating the sequence length and batch size of the input:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdccefffd-5712-45ab-9821-c794bce65d7d_1412x596.png" style="zoom:50%;"></p><p>This means that for every input token, we now have different<em>B</em> and <em>C</em> matrices which solves the problem withcontent-awareness!</p><blockquote><p><strong>NOTE</strong>: Matrix <em>A</em> remains the same since wewant the state itself to remain static but the way it is influenced(through <em>B</em> and <em>C</em>) to be dynamic.</p></blockquote><p>Together, they <em>selectively</em> choose what to keep in the hiddenstate and what to ignore since they are now dependent on the input.</p><p>A smaller <em>step size</em> <strong>∆</strong> results in ignoringspecific words and instead using the previous context more whilst alarger <em>step size</em> <strong>∆</strong> focuses on the input wordsmore than the context:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06b21aab-aa32-450a-ae02-b976a2c9f9d8_2520x616.png" style="zoom:50%;"></p><h3 id="the-scan-operation">The Scan Operation</h3><p>Since these matrices are now <em>dynamic</em>, they cannot becalculated using the convolution representation since it assumes a<em>fixed</em> kernel. We can only use the recurrent representation andlose the parallelization the convolution provides.</p><p>To enable parallelization, let’s explore how we compute the outputwith recurrency:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed3ad33b-7b7f-4b69-b28e-8437c7b71e2e_1540x536.png" style="zoom:50%;"></p><p>Each state is the sum of the previous state (multiplied by<em>A</em>) plus the current input (multiplied by <em>B</em>). This iscalled a <em>scan operation</em> and can easily be calculated with a forloop.</p><p>Parallelization, in contrast, seems impossible since each state canonly be calculated if we have the previous state. Mamba, however, makesthis possible through the <em><a href="https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda">parallelscan</a></em> algorithm.</p><p>It assumes the order in which we do operations does not matterthrough the associate property. As a result, we can calculate thesequences in parts and iteratively combine them:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F191fdabe-6b38-4e6b-a9f0-8240feef0a9d_1640x1100.png" style="zoom:50%;"></p><p>Together, dynamic matrices <em>B</em> and <em>C</em>, and theparallel scan algorithm create the <strong><em>selective scanalgorithm</em></strong> to represent the dynamic and fast nature ofusing the recurrent representation.</p><h3 id="hardware-aware-algorithm">Hardware-aware Algorithm</h3><p>A disadvantage of recent GPUs is their limited transfer (IO) speedbetween their small but highly efficient SRAM and their large butslightly less efficient DRAM. Frequently copying information betweenSRAM and DRAM becomes a bottleneck.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a1d4fa3-526d-488e-8768-c7208f018eb4_1728x300.png" style="zoom:50%;"></p><p>Mamba, like Flash Attention, attempts to limit the number of times weneed to go from DRAM to SRAM and vice versa. It does so through<em>kernel fusion</em> which allows the model to prevent writingintermediate results and continuously performing computations until itis done.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F563b3792-701a-42b1-9b68-7b6457f5e63d_1728x344.png" style="zoom:50%;"></p><p>We can view the specific instances of DRAM and SRAM allocation byvisualizing Mamba’s base architecture:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F724eceb1-4356-4ac5-b44e-f7fabce3b472_1728x580.png" style="zoom:50%;"></p><p>Here, the following are fused into one kernel:</p><ul><li><p>Discretization step with <em>step size</em><strong>∆</strong></p></li><li><p>Selective scan algorithm</p></li><li><p>Multiplication with <em>C</em></p></li></ul><p>The last piece of the hardware-aware algorithm is<em>recomputation</em>.</p><p>The intermediate states are not saved but are necessary for thebackward pass to compute the gradients. Instead, the authors recomputethose intermediate states <em>during</em> the backward pass.</p><p>Although this might seem inefficient, it is much less costly thanreading all those intermediate states from the relatively slow DRAM.</p><p>We have now covered all components of its architecture which isdepicted using the following image from its article:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc840fb8-2e24-4103-95c8-afa306ce0cfc_2409x743.png" style="zoom:50%;"></p><p><strong>The Selective SSM.</strong> Retrieved from: Gu, Albert, andTri Dao. "Mamba: Linear-time sequence modeling with selective statespaces." <em>arXiv preprint arXiv:2312.00752</em> (2023).</p><p>This architecture is often referred to as a <strong><em>selectiveSSM</em></strong> or <strong><em>S6</em></strong> model since it isessentially an S4 model computed with the selective scan algorithm.</p><h3 id="the-mamba-block">The Mamba Block</h3><p>The <em>selective SSM</em> that we have explored thus far can beimplemented as a block, the same way we can represent self-attention ina decoder block.</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb76db77-1dba-42bd-8de4-4bce240ff67e_1776x1012.png" style="zoom:50%;"></p><p>Like the decoder, we can stack multiple Mamba blocks and use theiroutput as the input for the next Mamba block:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc94d349d-8620-45a9-8095-7c27de8b7865_1660x1356.png" style="zoom:50%;"></p><p>It starts with a linear projection to expand upon the inputembeddings. Then, a convolution before the <em>Selective SSM</em> isapplied to prevent independent token calculations.</p><p>The <em>Selective SSM</em> has the following properties:</p><ul><li><p><em>Recurrent SSM</em> created through<em>discretization</em></p></li><li><p><em>HiPPO</em> initialization on matrix <em>A</em> to capture<em>long-range dependencies</em></p></li><li><p>S<em>elective scan algorithm</em> to selectively compressinformation</p></li><li><p><em>Hardware-aware algorithm</em> to speed upcomputation</p></li></ul><p>We can expand on this architecture a bit more when looking at thecode implementation and explore how an end-to-end example would looklike:</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa67d7341-9a43-4c67-aa88-6e802c0902ae_1660x2040.png" style="zoom:50%;"></p><p>Notice some changes, like the inclusion of normalization layers andsoftmax for choosing the output token.</p><p>When we put everything together, we get both fast inference andtraining and even unbounded context!</p><p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe528e5fa-0dd8-4e6c-b31a-5248aaee6c68_2072x912.png" style="zoom:50%;"></p><p>Using this architecture, the authors found it matches and sometimeseven exceeds the performance of Transformer models of the same size!</p><h2 id="conclusion"><strong>Conclusion</strong></h2><p>This concludes our journey in State Space Models and the incredibleMamba architecture using a selective State Space Model. Hopefully, thispost gives you a better understanding of the potential of State SpaceModels, particularly Mamba. Who knows if this is going to replace theTransformers but for now, it is incredible to see such differentarchitectures getting well-deserved attention!</p><h2 id="resources">Resources</h2><p>Hopefully, this was an accessible introduction to Mamba and StateSpace Models. If you want to go deeper, I would suggest the followingresources:</p><ul><li><a href="https://srush.github.io/annotated-s4/">The Annotated S4</a>is a JAX implementation and guide through the S4 model and is highlyadvised!</li><li>A great <a href="https://www.youtube.com/watch?v=ouF-H35atOY">YouTube video</a>introducing Mamba by building it up through foundational papers.</li><li><a href="https://github.com/state-spaces/mamba">The Mambarepository</a> with <a href="https://huggingface.co/state-spaces">checkpoints on HuggingFace</a>.</li><li>An amazing series of blog posts (<a href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-1">1</a>, <a href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-2">2</a>, <a href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-3">3</a>)that introduces the S4 model.</li><li>The <a href="https://jameschen.io/jekyll/update/2024/02/12/mamba.html">MambaNo. 5 (A Little Bit Of...)</a> blog post is a great next step to diveinto more technical details about Mamba but still from an amazinglyintuitive perspective.</li><li>And of course, <a href="https://arxiv.org/abs/2312.00752">the Mambapaper</a>! It was even used for DNA modeling and speech generation.</li></ul><hr><h2 id="references">References</h2><blockquote><p>[1]原Blog链接：https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state</p><p>[2] 一文通透想颠覆Transformer的Mamba：从SSM、HiPPO、S4到Mambahttps://blog.csdn.net/v_JULY_v/article/details/134923301</p><p>[3]</p><p>[4] Gu, A., &amp; Dao, T. (2023). Mamba: Linear-time sequencemodeling with selective state spaces. <em>arXiv preprintarXiv:2312.00752</em>.</p><p>[5]</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Sequence Model </tag>
            
            <tag> State space model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Support-Vector-Machine</title>
      <link href="/2024/05/29/Support-Vector-Machine/"/>
      <url>/2024/05/29/Support-Vector-Machine/</url>
      
        <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="支持向量机-support-vector-machine">支持向量机 (Support VectorMachine)</h1><h2 id="简介">简介</h2><p>支持向量机是无监督机器学习中一个经典的算法，具有完善的数学理论和推理证明。本文将从SVM问题定义，模型建立，数学推理对SVM进行详细的论述。</p><h2 id="线性可分-linear-separable">线性可分 (Linear Separable)</h2><h4 id="问题定义">问题定义</h4><p>在一个样本空间中，给定训练样本集，在样本空间中能够找到一个划分超平面，将不同类别的样本区分开。</p><p><img src="https://imgur.la/images/2024/06/03/image-20240530093244843.png" alt="SVM_image1" style="zoom:33%;"></p><center><p>图1：线性可分，存在多个超平面将两类训练样本分开</p></center><p><strong>结论1：在一个空间中，如果存在一条直线能够划分两个点集，那么将存在无数条直线能够划分这两个点集。(证明见附录)</strong></p><h4 id="数学定义"><strong>数学定义</strong></h4><p><span class="math inline">\(D_1\)</span>和 <span class="math inline">\(D_2\)</span> 是 n 维欧氏空间中的两个点集。如果存在n 维向量 <span class="math inline">\(w\)</span> 和实数 <span class="math inline">\(b\)</span> 使得所有属于 <span class="math inline">\(D_0\)</span> 的点 <span class="math inline">\(x_i\)</span> 都有 <span class="math inline">\(wx_i+b&gt;0\)</span>, 而对于所有属于 <span class="math inline">\(D_1\)</span> 的点 <span class="math inline">\(x_j\)</span> 则有 <span class="math inline">\(wx_j+b&lt;0\)</span>，则我们称 <span class="math inline">\(D_0\)</span> 和 <span class="math inline">\(D_1\)</span> 线性可分。</p><h2 id="svm解决线性问题">SVM解决线性问题</h2><h4 id="线性问题定义">线性问题定义</h4><p>由 <strong>结论1</strong>可得，对于一个线性可分的问题，存在无数条满足要求的直线，那么究竟哪一条直线是最好的？</p><p><img src="https://imgur.la/images/2024/06/03/image-20240530104549138.png" alt="image-20240530104549138" style="zoom: 33%;"></p><center><p>图2：线性可分，存在多个超平面将两类训练样本分开</p></center><p>直观上看，应该去找位于两类训练样本“正中间”的划分超平面，即图中红色的直线，因为该划分超平面对训练样本局部扰动的“容忍”性最好，例如，由于训练集的局限性或噪声的因素，训练集外的样本可能比图中的训练样本更接近两个类的分隔界，这将使许多划分超平面出现错误，而红色的超平面受影响最小，换言之，这个划分超平面所产生的分类结果是最鲁棒的，对未见示例的泛化能力最强。</p><p><strong>问题1：我们怎样定义这样一个最好的直线？</strong>答：为每条直线定义一个性能指标，将每条直线平移直至该直线能够插到最边缘的样本点，如下图所示：</p><p><img src="https://imgur.la/images/2024/06/03/image-20240530112035809.png" alt="image-20240530112035809" style="zoom: 33%;"></p><center><p>图3：支持向量与间隔</p></center><p>如图3所示，我们做如下定义：</p><p><span class="math inline">\(r\)</span> :间隔(Margin)，将平行线平移直至插到支持向量的距离</p><p><span class="math inline">\(x\)</span> : 训练集中的样本</p><p><span class="math inline">\(y\)</span> : 训练集中样本对应的标签</p><p><span class="math inline">\(D=\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\},y_i\in\{-1,+1\}\)</span> : 训练样本集</p><p><span class="math inline">\(w^Tx+b=0\)</span> :超平面(Hypeplane)，即上文中提到的直线，在二维空间中是一条直线，在高维空间中，则是超平面(hyperplane)。</p><h4 id="svm基本型">SVM基本型</h4><p>支持向量机——线性可分，本质为最大化 <strong>Margin</strong>问题，属于凸优化中的 <strong>二次规划</strong> 问题。</p><ul><li>最小化 (Minimize) : <span class="math inline">\(\frac{1}{2}||w||^2\)</span></li><li>限制条件 (Subject to) : <span class="math inline">\(y_i[w^Tx_i+b]\geq1, \ \ \ \ \ \ \ \ \ \ \ \i=1，2,\dots,n\)</span></li></ul><p><strong>解释：</strong></p><p><strong>定理1：</strong><span class="math inline">\(w^T+b=0\)</span>与 <span class="math inline">\(aw^T+ab=0\)</span> 是同一个超平面，若<span class="math inline">\((w,b)\)</span> 满足 <span class="math inline">\(y_i[w^Tx_i+b]\geq1\)</span> 那么 <span class="math inline">\((aw,ab)\)</span> 也满足 <span class="math inline">\(y_i[aw^Tx_i+ab]\geq1\)</span> 。</p><p><strong>定理2：</strong>点 <span class="math inline">\((x_0,y_0)\)</span> 到直线 <span class="math inline">\(ax+by+c=0\)</span> 的距离 <span class="math inline">\(d=\frac{|ax_0+by_0+c|}{\sqrt{a^2+b^2}}\)</span> ;点到超平面 <span class="math inline">\(H\)</span> 的距离即 <span class="math inline">\(r=\frac{|w^Tx_0+b|}{||w||}\)</span>.</p><p>由 <strong>定理1</strong>我们可以通过缩放将 <span class="math inline">\(|w^Tx_0+b|\)</span> 通过乘以一个缩放因子 <span class="math inline">\(a\)</span> 使得 <span class="math inline">\(|aw^Tx_0+ab|=1\)</span>, 则此时 <span class="math inline">\(r=\frac{1}{||w||}\)</span>, 故而最大化<strong>Margin</strong> 即可以转变为最小化 <span class="math inline">\(||w||\)</span>问题。由于我们在计算机中使用梯度下降进行最优化求解，为了求导数方便，因此常常使用<span class="math inline">\(||w||^2\)</span> 或者 <span class="math inline">\(\frac{1}{2}||w||^2\)</span> 的形式来代替 <span class="math inline">\(||w||\)</span> 。</p><p>限制条件 :即对于所有的样本点，除了支持向量，其他的点都必须比支持向量到超平面的距离要远。</p><p>到此，SVM的基本型便定义完成，这是一个简单的凸优化问题，我们可以使用简单的凸优化求解方法来实现计算(如梯度下降法)。</p><h2 id="svm解决非线性问题">SVM解决非线性问题</h2><h4 id="非线性可分">非线性可分</h4><p><img src="https://imgur.la/images/2024/06/05/SVM_image4_5.png" alt="SVM image4 5" border="0" style="zoom: 33%;"></p><center><p>图4：非线性可分，在二维空间中，我们无法找到一条直线来分离这两个类别</p></center><p>如图4所示，对于非线性可分的情况，在二维空间中我们无法使用一条直线或者一个平面将两个类别完全区分。在低维空间中我们无法有效地将这两类点完全区分，但在高维空间中，我们可以找到一个超平面来完全区分这两类点(<a href="https://en.wikipedia.org/wiki/Cover%27s_theorem"><strong>Cover’stheorem</strong></a>) .</p><p><img src="https://imgur.la/images/2024/06/03/SVM_image64b9b723ba732350b.png" alt="SVM image6" border="0" style="zoom: 25%;"></p><center><p>图5：在低维空间中非线性可分，但将这些点映射到高维空间中则线性可分</p></center><p>因此，为了寻找一个超平面，我们需要将原本的样本点 <span class="math inline">\(x\)</span> 映射到一个高维空间中去，即设 <span class="math inline">\(\varphi(x)\)</span> 为映射函数，<span class="math inline">\(\varphi(x): \mathcal{X}\longrightarrow\mathcal{H}\)</span>，其中 <span class="math inline">\(\mathcal{X}\)</span> 为原输入空间，<span class="math inline">\(\mathcal{H}\)</span> 为特征空间( <span class="math inline">\(\mathcal{H}\)</span> 是Hibert Space，即完备的内积空间) <span class="math display">\[x \stackrel{\varphi}{\longrightarrow}\varphi(x)\]</span> <strong>问题2: 如何寻找这个高维映射方法？</strong> 直接将<span class="math inline">\(x\)</span> 映射为为无限维。</p><p>但此时由于 <span class="math inline">\(\varphi(x)\)</span>是无限维，无法计算。 此时我们使用 <strong>核技巧(kernel trick) :在输入空间中找到一个核函数(kernel function) <span class="math inline">\(K(x_1,x_2)\)</span> 使得 <span class="math inline">\(K(x_1,x_2)=&lt;\varphi(x_1),\varphi(x_2)&gt;\)</span>,从而可以直接在低维空间中计算出结果，加速核方法计算。</strong>(相关性质及定义请见 <strong>附录：核函数</strong>)</p><p><strong>问题3：如何实现非线性中完全区分不同类的点？</strong></p><p>为了使线性可分的SVM优化泛化到非线性可分的情况，我们允许支持向量机在一些样本上出错，为此引入了<strong>软间隔 (Soft Margin)</strong>，允许少部分样本可以不满足约束<span class="math inline">\(y_i[w^Tx_i+b]\geq1\)</span>,但在最大化间隔的同时，应该保证不满足约束的样本数量尽可能的少，因此优化目标进一步可以变为：<span class="math display">\[\mathop{\min}_{w,b}\  \frac{1}{2}||w||^2+C\sum\limits_{i=1}^{N}\mathscr{l}_{0/1}(y_i[w^Tx_i+b]-1)\]</span> 其中 <span class="math inline">\(C&gt;0\)</span>是一个常数，<span class="math inline">\(\mathscr{l}_{0/1}\)</span>是"0/1损失函数" <span class="math display">\[\mathscr{l}_{0/1}(z)= \left \{ \begin{array}{rcl} 1, &amp; \mbox{if}\ \z&lt;0; \\ 0, &amp; \mbox{otherwise.} \end{array} \right.\]</span> 其中，当 <span class="math inline">\(C\)</span>为无穷大时，新的优化目标式子能够迫使所有的样本均满足约束式子。此时 <span class="math inline">\(\frac{1}{2}||\omega||^2+C\sum\limits_{i=1}^{N}\mathscr{l}_{0/1}(y_i[w^Tx_i+b]-1)\)</span> 与 <span class="math inline">\(\frac{1}{2}||w||^2\)</span> 等价；当 <span class="math inline">\(C\)</span>为有限值时，新的优化目标允许一部分样本不满足约束。</p><p>然而，<span class="math inline">\(\mathscr{l}_{0/1}\)</span>非凸、非连续，数学性质不好，导致不易直接求解，故而我们通常使用其他的一些函数代替<span class="math inline">\(\mathscr{l}_{0/1}\)</span> 并称之为"替代损失 (surrogateloss)"。这些替代损失通常具有较好的数学性质，通常是凸的连续函数且是 <span class="math inline">\(\mathscr{l}_{0/1}\)</span>的上界。如我们采用hinge损失代替 <span class="math inline">\(\mathscr{l}_{0/1}\)</span> 则优化目标可以变为<span class="math display">\[\mathop{\min}_{w,b}\  \frac{1}{2}||\omega||^2+C\sum\limits_{i=1}^{N}\max(0,\ 1-y_i[w^Tx_i+b])\]</span> 实际中我们可以引入<strong>松弛变量 (Slack Variable) <span class="math inline">\(\xi_i\geq0\)</span></strong>来代替上式中的替代损失，得到 <span class="math display">\[\mathop{\min}_{w,b}\ \frac{1}{2}||\omega||^2+C\sum\limits_{i=1}^{N}\xi_i\]</span>从机器学习的角度来看，非线性SVM的优化目标可以看作为对线性的SVM优化目标添加了一个<strong>正则项 (Regulation) <span class="math inline">\(C\sum\limits_{i=1}^{N}\xi_i\)</span></strong></p><h4 id="非线性svm优化">非线性SVM优化</h4><p>支持向量机——非线性可分 ("软间隔"支持向量机)</p><ul><li>最小化 (Minimize) : <span class="math inline">\(\frac{1}{2}||w||^2+C\sum\limits_{i=1}^{N}\xi_i\)</span></li><li>限制条件 (Subject to) :<ul><li>​ <span class="math inline">\(y_i[w^T\varphi(x_i)+b]\geq1-\xi_i, \ \\ \ \ \ \ \ \ \ \ \ i=1，2,\dots,n\)</span></li><li>​ <span class="math inline">\(\xi_i\geq0\)</span></li></ul></li></ul><p>在这个式子中存在高维映射 <span class="math inline">\(\varphi(x_i)\)</span>,因而无法直接求解，因此我们要将该优化问题转换为可以求解的形式。</p><h4 id="解非线性svm的优化问题">解非线性SVM的优化问题</h4><p><strong>关键：将非线性的SVM优化问题转换为其对应的对偶问题，利用求其对偶问题的解来代替求原非线性SVM问题的解</strong></p><p>(原问题与对偶问题的关系证明见附录——优化理论：原问题与对偶问题)</p><ul class="task-list"><li><p><label><input type="checkbox"><strong>Step 1:</strong>为了方便转换为其对应的对偶问题，首先要转变原问题的形式，使其与附录中的原问题的形式相一致。</label></p><p><img src="https://imgur.la/images/2024/06/05/SVM_image7.png" alt="SVM image7" border="0"></p><center><p></p><p>图6：将非线性SVM的优化问题形式转换，与原问题中的形式保持一致</p><p></p></center><p><strong>解释：</strong>其中 <span class="math inline">\(\xi_i\)</span> 是松弛变量，我们调整其取值范围为<span class="math inline">\(\xi_i\leq0\)</span>,相比较于原式子，只是在原来的 <span class="math inline">\(\xi_i\)</span>前面加了一个 "负号"。因此保证之前的优化目标不变，故而要变为 <span class="math inline">\(\frac{1}{2}||w||^2-C\sum\limits_{i=1}^{N}\xi_i\)</span> ,限制条件变为 <span class="math inline">\(y_i[w^T\varphi(x_i)+b]\geq1+\xi_i\)</span> ,将其移项得 <span class="math inline">\(1+\xi_i-y_iw^T\varphi(x_i)-y_ib\leq0\)</span>。此时原问题中的 <span class="math inline">\(f(w)\Longrightarrow\frac{1}{2}||\omega||^2-C\sum \limits_{i=1}^{N}\xi_i\)</span> , <span class="math inline">\(g_i(w)\leq0 \Longrightarrow1+\xi_i-y_iw^T\varphi(x_i)-y_ib\leq0 和 \xi_i\leq0\)</span> 。</p><h5 id="非线性svm优化-转换后">非线性SVM优化 (转换后)</h5><p>支持向量机——非线性可分 ("软间隔"支持向量机)</p><ul><li>最小化 (Minimize) : <span class="math inline">\(\frac{1}{2}||\omega||^2-C\sum\limits_{i=1}^{N}\xi_i\)</span></li><li>限制条件 (Subject to) :<ul><li>​ <span class="math inline">\(1+\xi_i-y_iw^T\varphi(x_i)-y_ib\leq0, \\ \ \ \ \ \ \ \ \ \ \ i=1，2,\dots,n\)</span></li><li>​ <span class="math inline">\(\xi_i\leq0\)</span></li></ul></li></ul></li><li><p><label><input type="checkbox"><strong>Step 2: </strong>寻找转换后的非线性SVM优化问题的对偶问题，根据<strong>附录——优化理论：原问题与对偶问题</strong>我们可得其对偶问题为：</label></p><h5 id="非线性svm优化问题的对偶问题">非线性SVM优化问题的对偶问题</h5><ul><li>最大化 (Maximum) : <span class="math inline">\(\theta(\alpha,\beta)=\inf\limits_{w,\xi_i,b}\{\frac{1}{2}||\omega||^2-C\sum\limits_{i=1}^N\xi_i+\sum\limits_{i=1}^{N}\alpha_i[1+\xi_i-y_iw^T\varphi(x_i)-y_ib]+\sum\limits_{i=1}^{N}\beta_i\xi_i\}\)</span></li><li>限制条件 (Subject to) : <span class="math inline">\(i=1,2,\dots,N\)</span><ul><li><span class="math inline">\(\alpha_i\geq0\)</span> or <span class="math inline">\(\alpha\succeq0\)</span></li><li><span class="math inline">\(\beta_i\geq0\)</span> or <span class="math inline">\(\beta\succeq0\)</span></li></ul></li></ul><p><strong>解释：</strong>其中优化目标中的 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span> 与<strong>附录——优化理论：原问题与对偶问题</strong>中的拉格朗日乘子中稍微有些不同，在原拉格朗日乘子中<span class="math inline">\(\alpha\)</span> 控制限制条件中不等式， <span class="math inline">\(\beta\)</span>控制限制条件中的等式。由于我们转换后的非线性SVM优化问题的限制条件中不存在等式<span class="math inline">\(h_i(w)=0\)</span>这一项，故而非线性SVM优化问题对应的拉格朗日乘子 <span class="math inline">\(L(w,\alpha,\beta)=f(w)+\sum\limits_{i=1}^{K}\alpha_ig_i(w)+\sum\limits_{i=1}^{M}\beta_ih_i(w)\)</span> 中不存在 <span class="math inline">\(\sum \limits_{i=1}^{M}\beta_ih_i(w)\)</span>项。而我们转换后的非线性SVM优化问题的限制条件中存在两个不等式，故而非线性SVM优化问题对应的拉格朗日乘子包含了两个控制不等式的<span class="math inline">\(\alpha\)</span>,这里是为了在同一个式子中便于区分故而分别写为了 <span class="math inline">\(\alpha,\beta\)</span>, 我们转化后的对偶问题中的<span class="math inline">\(\alpha,\beta\)</span> 都对应拉格朗日乘子中的<span class="math inline">\(\alpha\)</span>. 对于限制条件中 <span class="math inline">\(\alpha_i\geq0\)</span> 与 $$0的写法不同但含义相同，只不过前者表示向量 <span class="math inline">\(\alpha\)</span> 中的每一项都大于0，而后者表示向量<span class="math inline">\(\alpha\)</span> 大于0.</p></li><li><p><label><input type="checkbox"><strong>Step 3:</strong>求对偶问题的解，即要求出一组 <span class="math inline">\(w,\xi_i,b\)</span> 使优化目标中 <span class="math inline">\(\{\cdots\}\)</span> 部分最小。</label></p><p>利用求导法，令 <span class="math display">\[\begin{align}&amp;\frac{\partial L}{\partial\omega}=0 \ \ \ \Longrightarroww-\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)=0 \ \ \ \Longrightarroww=\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i) \\&amp;\frac{\partial L}{\partial\xi_i}=0 \ \ \ \Longrightarrow\alpha_i+\beta_i=C \ \ \ \Longrightarrow \alpha_i+\beta_i=C \\&amp;\frac{\partial L}{\partial b}=0 \ \ \ \Longrightarrow\sum_{i=1}^{N}\alpha_iy_i=0\end{align}\]</span> 将上式代入 <span class="math inline">\(\theta(\alpha,\beta)\)</span> 得： <span class="math display">\[\begin{align}\theta(\alpha,\beta)&amp;=\inf \limits_{w,\xi_i,b}\{\frac{1}{2}||w||^2-C\sum\limits_{i=1}^N\xi_i+\sum_{i=1}^{N}\alpha_i[1+\xi_i-y_iw^T\varphi(x_i)-y_ib]+\sum_{i=1}^{N}\beta_i\xi_i\} \\&amp;=\inf \limits_{w,\xi_i,b}\{\frac{1}{2}w^Tw-\bcancel{\sum_{i=1}^{N}\alpha_i\xi_i}-\bcancel{\sum_{i=1}^{N}\beta_i\xi_i}+\sum_{i=1}^{N}\alpha_i+\bcancel{\sum_{i=1}^{N}\alpha_i\xi_i}-\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)w^T-\bcancel{\sum_{i=1}^{N}\alpha_iy_ib}+\bcancel{\sum_{i=1}^{N}\beta_i\xi_i}\} \\&amp;=\inf \limits_{w,\xi_i,b}\{\frac{1}{2}\big(\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)\big)^T\big(\sum_{j=1}^{N}\alpha_jy_j\varphi(x_j)\big)+\sum_{i=1}^{N}\alpha_i-\big(\sum_{i=1}^{N}\alpha_iy_i\varphi(x_i)\big)^T\big(\sum_{j=1}^{N}\alpha_jy_j\varphi(x_j)\big)\} \\&amp;=\inf \limits_{w,\xi_i,b}\{\sum_{i=1}^{N}\alpha_i-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\underbrace{\alpha_i\alpha_j}_{常数}\underbrace{y_iy_j}_{y=\pm1}\underbrace{\varphi(x_i)^T\varphi(x_j)}_{K(x_i,x_j)}\}\end{align}\]</span> 最后，对偶问题可以转换为：</p><h5 id="非线性svm优化问题的对偶问题-求解后">非线性SVM优化问题的对偶问题(求解后)</h5><ul><li>最大化 (Maximum) : <span class="math inline">\(\theta(\alpha)=\sum\limits_{i=1}^{N}\alpha_i-\frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i\alpha_j\underbrace{y_jy_i}_{已知}\underbrace{K(x_i,x_j)}_{已知}\)</span></li><li>限制条件 (Subject to) : <span class="math inline">\(i=1,2,\dots,N\)</span><ul><li><span class="math inline">\(\alpha\geq0, \&amp;\&amp;\  \beta_i\geq0\ \ \ \Longrightarrow\ 0\leq\alpha_i\leq C\)</span></li><li><span class="math inline">\(\sum\limits_{i=1}^{N}\alpha_iy_i=0\)</span></li></ul></li></ul><p>此时，该问题变为了一个基本的凸优化问题，我们可以使用 <a href="https://en.wikipedia.org/wiki/Sequential_minimal_optimization"><strong>SMO算法</strong></a>对其进行求解。</p></li><li><p><label><input type="checkbox"><strong>Step 4: </strong>将求<span class="math inline">\(\alpha\)</span> 转化为求 <span class="math inline">\(w,b\)</span>.</label></p><p>由 <strong>Step 3</strong> 有 <span class="math inline">\(w=\sum\limits_{i=1}^{N}\alpha_iy_i\varphi(x_i)\)</span>，但其中仍然存在高维向量 <span class="math inline">\(\varphi(x_i)\)</span>似乎无法求解，但实际上，我们对于一个测试样本 <span class="math inline">\(x\)</span> ,则有: <span class="math display">\[\left\{ \begin{array}{rcl}若w^T\varphi(x)+b\geq0,则\ y=+1\\若w^T\varphi(x)+b&lt;0,则\ y=-1\end{array}\right.\]</span> 其中 <span class="math inline">\(w^T\varphi(x)=\sum\limits_{i=1}^{N}\alpha_iy_i\varphi(x_i)^T\varphi(x_i)\\  \Longrightarrow \sum\limits_{i=1}^{N}\alpha_iy_iK(x_i,x)\)</span> ,即我们不需要知道 <span class="math inline">\(w\)</span>具体是多少，我们可以直接算出 <span class="math inline">\(w^T\varphi(x)\)</span> 的值。</p><p>现在只需要求出 <span class="math inline">\(b\)</span> 即可，利用<strong>KKT条件</strong> 求 <span class="math inline">\(b\)</span>.</p><p><strong>KKT条件：</strong>对于 <span class="math inline">\(\foralli=1,2,\dots,K.\)</span> 则有 <span class="math inline">\(\alpha_i^*=0\)</span> 或者 <span class="math inline">\(g_i(w^*)=0\)</span> .</p><p><strong>转化为SVM中的KKT</strong>，则有 <span class="math inline">\(\forall\  i=1,2,\dots,N\)</span></p><p><span class="math inline">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\ \ \ \blacktriangleright\)</span> 要么 $ _i=0$, 要么 <span class="math inline">\(\xi_i=0\)</span></p><p><span class="math inline">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\ \ \ \blacktriangleright\)</span> 要么 <span class="math inline">\(\alpha_i=0\)</span>, 要么 <span class="math inline">\(1+\xi_i-y_iw^T\varphi(x_i)-y_ib=0\)</span></p><p>因此，我们随机选取一个 <span class="math inline">\(\alpha_i\)</span>, 则 <span class="math inline">\(0&lt;\alpha_i&lt;C\)</span>， 故而<span class="math inline">\(\beta_i=C-\alpha_i&gt;0\)</span></p><p><span class="math inline">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\therefore \beta_i\neq0, \ 则\ \xi_i=0\)</span>,</p><p><span class="math inline">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\therefore \alpha_i\neq0, \ 则 \1+\xi_i-y_iw^T\varphi(x_i)-y_ib=0\)</span></p><p><span class="math inline">\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\thereforeb=\frac{1}{y_i}-\sum_{i=1}^{N}\alpha_iy_iK(x_i,x)\)</span></p><p>至此，非线性SVM的优化问题求解完毕！</p></li></ul><h2 id="总结">总结</h2><h4 id="svm算法训练过程">SVM算法：训练过程</h4><p>输入 <span class="math inline">\(\{(x_i,y_i)\}_{i=1,2,\dots,N}\)</span>，解优化问题</p><ul><li>最大化 (Maximum) : <span class="math inline">\(\theta(\alpha)=\sum\limits_{i=1}^{N}\alpha_i-\frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i\alpha_j\underbrace{y_jy_i}_{已知}\underbrace{K(x_i,x_j)}_{已知}\)</span></li><li>限制条件 (Subject to) : <span class="math inline">\(i=1,2,\dots,N\)</span><ul><li><span class="math inline">\(\alpha\geq0, \&amp;\&amp;\  \beta_i\geq0\ \ \ \Longrightarrow\ 0\leq\alpha_i\leq C\)</span></li><li><span class="math inline">\(\sum\limits_{i=1}^{N}\alpha_iy_i=0\)</span></li></ul></li></ul><p>利用SMO算法求解最优化问题，得出 <span class="math inline">\(\alpha\)</span>, 下一步寻找一个 <span class="math inline">\(0&lt;\alpha_i&lt;0\)</span>，计算 <span class="math inline">\(b\)</span> :</p><p>​ <span class="math inline">\(b=\frac{1}{y_i}-\sum_{i=1}^{N}\alpha_iy_iK(x_i,x)\)</span></p><h4 id="svm算法测试过程">SVM算法：测试过程</h4><p>输入样本 <span class="math inline">\(x\)</span>：</p><ul><li>若 <span class="math inline">\(\sum\limits_{i=1}^{N}\alpha_iy_iK(x_i,x)+b\geq0\)</span>,则 $ y=+1$,</li><li>若 <span class="math inline">\(\sum\limits_{i=1}^{N}\alpha_iy_iK(x_i,x)+b&lt;0\)</span>,则 $ y=+1$.</li></ul><h2 id="附录">附录</h2><h4 id="结论1证明">结论1证明：</h4><h5 id="二维平面上的证明">二维平面上的证明</h5><p>假设我们有两个点集 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span>，并且这两个点集是线性可分的。这意味着存在一条直线可以将这两个点集分开。为了证明存在无数条直线能够划分这两个点集，我们可以如下进行证明：</p><ol type="1"><li><p><strong>存在一条直线</strong>：假设存在一条直线 <span class="math inline">\(L\)</span> 可以将点集 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 分开。直线 <span class="math inline">\(L\)</span> 可以表示为： <span class="math display">\[𝑦=k𝑥+b\]</span> 其中 <span class="math inline">\(k\)</span> 是斜率，<span class="math inline">\(b\)</span> 是截距。</p></li><li><p><strong>平行直线的性质</strong>：对于任何一个固定的斜率𝑚<em>m</em>，不同的截距 𝑐<em>c</em>会产生不同的平行直线。设我们有另一条直线 𝐿′<em>L</em>′ 其方程为： <span class="math display">\[𝑦=𝑚𝑥+b^′\]</span> 其中 <span class="math inline">\(b&#39;\neq b\)</span>，则<span class="math inline">\(L&#39;\)</span> 与 <span class="math inline">\(L\)</span> 平行。</p></li><li><p><strong>平行直线的分离能力</strong>：由于 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 是线性可分的，这意味着可以找到一个间隔<span class="math inline">\(\epsilon\)</span> 使得在直线 <span class="math inline">\(L\)</span>的一侧存在一个空隙，其中没有任何点在此区域。换句话说，我们可以调整<span class="math inline">\(b\)</span> 的值，使得新的直线 <span class="math inline">\(L&#39;\)</span> 依然可以将 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 分开。</p></li><li><p><strong>无数条平行直线</strong>：由于 <span class="math inline">\(b\)</span>可以在实数范围内任意取值，因此存在无数个不同的 <span class="math inline">\(b\)</span> 值，这意味着存在无数条平行直线可以将<span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 分开。</p></li><li><p><strong>非平行直线</strong>：除了平行直线之外，我们还可以选择不同的斜率<span class="math inline">\(k&#39;\)</span>。对任何新的斜率 <span class="math inline">\(k&#39;\)</span>，只要新的直线方程能够满足分离点集的条件，我们就可以调整其截距<span class="math inline">\(b&#39;\)</span> 使得新的直线 <span class="math inline">\(y=k&#39;x+b&#39;\)</span> 依然可以将 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 分开。</p></li></ol><h5 id="高维空间的推广">高维空间的推广</h5><p>在高维空间中，能够分离点集的“直线”实际上是一个超平面。假设我们有两个点集<span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span>，并且它们是线性可分的，即存在一个超平面<span class="math inline">\(H\)</span>可以将它们分开。超平面方程可以表示为： <span class="math display">\[w\cdot x+b=0\]</span> 其中 <span class="math inline">\(w\)</span> 是法向量，<span class="math inline">\(b\)</span> 是偏置项。</p><ol type="1"><li><strong>存在一个超平面</strong>：假设存在一个超平面 <span class="math inline">\(H\)</span> 可以将 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 分开。</li><li><strong>平行超平面</strong>：对于任何固定的法向量 <span class="math inline">\(w\)</span>，不同的偏置 <span class="math inline">\(b\)</span> 会产生不同的平行超平面。</li><li><strong>无数个平行超平面</strong>：由于 <span class="math inline">\(b\)</span>可以在实数范围内任意取值，因此存在无数个不同的 <span class="math inline">\(b\)</span> 值，这意味着存在无数个平行超平面可以将𝐴<em>A</em> 和 𝐵<em>B</em> 分开。</li><li><strong>非平行超平面</strong>：我们还可以选择不同的法向量 <span class="math inline">\(w&#39;\)</span>。对于任何新的法向量 <span class="math inline">\(w&#39;\)</span>，只要新的超平面方程能够满足分离点集的条件，我们就可以调整其偏置<span class="math inline">\(b\)</span> 使得新的超平面 <span class="math inline">\(w&#39;\cdot x+b&#39;=0\)</span> 依然可以将 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 分开。</li></ol><h4 id="核函数">核函数</h4><ul><li><p>在实际应用时，映射函数 <span class="math inline">\(\varphi(x)\)</span>不需要是已知的。换句话说，核技巧的目的就是在不需要显式地定义特征空间和映射函数的条件下，计算映射之后的内积</p></li><li><p>核函数 <span class="math inline">\(K(x_1,x_2)=&lt;\varphi(x_1),\varphi(x_2)&gt;\)</span>需要满足的充要条件 (不需要 <span class="math inline">\(\varphi(x)\)</span> 已知) (<a href="%5BMercer&#39;s%20theorem%20-%20Wikipedia%5D(https://en.wikipedia.org/wiki/Mercer&#39;s_theorem)">Mercer'sTheorem</a>):</p><ul><li><p>对称性: <span class="math inline">\(K(x_1,x_2)=K(x_2,x_1)\)</span></p></li><li><p>半正定性: 对于任意 <span class="math inline">\(n\)</span> 和任意<span class="math inline">\(x_1,x_2,\dots,x_n∈\mathcal{X}\)</span>，由<span class="math inline">\(K(x_i,x_j)\)</span> 定义的 Gram matrix总是半正定的，即 <span class="math display">\[对于\forall C_i,x_i, (i=1，2,\dots,n),有\sum_{i=1}^N\sum_{j=1}^NC_iC_jK(x_1,x_2)\geq0\ \ \ \ \ \ \ \ \ \ \ \ \ \\ C_i为任意实数，x_i为任意的向量.\]</span></p></li></ul></li><li><p>只要 <span class="math inline">\(K\)</span>是核函数，那么一定存在一个Hilbert space和一个映射函数 <span class="math inline">\(\varphi(x)\)</span>，使得 <span class="math inline">\(K(x_1,x_2)=&lt;\varphi(x_1),\varphi(x_2)&gt;\)</span></p></li><li><p>常见核函数</p><table><colgroup><col style="width: 9%"><col style="width: 47%"><col style="width: 42%"></colgroup><thead><tr class="header"><th>名称</th><th>表达式</th><th>参数</th></tr></thead><tbody><tr class="odd"><td>线性核</td><td><span class="math inline">\(K(x_1,x_2)=x_i^Tx_j\)</span></td><td></td></tr><tr class="even"><td>多项式核</td><td><span class="math inline">\(K(x_1,x_2)=(x_i^Tx_j)^d\)</span></td><td><span class="math inline">\(d\geq1\)</span>为多项式的次数</td></tr><tr class="odd"><td>高斯核</td><td><span class="math inline">\(K(x_1,x_2)=\exp(-\frac{||x_i-x_j||^2}{2\sigma^2})\)</span></td><td><span class="math inline">\(\sigma&gt;0\)</span>，为高斯核的带宽(width)</td></tr><tr class="even"><td>拉普拉斯核</td><td><span class="math inline">\(K(x_1,x_2)=\exp(-\frac{||x_i-x_j||}{\sigma})\)</span></td><td><span class="math inline">\(\sigma&gt;0\)</span></td></tr><tr class="odd"><td>Sigmoid核</td><td><span class="math inline">\(K(x_1,x_2)=\tanh(\betax_i^Tx_j+\theta)\)</span></td><td><span class="math inline">\(\tanh\)</span> 为双曲正切函数，<span class="math inline">\(\beta&gt;0,\ \ \theta&lt;0\)</span></td></tr></tbody></table><h4 id="替代损失函数">替代损失函数</h4><table><colgroup><col style="width: 44%"><col style="width: 55%"></colgroup><thead><tr class="header"><th>名称</th><th>表达式</th></tr></thead><tbody><tr class="odd"><td>hinge损失函数</td><td><span class="math inline">\(\mathscr{l}_{hinge}(z)=\max(0,1-z)\)</span></td></tr><tr class="even"><td>指数损失函数 (exponential loss)</td><td><span class="math inline">\(\mathscr{l}_{exp}(z)=\exp(-z)\)</span></td></tr><tr class="odd"><td>对率损失 (logistic loss)</td><td><span class="math inline">\(\mathscr{l}_{log}(z)=\log(1+\exp(-z))\)</span></td></tr></tbody></table><p><img src="https://imgur.la/images/2024/06/04/surrogate_loss.png" alt="surrogate loss" border="0" style="zoom: 5%;"></p><center><p></p><p>图7：三种常见的替代损失函数: hinge损失、指数损失、对率损失</p><p></p></center></li></ul><h4 id="优化理论原问题与对偶问题">优化理论：原问题与对偶问题</h4><p><strong>1. 原问题 (Prime Problem)</strong></p><ul><li>最小化 (Minimize) : <span class="math inline">\(f(w)\)</span></li><li>限制条件 (Subject to) :<ul><li><span class="math inline">\(g_i(w)\leq0 \ \ \ \(i=1,2,\dots,K)\)</span></li><li><span class="math inline">\(h_i(w)=0 \ \ \ \(i=1,2,\dots,M)\)</span></li></ul></li></ul><p>对原问题使用拉格朗日乘子法可以得到其"对偶问题" <span class="math display">\[\begin{align}&amp;L(w,\alpha,\beta) \\=&amp;f(w)+\sum \limits_{i=1}^{K}\alpha_ig_i(w)+\sum\limits_{i=1}^{M}\beta_ih_i(w) \\=&amp;f(w)+\alpha^Tg(w)+\beta^Th(w)\end{align}\]</span> 根据您上述过程，我们可以得到原问题的对偶问题：</p><p><strong>2. 对偶问题 (Dual Problem)</strong></p><ul><li>最大化 (Maximum) : <span class="math inline">\(\theta(\alpha,\beta)=\inf\{L(w,\alpha,\beta)\}\)</span></li><li>限制条件 (Subject to) :<ul><li><span class="math inline">\(\alpha_i\geq0 \ \ \ \(i=1,2,\dots,K)\)</span></li></ul></li></ul><p><strong>解释：</strong></p><p>其中 <span class="math inline">\(\inf\{L(w,\alpha,\beta)\}\)</span>表示在限定的 <span class="math inline">\(\alpha,\beta\)</span>的条件下，遍历所有的 <span class="math inline">\(w\)</span>，求 <span class="math inline">\(L(w,\alpha,\beta)\)</span>的最小值。因此对于每确定的一组 <span class="math inline">\(\alpha,\beta\)</span> 都可以算出 <span class="math inline">\(L(w,\alpha,\beta)\)</span>的最小值。而我们此时要找的就是所有的能算出 <span class="math inline">\(L(w,\alpha,\beta)\)</span> 最小值的 <span class="math inline">\(\alpha,\beta\)</span> 中最大的那一组 <span class="math inline">\(\alpha,\beta\)</span>。</p><p><strong>3. 将原问题的解转化为对偶问题的解</strong></p><p><strong>定理：</strong>如果 <span class="math inline">\(w^*\)</span>是原问题的解，而<span class="math inline">\(\alpha^*,\beta^*\)</span>是对偶问题的解，则有<span class="math inline">\(f(w^*)\geq\theta(\alpha^*,\beta^*)\)</span></p><p>证明：(其中 <span class="math inline">\(\alpha^*\geq0, \ \g_i(w^*)\leq0, \ \ h_i(w)=0\)</span>) <span class="math display">\[\begin{align}\theta(\alpha^*,\beta^*)&amp;=\inf\{L(w,\alpha^*,\beta^*)\} \\&amp;\leq L(w^*,\alpha^*,\beta^*) \\&amp;=f(w^*)+\sum \limits_{i=1}^{K}\alpha_i^*g_i(w^*)+\sum\limits_{i=1}^{M}\beta_i^*h_i(w^*) \\&amp;\leq f(w^*)\end{align}\]</span> <strong>定义：</strong><span class="math inline">\(G=f(\omega^*)-\theta(\alpha^*,\beta^*)\geq0\)</span>,<span class="math inline">\(G\)</span> 为原问题与对偶问题的间距 (DualityGap)。</p><p><strong>强对偶定理：</strong>若 <span class="math inline">\(f(w)\)</span> 为凸函数，且 <span class="math inline">\(g(w)=aw+b, \ \ h(w)=cw+d\)</span>(即其约束条件均为线性函数), 则此优化问题得原问题与对偶问题的对偶间距<span class="math inline">\(G=0\)</span>.</p><p>由强对偶性定理得： <span class="math display">\[f(w^*)=\theta(\alpha^*,\beta^*)\]</span> 此时意味着，对于 <span class="math inline">\(\foralli=1,2,\dots,K.\)</span> 则有 <span class="math inline">\(\alpha_i^*=0\)</span> 或者 <span class="math inline">\(g_i(w^*)=0\)</span> .<strong>——KKT条件</strong></p><hr><h2 id="references">References</h2><blockquote><p>[1] 周志华，机器学习，清华大学出版社，2016</p><p>[2] Boyd S P, Vandenberghe L. Convex optimization[M]. Cambridgeuniversity press, 2004.</p><p>[3] Cortes C, Vapnik V. Support-vector networks[J]. Machine learning,1995, 20: 273-297.</p><p>[4] Ruszczyński A P. Nonlinear optimization[M]. Princeton universitypress, 2006.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Supervised machine learning </tag>
            
            <tag> Convex optimization </tag>
            
            <tag> Dural Problem </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>markov_process</title>
      <link href="/2023/12/27/markov-process/"/>
      <url>/2023/12/27/markov-process/</url>
      
        <content type="html"><![CDATA[<h2 id="马尔可夫过程-state-probability">马尔可夫过程 (State +Probability)</h2><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Markov_process-example.svg/2880px-Markov_process-example.svg.png" alt="undefined" style="zoom:50%;"></p><h3 id="状态state">状态（State）</h3><p><strong>定义：</strong>具有<strong>马尔可夫性质</strong>的有限随机状态序列<strong><span class="math inline">\(S_1, S_2, ......\)</span></strong></p><p>马尔可夫性质（markovproperty）：当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；</p><p><span class="math inline">\(P(S_{t+1} | S_t) = P(S_{t+1}|S_1, ...,S_t)\)</span> <strong>无后效性</strong></p><p>解释：从<span class="math inline">\(S_1, ...,S_t\)</span>所蕴含的信息与<span class="math inline">\(S_t\)</span>是等价的，当前的状态只由上一个状态所决定，而不受更早之前状态影响。</p><h3 id="转移概率transition-probability">转移概率（TransitionProbability）</h3><p>状态转移矩阵</p><p><img src="/2023/12/27/markov-process/Users\S\AppData\Roaming\Typora\typora-user-images\image-20231228110315549.png" alt="image-20231228110315549" style="zoom:50%;"><img src="/2023/12/27/markov-process/Users\S\AppData\Roaming\Typora\typora-user-images\image-20231228110454720.png" alt="image-20231228110454720" style="zoom:50%;"></p><h2 id="马尔可夫奖励过程state-probabilityreward">马尔可夫奖励过程（State+ Probability+Reward）</h2><p>马尔可夫过程+奖励=马尔可夫奖励过程</p><p>奖励：</p><p>即时奖励：<span class="math inline">\(R_{t+1}\)</span></p><p>其中即时奖励与状态转移相伴随： <span class="math inline">\(R_{t+1}=f(S_t \rightarrow S_{t+1})\)</span></p><p>只要发生状态转移就会产生即时奖励。</p><p>长期奖励：<span class="math inline">\(G_t\)</span></p><p><span class="math inline">\(G_t = R_{t+1}+\gamma R_{t+2}+\gamma ^2R_{t+3}+\dots=\sum \limits_{k=0}^{\infty}\gamma^kR_{t+k+1}\)</span></p><p><span class="math inline">\(\gamma:\)</span>衰减系数，表示即时奖励在当前的折扣值， <span class="math inline">\(\gamma \in [0,1]\)</span></p><p><span class="math inline">\(\gamma\)</span> 越接近0，更喜欢现在的reward；<span class="math inline">\(\gamma\)</span>越接近1， 越不关心现在已经得到的reward。</p><p>长期奖励与状态转移链相伴随， 不同的状态转移链对应不同的长期奖励。</p><p>价值函数：<span class="math inline">\(V(s)\)</span></p><p>评价状态 <span class="math inline">\(S\)</span>的质量，使用以当前状态 <span class="math inline">\(S\)</span>起点的所有状态转移链的 <span class="math inline">\(G_t\)</span>的期望来作为衡量 <span class="math inline">\(S\)</span> 的价值指标 <span class="math display">\[\begin{align}V(S)&amp;=E[G_t | S_t=S]  \\&amp;=E[R_{t+1}+\gamma R_{t+2}+\gamma ^ 2R_{t+3}+\dots | S_t = S] \\&amp;=E[R_{t+1}+(\gamma R_{t+2}+\gamma ^ 2R_{t+3}+\dots) | S_t = S] \\&amp;=E[R_{t+1}+\gamma G_{t+1} | S_t = S] \\&amp;=E[R_{t+1}+\gamma V(S_t+1) | S_t = S]\end{align}\]</span> <span class="math inline">\(V(S)\)</span>取决于前一个状态的奖励 <span class="math inline">\(R_{t+1}\)</span>，也取决于当前状态的</p><p>Bellman Equation</p><h3 id="马尔可夫决策过程-state-probability-reward-action">马尔可夫决策过程（State + Probability + Reward + Action）</h3><p>MDP 元组定义 <span class="math inline">\(\langleS,A,P,R,\gamma\rangle\)</span>：</p><p><span class="math inline">\(S\)</span> 有限状态集</p><p><span class="math inline">\(A\)</span> 有限动作集</p><p><span class="math inline">\(P\)</span> 状态转移概率矩阵， <span class="math inline">\(P_{ss&#39;}^a=P[S_{t+1}=s&#39; | S_t=s,A_t=a]\)</span></p><p><span class="math inline">\(R\)</span> 奖励函数， <span class="math inline">\(R_s^a=E[R_{t+1}|S_t=s, A_t=a]\)</span></p><p><span class="math inline">\(\gamma\)</span> 折扣函数， <span class="math inline">\(\gamma\in[0, 1]\)</span></p><p>目标：找到一条最佳路径，使得这条路径上的Reward最大</p><p>给定状态的动作概率分布（policy）：<span class="math inline">\(\pi\)</span> <span class="math display">\[\pi(a | s)=P[A_t=a|S_t=s]\]</span> 状态价值函数 <span class="math inline">\(v_{\pi}(s)\)</span><span class="math display">\[\begin{align}v_\pi(s) &amp;= E_\pi[G_t | S_t=s] \\&amp;=E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1}) | S_t=s]\end{align}\]</span> 从状态 <span class="math inline">\(s\)</span> 开始，遵循policy<span class="math inline">\(\pi\)</span>，期望获得的累计奖励。</p><p>动作价值函数 <span class="math inline">\(q_\pi(s,a)\)</span> <span class="math display">\[\begin{align}q_\pi(s,a)&amp;=E_\pi [G_t|S_t=s, A_t=a] \\&amp;=E_\pi [R_{t+1}+\gamma q_\pi(S_{t+1}, A_{t+1})|S_t=s, A_t=a] \\&amp;=R_s^a+\gamma \sum \limits_{s&#39;\in S}P_{ss&#39;}^av_\pi(s&#39;)\end{align}\]</span> 从状态 <span class="math inline">\(s\)</span> 开始，执行动作<span class="math inline">\(a\)</span>, 遵循policy <span class="math inline">\(\pi\)</span> 期望获得的累计奖励。</p><p>包含动作的状态转移过程：</p><p><img src="/2023/12/27/markov-process/Users\S\AppData\Roaming\Typora\typora-user-images\image-20231229150527704.png" alt="image-20231229150527704" style="zoom:50%;"><img src="/2023/12/27/markov-process/Users\S\AppData\Roaming\Typora\typora-user-images\image-20231229150542970.png" alt="image-20231229150542970" style="zoom:50%;"><img src="/2023/12/27/markov-process/Users\S\AppData\Roaming\Typora\typora-user-images\image-20231229150558157.png" alt="image-20231229150558157" style="zoom:50%;"><img src="/2023/12/27/markov-process/Users\S\AppData\Roaming\Typora\typora-user-images\image-20231229160323899.png" alt="image-20231229160323899" style="zoom:50%;"></p><p>贝尔曼期望方程求解 <span class="math inline">\(V^\pi\)</span> <span class="math display">\[v_\pi(S)=\sum \limits_{a\in A}\pi(a|s)q_\pi(s,a)\]</span></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/12/08/hello-world/"/>
      <url>/2023/12/08/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
