<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Mamba | Welcome to Junlei's word!</title><meta name="author" content="坚竹韧周"><meta name="copyright" content="坚竹韧周"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Mamba  The Transformer architecture has been a major component in the success of Large Language Models (LLMs). It has been used for nearly all LLMs that are being used today, from open-source mode">
<meta property="og:type" content="article">
<meta property="og:title" content="Mamba">
<meta property="og:url" content="http://junlei-zhou.com/2024/06/19/Mamba/index.html">
<meta property="og:site_name" content="Welcome to Junlei&#39;s word!">
<meta property="og:description" content="Mamba  The Transformer architecture has been a major component in the success of Large Language Models (LLMs). It has been used for nearly all LLMs that are being used today, from open-source mode">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://junlei-zhou.com/img/avatar.jpg">
<meta property="article:published_time" content="2024-06-19T02:35:00.000Z">
<meta property="article:modified_time" content="2024-06-20T09:53:43.748Z">
<meta property="article:author" content="坚竹韧周">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Sequence Model">
<meta property="article:tag" content="State space model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://junlei-zhou.com/img/avatar.jpg"><link rel="shortcut icon" href="/img/URL_LOGO.png"><link rel="canonical" href="http://junlei-zhou.com/2024/06/19/Mamba/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Mamba',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-20 17:53:43'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="Welcome to Junlei's word!" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-blog"></i><span> Library</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Live</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fa fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa fa-gamepad"></i><span> Game</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-paper-plane"></i><span> Comments</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Welcome to Junlei's word!"><span class="site-name">Welcome to Junlei's word!</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-blog"></i><span> Library</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Live</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fa fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa fa-gamepad"></i><span> Game</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-paper-plane"></i><span> Comments</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Mamba</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-06-19T02:35:00.000Z" title="Created 2024-06-19 10:35:00">2024-06-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-06-20T09:53:43.748Z" title="Updated 2024-06-20 17:53:43">2024-06-20</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Mamba"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="mamba">Mamba</h1>
<!--本文是根据Mamba论文原作者撰写的Blog进行阅读的重点解读-->
<p>The Transformer architecture has been a major component in the
success of Large Language Models (LLMs). It has been used for nearly all
LLMs that are being used today, from open-source models like Mistral to
closed-source models like ChatGPT.</p>
<p>To further improve LLMs, new architectures are developed that might
even outperform the Transformer architecture. One of these methods is
<em>Mamba</em>, a <em>State Space Model</em>.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6bc1ca-a387-47ed-a9a6-077af838b359_1148x892.png" style="zoom:50%;" /></p>
<p>Mamba was proposed in the paper <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time Sequence
Modeling with Selective State Spaces</a>.<a
target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-1-141228095">1</a>
You can find its official implementation and model checkpoints in its <a
target="_blank" rel="noopener" href="https://github.com/state-spaces/mamba">repository</a>.</p>
<p>In this post, I will introduce the field of State Space Models in the
context of language modeling and explore concepts one by one to develop
an intuition about the field. Then, we will cover how Mamba might
challenge the Transformers architecture.</p>
<p>As a visual guide, expect many visualizations to develop an intuition
about Mamba and State Space Models!</p>
<h2 id="part-1-the-problem-with-transformers">Part 1: The Problem with
Transformers</h2>
<p>To illustrate why Mamba is such an interesting architecture, let’s do
a short re-cap of transformers first and explore one of its
disadvantages.</p>
<p>A Transformer sees any textual input as a <em>sequence</em> that
consists of <em>tokens</em>.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b8c299a-c0c0-46fe-86cf-b22e08a91b32_1776x544.png" style="zoom:50%;" /></p>
<p>A major benefit of Transformers is that whatever input it receives,
it can look back at any of the earlier tokens in the sequence to derive
its representation.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c01c75-1105-4aeb-a608-f00c85bbe5f7_1776x532.png" style="zoom:50%;" /></p>
<h3 id="the-core-components-of-transformers">The Core Components of
Transformers</h3>
<p>Remember that a Transformer consists of two structures, a set of
encoder blocks for representing text and a set of decoder blocks for
generating text. Together, these structures can be used for several
tasks, including translation.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62a21d60-2e84-4c19-a6fb-d2eff501af1c_1776x952.png" style="zoom:50%;" /></p>
<p>We can adopt this structure to create generative models by using only
decoders. This Transformer-based model, <em>Generative Pre-trained
Transformers</em> (GPT), uses decoder blocks to complete some input
text.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58e51959-d4ef-4fa9-a1c7-dab0e5ca4dc0_1776x1012.png" style="zoom: 50%;" /></p>
<p>Let’s take a look at how that works!</p>
<h3 id="a-blessing-with-training">A Blessing with Training…</h3>
<p>A single decoder block consists of two main components, masked
self-attention followed by a feed-forward neural network.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5af9c5-5266-4c2b-b583-b20a19f19fcc_1776x464.png" style="zoom:50%;" /></p>
<p>Self-attention is a major reason why these models work so well. It
enables an uncompressed view of the entire sequence with fast
training.</p>
<p>So how does it work?</p>
<p>It creates a matrix comparing each token with every token that came
before. The weights in the matrix are determined by how relevant the
token pairs are to one another.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F167cfe80-2863-47c8-a969-cb2eeedbd353_1776x860.png" style="zoom:50%;" /></p>
<p>During training, this matrix is created in one go. The attention
between “<em>My</em>” and “<em>name</em>” does not need to be calculated
first before we calculate the attention between “<em>name</em>” and
“<em>is</em>”.</p>
<p>It enables <strong>parallelization</strong>, which speeds up training
tremendously!</p>
<h3 id="and-the-curse-with-inference">And the Curse with Inference!</h3>
<p>There is a flaw, however. When generating the next token, we need to
re-calculate the attention for the <em>entire sequence</em>, even if we
already generated some tokens.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb66f1965-fc44-4a61-b9c6-912c8120ecad_2420x580.png" style="zoom:50%;" /></p>
<p>Generating tokens for a sequence of length <em>L</em> needs roughly
<em>L²</em> computations which can be costly if the sequence length
increases.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405074ed-aa8c-44b4-88a5-bae1dad0412e_2072x392.png" style="zoom:50%;" /></p>
<p>This need to recalculate the entire sequence is a major bottleneck of
the Transformer architecture.</p>
<p>Let’s look at how a “classic” technique, Recurrent Neural Networks,
solves this problem of slow inference.</p>
<hr />
<blockquote>
<p><strong>为什么Transformer的推理速度会很慢？</strong></p>
<p>因为Transformer在推理当前的状态时，需要计算包含之前所有状态的信息，也就是说，当计算到第N个状态时，需要同时计算包含前N个状态的信息以得出当前状态的取值，这也就时为什么以Transformer为基础的模型在输入时会有长度限制的前提条件。而相比较于Transformer，RNN模型有着比较快的推理速度因为它只用重点关注前面重点的状态，根据此来推理出当前状态，故而其计算速度快，但由于只关注之前的部分状态，也导致了其推理的准确度相比较于Transformer略有不及。</p>
<p>例如：当输入的Batch Size=b, Sequence length=N,那么一个具有 <span
class="math inline">\(l\)</span> 层的Transformer模型的计算量为 <span
class="math inline">\(l*(24bNd^2+4bN^2d)\)</span>, <span
class="math inline">\(d\)</span>
为词向量的维度或者是隐藏层的维度，详细的计算过程见<a
target="_blank" rel="noopener" href="https://blog.csdn.net/v_JULY_v/article/details/133619540">原文</a>。</p>
</blockquote>
<h3 id="are-rnns-a-solution">Are RNNs a Solution?</h3>
<p>Recurrent Neural Networks (RNN) is a sequence-based network. It takes
two inputs at each time step in a sequence, namely the input at time
step <strong><em>t</em></strong> and a hidden state of the previous time
step <strong><em>t-1</em></strong>, to generate the next hidden state
and predict the output.</p>
<p>RNNs have a looping mechanism that allows them to pass information
from a previous step to the next. We can “unfold” this visualization to
make it more explicit.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddc71706-8da8-4c28-921b-675e9164c7ab_2404x872.png" style="zoom:50%;" /></p>
<p>When generating the output, the RNN only needs to consider the
previous hidden state and current input. It prevents recalculating all
previous hidden states which is what a Transformer would do.</p>
<p>In other words, RNNs can do inference fast as it scales linearly with
the sequence length! In theory, it can even have an <em>infinite context
length</em>.</p>
<p>To illustrate, let’s apply the RNN to the input text we have used
before.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2484541-f7b1-4950-b04f-5a3177596fbb_2228x808.png" style="zoom:50%;" /></p>
<p>Each hidden state is the aggregation of all previous hidden states
and is typically a compressed view.</p>
<p>There is a problem, however…</p>
<p>Notice that the last hidden state, when producing the name
“<em>Maarten</em>” does not contain information about the word
“<em>Hello</em>” anymore. RNNs tend to forget information over time
since they only consider one previous state.</p>
<p>This sequential nature of RNNs creates another problem. Training
cannot be done in parallel since it needs to go through each step at a
time sequentially.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819fcf9b-ea31-4954-8496-4b66c5b46dc2_2236x828.png" style="zoom:50%;" /></p>
<p>The problem with RNNs, compared to Transformers, is completely the
opposite! Its inference is incredibly fast but it is not
parallelizable.</p>
<blockquote>
<p><strong>RNN存在的问题</strong></p>
<ol type="1">
<li><p>虽然在RNN中，每个状态都是先前所有隐藏状态的聚合，然而随着时间的推移，RNN会忘记掉一部分信息。</p></li>
<li><p>RNN没办法并行训练，即推理速度快但训练慢，而且因为RNN的结构，也无法进行卷积运算。</p></li>
</ol>
<p>由于RNN的每个时间 <span class="math inline">\(t\)</span>
的输出需要依赖前一个时间 <span class="math inline">\(t-1\)</span> 的
输出，导致其循环操作无法并行训练。</p>
<p>RNN中通过循环结构来实现权重共享，而CNN中通过卷积操作实现局部链接和全局共享。</p>
</blockquote>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7af6befe-e99d-4350-b555-21ce543cae53_2072x580.png" style="zoom:50%;" /></p>
<p>Can we somehow find an architecture that does parallelize training
like Transformers whilst still performing inference that scales linearly
with sequence length?</p>
<p>Yes! This is what Mamba offers but before diving into its
architecture, let’s explore the world of State Space Models first.</p>
<h2 id="part-2-the-state-space-model-ssm">Part 2: The <strong>State
Space Model (SSM)</strong></h2>
<p>A State Space Model (SSM), like the Transformer and RNN, processes
sequences of information, like text but also signals. In this section,
we will go through the basics of SSMs and how they relate to textual
data.</p>
<h3 id="what-is-a-state-space">What is a State Space?</h3>
<p>A State Space contains the minimum number of variables that fully
describe a system. It is a way to mathematically represent a problem by
defining a system's possible states.</p>
<p>Let’s simplify this a bit. Imagine we are navigating through a maze.
The “<em>state space</em>” is the map of all possible locations
(states). Each point represents a unique position in the maze with
specific details, like how far you are from the exit.</p>
<p>The “<em>state space representation</em>” is a simplified description
of this map. It shows where you are (current state), where you can go
next (possible future states), and what changes take you to the next
state (going right or left).</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6480800-2449-456a-87a7-27c8a4e9e718_2520x1388.png" style="zoom:50%;" /></p>
<p>Although State Space Models use equations and matrices to track this
behavior, it is simply a way to track where you are, where you can go,
and how you can get there.</p>
<p>The variables that describe a state, in our example the X and Y
coordinates, as well as the distance to the exit, can be represented as
“<em>state vectors</em>”.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c79eba-2559-4d9d-8999-bee33666f2e3_2364x736.png" style="zoom:50%;" /></p>
<p>Sounds familiar? That is because embeddings or vectors in language
models are also frequently used to describe the “state” of an input
sequence. For instance, a vector of your current position (state vector)
could look a bit like this:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ff8812a-64d2-4fc6-8e54-eb86222333b0_1496x444.png" style="zoom:50%;" /></p>
<p>In terms of neural networks, the “state” of a system is typically its
hidden state and in the context of Large Language Models, one of the
most important aspects of generating a new token.</p>
<p>What is a State Space Model?</p>
<p>SSMs are models used to describe these state representations and make
predictions of what their next state could be depending on some
input.</p>
<p>Traditionally, at time <strong><em>t</em></strong>, SSMs:</p>
<ul>
<li><p>map an input sequence <strong><em>x(t)</em></strong> — (e.g.,
moved left and down in the maze)</p></li>
<li><p>to a latent state representation <strong><em>h(t)</em></strong> —
(e.g., distance to exit and x/y coordinates)</p></li>
<li><p>and derive a predicted output sequence
<strong><em>y(t)</em></strong> — (e.g., move left again to reach the
exit sooner)</p>
<blockquote>
<p><strong>A:</strong> 当前状态如何影响下一状态</p>
<p><strong>B:</strong> 输入序列 <strong>x</strong>
如何影响当前的状态</p>
<p><strong>C:</strong> 当前的状态如何影响当前的输出</p>
<p><strong>D:</strong> 输入序列 <strong>x</strong>
如何影响当前的输出</p>
</blockquote></li>
</ul>
<p>However, instead of using <em>discrete</em> <em>sequences</em> (like
moving left once) it takes as input a <em>continuous</em>
<em>sequence</em> and predicts the output sequence.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5788c3e4-8794-4492-af87-3a45f7a6aa70_1992x624.png" style="zoom:50%;" /></p>
<p>SSMs assume that dynamic systems, such as an object moving in 3D
space, can be predicted from its state at time
<strong><em>t</em></strong> through two equations.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32401c6d-39b6-4619-a75e-6b33d3268bca_2520x388.png" style="zoom:50%;" /></p>
<blockquote>
<p>这里的 <span class="math inline">\(h&#39;(t)\)</span>
是SSM一阶微分方程中的导数，指的是连续状态下的当前状态是由上一状态得出的。</p>
<p>写成离散形态的即为 <span
class="math inline">\(h_t=Ah_{t-1}+Bx_t\)</span>.</p>
<p>通过求解这两个方程，可以根据观察到的数据“输入序列 x 和先前的状态 h(t)
，即可实现对未来状态 y(t) 进行预测。”</p>
</blockquote>
<p>By solving these equations, we assume that we can uncover the
statistical principles to predict the state of a system based on
observed data (input sequence and previous state).</p>
<p>==Its goal is to find this state representation
<strong><em>h(t)</em></strong> such that we can go from an input to an
output sequence.==</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5c7ae-3dbe-44d8-8b13-7f4dcc14a29b_2008x624.png" style="zoom:50%;" /></p>
<p>These two equations are the core of the State Space Model.</p>
<p>The two equations will be referenced throughout this guide. To make
them a bit more intuitive, they are <strong>color-coded</strong> so you
can quickly reference them.</p>
<p>The <strong>state equation</strong> describes how the state changes
(through <em>matrix A</em>) based on how the input influences the state
(through <em>matrix B</em>).</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0876819d-8a46-4187-9826-14391bfd47b9_1796x624.png" style="zoom:50%;" /></p>
<p>As we saw before, <strong><em>h(t)</em></strong> refers to our latent
state representation at any given time <strong><em>t</em></strong>, and
<strong><em>x(t)</em></strong> refers to some input.</p>
<p>The <strong>output equation</strong> describes how the state is
translated to the output (through <em>matrix C</em>) and how the input
influences the output (through <em>matrix D</em>).</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2e87708-9676-4a1c-b32c-d703026f64d9_1796x624.png" style="zoom:50%;" /></p>
<blockquote>
<p><strong>NOTE</strong>: Matrices <em>A</em>, <em>B</em>, <em>C</em>,
and <em>D</em> are also commonly refered to as <em>parameters</em> since
they are learnable.</p>
</blockquote>
<p>Visualizing these two equations gives us the following
architecture:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc358439e-c507-49f1-ac2e-5dedaccc2a8b_1728x364.png" style="zoom:50%;" /></p>
<p>Let’s go through the general technique step-by-step to understand how
these matrices influence the learning process.</p>
<p>Assume we have some input signal <strong><em>x(t)</em></strong>, this
signal first gets multiplied by <em>matrix B</em> which describes how
the inputs influence the system.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd6f8dae-2281-47af-8ba3-06bbdc594d1c_1956x360.png" style="zoom: 67%;" /></p>
<p>The updated state (akin to the hidden state of a neural network) is a
latent space that contains the core “knowledge” of the environment. We
multiply the state with <em>matrix A</em> which describes how all the
internal states are connected as they represent the underlying dynamics
of the system.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cedc98a-d200-4fe4-b311-6d68dcaa50af_1956x572.png" style="zoom:50%;" /></p>
<blockquote>
<p><strong>A</strong>
矩阵决定了整个状态的演化，即如何从一个状态演化到下一个状态。其本质为存储着之前所有历史信息的浓缩精华。</p>
</blockquote>
<p>As you might have noticed, <em>matrix A</em> is applied before
creating the state representations and is updated after the state
representation has been updated.</p>
<p>Then, we use <em>matrix C</em> to describe how the state can be
translated to an output.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8599487f-1023-4069-be7a-8056e63b0574_1956x572.png" style="zoom:50%;" /></p>
<p>Finally, we can make use of <em>matrix D</em> to provide a direct
signal from the input to the output. This is also often referred to as a
<em>skip-connection</em>.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf79721f-5cef-44da-98c5-f63a7839ebc3_1956x756.png" style="zoom:50%;" /></p>
<p>Since <em>matrix D</em> is similar to a skip-connection, the SSM is
often regarded as the following without the skip-connection.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca1d511-7d31-42a0-9220-2fb85b256efd_1956x864.png" style="zoom:50%;" /></p>
<p>Going back to our simplified perspective, we can now focus on
matrices <em>A</em>, <em>B</em>, and <em>C</em> as the core of the
SSM.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e52f4f0-d7ad-453d-a741-6dfa4a998964_1728x352.png" style="zoom:50%;" /></p>
<p>We can update the original equations (and add some pretty colors) to
signify the purpose of each matrix as we did before.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55df8ede-3a16-4473-8ea9-872fe199d3a1_1904x676.png" style="zoom:50%;" /></p>
<p>Together, these two equations aim to predict the state of a system
from observed data. Since the input is expected to be continuous, the
main representation of the SSM is a <strong>continuous-time
representation</strong>.</p>
<h3 id="from-ssm-to-s4">From SSM to S4</h3>
<h4 id="from-a-continuous-to-a-discrete-signal">From a Continuous to a
Discrete Signal</h4>
<p>Finding the state representation <strong><em>h(t)</em></strong> is
analytically challenging if you have a continuous signal. Moreover,
since we generally have a discrete input (like a textual sequence), we
want to discretize the model.</p>
<p>To do so, we make use of the <em>Zero-order hold technique.</em> It
works as follows.</p>
<ol type="1">
<li>First, every time we receive a discrete signal, we hold its value
until we receive a new discrete signal. This process creates a
continuous signal the SSM can use:</li>
</ol>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a2ffb18-2e66-4135-9888-98e9ab88d0d8_1488x472.png" style="zoom:50%;" /></p>
<ol start="2" type="1">
<li><p>How long we hold the value is represented by a new learnable
parameter, called the <em>step size</em> <span
class="math inline">\(\Delta\)</span> . It represents the resolution of
the input.</p></li>
<li><p>Now that we have a continuous signal for our input, we can
generate a continuous output and only sample the values according to the
time steps of the input.</p></li>
</ol>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F042ff699-81af-4479-b99f-92e4997c4c81_1488x476.png" style="zoom:50%;" /></p>
<p>These sampled values are our discretized output!</p>
<p>Mathematically, we can apply the <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Zero-order_hold">Zero-order hold</a>
as follows:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6df4b59-6f76-4f13-a201-7b69e59df164_6200x1176.png" style="zoom:50%;" /></p>
<p>Together, they allow us to go from a continuous SSM to a discrete SSM
represented by a formulation that instead of a
<em>function-to-function</em>, <span
class="math inline">\(x(t)\rightarrow y(t)\)</span>, is now a
<em>sequence-to-sequence</em>, <span
class="math inline">\(x_k\rightarrow y_k\)</span>:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc29cfbbb-ae41-4dc2-b899-9e0a81cba34d_1980x1012.png" style="zoom:50%;" /></p>
<p>Here, matrices <strong><em>A</em></strong> and
<strong><em>B</em></strong> now represent discretized parameters of the
model.</p>
<p>We use <strong><em>k</em></strong> instead of
<strong><em>t</em></strong> to represent discretized timesteps and to
make it a bit more clear when we refer to a continuous versus a discrete
SSM.</p>
<blockquote>
<p><strong>NOTE:</strong> We are still saving the continuous form of
<em>Matrix A</em> and not the discretized version during training.
During training, the continuous representation is discretized.</p>
</blockquote>
<p>Now that we have a formulation of a discrete representation, let’s
explore how we can actually <em>compute</em> the model.</p>
<h4 id="the-recurrent-representation">The Recurrent Representation</h4>
<p>Our discretized SSM allows us to formulate the problem in specific
timesteps instead of continuous signals. A recurrent approach, as we saw
before with RNNs is quite useful here.</p>
<p>If we consider discrete timesteps instead of a continuous signal, we
can reformulate the problem with timesteps:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b70ba4-b068-44d1-8641-9b224d103c51_1980x548.png" style="zoom:50%;" /></p>
<p>At each timestep, we calculate how the current input (<strong><span
class="math inline">\(Bx_k\)</span></strong>) influences the previous
state (<strong><span class="math inline">\(Ah_{k-1}\)</span></strong>)
and then calculate the predicted output (<strong><span
class="math inline">\(Ch_k\)</span></strong>).</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb4d0412-87fb-4507-bedb-4793588bd465_2116x788.png" style="zoom:50%;" /></p>
<p>This representation might already seem a bit familiar! We can
approach it the same way we did with the RNN as we saw before.</p>
<blockquote>
<p>对于 <span class="math inline">\(y_2\)</span> 展开计算则有 <span
class="math display">\[
\begin{align}
y_2&amp;=Ch_2 \\
&amp;=C(\bar{A}h_1+\bar{B}x_2) \\
&amp;=C(\bar{A}(\bar{A}h_0+Bx_1)+\bar{B}x_2) \\
&amp;=C(\bar{A}(\bar{A} \cdot \bar{B}x_0 +Bx_1)+\bar{B}x_2) \\
&amp;=C\cdot \bar{A}^2\cdot \bar{B}x_0+C\cdot \bar{A}\cdot
\bar{B}x_1+C\cdot \bar{B}x_2
\end{align}
\]</span> 由此可推得 <span class="math inline">\(y_k=C\cdot \bar{A}^k
\cdot \bar{B}x_0+C\cdot \bar{A}^{k-1} \cdot \bar{B}x_1+\cdots+C\cdot
\bar{A} \cdot \bar{B}x_k\)</span></p>
</blockquote>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91ca51f7-9b9b-4f17-bccb-32a5a96f3339_2184x868.png" style="zoom:50%;" /></p>
<p>Which we can unfold (or unroll) as such:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1084e8a-a70d-450b-beb0-f18117ade5ed_2184x876.png" style="zoom:50%;" /></p>
<p>Notice how we can use this discretized version using the underlying
methodology of an RNN.</p>
<p>This technique gives us both the advantages and disadvantages of an
RNN, namely fast inference and slow training.</p>
<h4 id="the-convolution-representation">The Convolution
Representation</h4>
<p>Another representation that we can use for SSMs is that of
convolutions. Remember from classic image recognition tasks where we
applied filters (<em>kernels</em>) to derive aggregate features:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47f05950-bfad-4013-b854-679c9a47ada9_3216x2144.png" style="zoom:50%;" /></p>
<p>Since we are dealing with text and not images, we need a
1-dimensional perspective instead:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb943872f-de72-43e8-b2f1-cb8213f120a3_3216x1296.png" style="zoom:50%;" /></p>
<p>The kernel that we use to represent this “filter” is derived from the
SSM formulation:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05049821-2352-4c04-8fb2-07fe15c20a9c_2620x824.png" style="zoom:50%;" /></p>
<blockquote>
<p><strong>为什么可以写成卷积的形式？</strong></p>
<p>如上图，其中 <span class="math inline">\(K\)</span>
为卷积核，其中此时的 <strong>A、B、C</strong> 都是常数，可以看出卷积核
<span class="math inline">\(K\)</span>
是通过固定数值矩阵得到的，只需确定 <strong>A、B、C</strong>
即可并行运算。</p>
</blockquote>
<p>Let’s explore how this kernel works in practice. Like convolution, we
can use our SSM kernel to go over each set of tokens and calculate the
output:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9007d03b-c1c9-4b37-8c83-f27bfe8318f4_2620x1080.png" style="zoom:50%;" /></p>
<p>This also illustrates the effect padding might have on the output. I
changed the order of padding to improve the visualization but we often
apply it at the end of a sentence.</p>
<p>In the next step, the kernel is moved once over to perform the next
step in the calculation:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ed71fb-f237-4173-bb23-bd1bf02ff123_2620x1080.png" style="zoom:50%;" /></p>
<p>In the final step, we can see the full effect of the kernel:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4387b79-5e92-4fe2-8b30-2abf112f5a73_2620x1080.png" style="zoom:50%;" /></p>
<p>A major benefit of representing the SSM as a convolution is that it
can be trained in parallel like Convolutional Neural Networks (CNNs).
However, due to the fixed kernel size, their inference is not as fast
and unbounded as RNNs.</p>
<h4 id="the-three-representations">The Three Representations</h4>
<p>These three representations, <em>continuous</em>, <em>recurrent</em>,
and <em>convolutional</em> all have different sets of advantages and
disadvantages:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F682187d6-f402-44aa-8097-8a2e5b6179a7_2072x744.png" style="zoom:50%;" /></p>
<p>Interestingly, we now have efficient inference with the recurrent SSM
and parallelizable training with the convolutional SSM.</p>
<blockquote>
<ol type="1">
<li>在训练的时候怎么使用CNN进行训练？</li>
</ol>
<p><span class="math display">\[
y=\bar{K}x
\]</span></p>
<p>​ 能够通过使用卷积的方法使其进行并行计算。</p>
<ol start="2" type="1">
<li>在推理的时候怎么进行推理？ <span class="math display">\[
\left\{ \begin{array}{lcl}
h_k=\bar{A}h_{k-1}+\bar{B}x_k \\
y_k=Ch_k+Dx_k
\end{array}\right.
\]</span></li>
</ol>
</blockquote>
<p>With these representations, there is a neat trick that we can use,
namely choose a representation depending on the task. During training,
we use the convolutional representation which can be parallelized and
during inference, we use the efficient recurrent representation:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c43c82d-9735-4d55-97bb-8ad6f504909e_1960x1008.png" style="zoom:50%;" /></p>
<p>This model is referred to as the <a
target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/05546b0e38ab9175cd905eebcc6ebb76-Abstract.html">Linear
State-Space Layer (LSSL)</a>.<a
target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-2-141228095">2</a></p>
<p>These representations share an important property, namely that of
<strong><em>Linear Time Invariance</em></strong> (LTI). LTI states that
the SSMs parameters, <em>A</em>, <em>B</em>, and <em>C</em>, are fixed
for all timesteps. This means that matrices <em>A</em>, <em>B</em>, and
<em>C</em> are the same for every token the SSM generates.</p>
<p>In other words, regardless of what sequence you give the SSM, the
values of <em>A</em>, <em>B</em>, and <em>C</em> remain the same. We
have a static representation that is not content-aware.</p>
<p>Before we explore how Mamba addresses this issue, let’s explore the
final piece of the puzzle, <em>matrix A</em>.</p>
<h4 id="the-importance-of-matrix-a">The Importance of Matrix
<em>A</em></h4>
<blockquote>
<p><strong>A</strong>
矩阵决定了整个状态的演化，即如何从一个状态演化到下一个状态。其本质为存储着之前所有历史信息的浓缩精华，
<strong>A</strong> 决定了系统的动态特性，但 <strong>A</strong>
可能存在一个问题，即 <strong>A</strong>
只记住了之前的部分状态，像RNN那样忘记了部分状态，从而会导致SSM在性能上与RNN相似。</p>
<p><strong>A</strong> 设计的关键是如何在有限的空间中保留更多的记忆！</p>
</blockquote>
<p>Arguably one of the most important aspects of the SSM formulation is
<em>matrix A</em>. As we saw before with the recurrent representation,
it captures information about the <em>previous</em> state to build the
<em>new</em> state.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07542fb1-d4b6-421e-8b2a-a3f0a7790939_2028x876.png" style="zoom:50%;" /></p>
<p>In essence, <em>matrix</em> <em>A</em> produces the hidden state:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47635355-6b9a-4981-af3a-7ee6a12b87b3_1412x468.png" style="zoom:50%;" /></p>
<p>Creating <em>matrix A</em> can therefore be the difference between
remembering only a few previous tokens and capturing every token we have
seen thus far. Especially in the context of the Recurrent representation
since it only <em>looks back</em> <em>at the previous state</em>.</p>
<p>So how can we create <em>matrix A</em> in a way that retains a large
memory (context size)?</p>
<p>We use Hungry Hungry Hippo! Or <a
target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2020/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html">HiPPO</a><a
target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-3-141228095">3</a>
for <strong>Hi</strong>gh-order <strong>P</strong>olynomial
<strong>P</strong>rojection <strong>O</strong>perators. HiPPO attempts
to compress all input signals it has seen thus far into a vector of
coefficients.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07985a64-fc26-4ee8-9ec2-c488e4cb709a_1492x488.png" style="zoom:50%;" /></p>
<p>It uses <em>matrix A</em> to build a state representation that
captures recent tokens well and decays older tokens. Its formula can be
represented as follows:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bc7c768-7f0c-4983-a21e-70a4f587e6aa_2520x628.png" style="zoom:50%;" /></p>
<p>Assuming we have a square <em>matrix <strong>A</strong></em>, this
gives us:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8f5de9-6448-43d5-9878-c8cd1d938b7c_1436x708.png" style="zoom:50%;" /></p>
<p>Building <em>matrix A</em> using HiPPO was shown to be much better
than initializing it as a random matrix. As a result, it more accurately
reconstructs <em>newer</em> signals (recent tokens) compared to
<em>older</em> signals (initial tokens).</p>
<p>The idea behind the HiPPO Matrix is that it produces a hidden state
that memorizes its history.</p>
<p>Mathematically, it does so by tracking the coefficients of a <a
target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html">Legendre
polynomial</a> which allows it to approximate all of the previous
history.<a
target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-4-141228095">4</a></p>
<p>HiPPO was then applied to the recurrent and convolution
representations that we saw before to handle long-range dependencies.
The result was <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.00396">Structured
State Space for Sequences (S4)</a>, a class of SSMs that can efficiently
handle long sequences.<a
target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-5-141228095">5</a></p>
<p>It consists of three parts:</p>
<ul>
<li><p>State Space Models</p></li>
<li><p>HiPPO for handling <strong>long-range
dependencies</strong></p></li>
<li><p>Discretization for creating <strong>recurrent</strong> and
<strong>convolution</strong> representations</p></li>
</ul>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb055ec5-f8c7-4862-ab88-f4fb38abf042_1892x844.png" style="zoom:50%;" /></p>
<p>This class of SSMs has several benefits depending on the
representation you choose (recurrent vs. convolution). It can also
handle long sequences of text and store memory efficiently by building
upon the HiPPO matrix.</p>
<blockquote>
<p><strong>NOTE</strong>: If you want to dive into more of the technical
details on how to calculate the HiPPO matrix and build a S4 model
yourself, I would HIGHLY advise going through the <a
target="_blank" rel="noopener" href="https://srush.github.io/annotated-s4/">Annotated S4</a>.</p>
<p>其中在S4中对 <strong>A</strong> 进行了改进：</p>
<p><strong>Theorem 1.</strong> All <em>HiPPO</em> matrices have a
<em>Normal Plus Low-Rank</em> (NPLR) representation <span
class="math display">\[
A=VAV^*-PQ^\top=V(\Lambda-(V^*P)(V^*Q))V^*
\]</span> for unitary <span class="math inline">\(V\in
\mathbb{C}^{N\times N}\)</span>, diagonal <span
class="math inline">\(\Lambda\)</span>， and low-rank factorization
<span class="math inline">\(P,Q\in\mathbb{R}^{N\times r}\)</span>. These
matrices <em>HiPPO-LegS, LegT, LagT</em> all satisfy <span
class="math inline">\(r=1 \ or \ r=2\)</span>.</p>
</blockquote>
<hr />
<h3 id="introduction-of-s4">Introduction of S4</h3>
<h4
id="improving-transformer-struggles-with-handling-very-long-sequences">Improving
transformer struggles with handling very long sequences</h4>
<p>序列数据一般都是离散的数据 比如文本、图、DNA</p>
<ol type="1">
<li>但现实生活中还有很多连续的数据，比如音频、视频，对于音视频这种信号而言，其一个重要特点就是有极长的context
window</li>
<li>而在transformer长context上往往会失败，或者注意力机制在有着超长上下文长度的任务上并不擅长，而S4擅长这类任务。</li>
</ol>
<h4
id="the-definition-and-derivation-of-hippostate-compresses-the-history-of-input">The
definition and derivation of HiPPO：State compresses the history of
input</h4>
<p>我们已经知道一个RNN网络更擅长于处理这种序列数据，但是RNN网络最大的缺点是由于它的hidden
state记忆能力有限，导致其会忘记掉一些以前的特征。</p>
<p>==关键：怎样改善RNN记忆有限的问题？==</p>
<p>假设我们在 <span class="math inline">\(t_0\)</span>
时刻有接收到袁术输入信号 <span
class="math inline">\(u(t)\)</span>之前的部分：</p>
<ol type="1">
<li><p>我们希望使用一个记忆单元去压缩之前这段的输入部分来学习特征，我们使用一个多项式去近似这段之前的输入
<span class="math display">\[
x(t_0)=\begin{bmatrix} 0.1 \\
-1.1 \\
3.7 \\
2.5
\end{bmatrix}
\]</span></p></li>
<li><p>当我们在接收更多的signal的时候，我们仍然希望这个记忆单元能够对已经接收到的所有的signal进行压缩，因此需要自动更新多项式的各项系数，如下图所示</p>
<p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_1.png" alt="Mamba HiPPO 1" border="0" style="zoom:50%;" ></p></li>
<li><p>此时，HiPPO的关键问题就变成了一个优化问题</p>
<ul>
<li><p>如何能够找到这些最优的近似？</p></li>
<li><p>如何快速的更新这些多项式的参数？</p></li>
</ul>
<p>因此，我们需要定义一个指标去判断一个近似的好坏程度。</p>
<p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_2.png" alt="Mamba HiPPO 2" border="0" style="zoom:50%;" ></p>
<p>(Note: 添加推导过程)</p></li>
<li><p>HiPPO的定义，两个矩阵 (<strong>A</strong>
表示之前的状态怎样影响当前的状态，<strong>B</strong>当前的输入怎样影响当前的状态)与两个信号
( <span class="math inline">\(x(t)\)</span> 当前状态之前接收到的signal与
<span class="math inline">\(u(t)\)</span> 当前接收到的signal)</p>
<p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_3.png" alt="Mamba HiPPO 3" border="0" style="zoom:50%;" ></p>
<blockquote>
<p>需要澄清一点的是作者在这里使用了State space
Model中状态的定义，与本文前面部分的定义有些许不同. 但事实上这里的 <span
class="math inline">\(x(t)\)</span> 就等同于上文中的 <span
class="math inline">\(h(t)\)</span> 这里的 <span
class="math inline">\(u(t)\)</span> 就等同于上文中的 <span
class="math inline">\(x(t)\)</span>
。我们更换一下这个表示即可转变为上文中的形式 <span
class="math display">\[
x&#39;(t)=Ax(t)+Bu(t) \Longrightarrow h&#39;(t)=Ah(t)+Bx(t)
\]</span> 其中 <strong>A</strong> 就是HiPPO matrix.</p>
</blockquote></li>
<li><p>HiPPO就相当于把一个高维的复杂函数映射压缩成一个简单的函数，这样既保留了基本信息，又节省了空间。如下图所示，<span
class="math inline">\(u(t)\)</span> 是原始输入的signal，<span
class="math inline">\(x(t)\)</span> 是经过压缩后产生到的
signal，即对应上文中的 <span class="math inline">\(h(t)\)</span>.</p>
<p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_4.png" alt="Mamba HiPPO 4" border="0" style="zoom:50%;" ></p></li>
<li></li>
</ol>
<hr />
<h2 id="part-3-mamba---a-selective-ssm">Part 3: Mamba - A Selective
SSM</h2>
<p>We finally have covered all the fundamentals necessary to understand
what makes Mamba special. State Space Models can be used to model
textual sequences but still have a set of disadvantages we want to
prevent.</p>
<p>In this section, we will go through Mamba’s two main
contributions:</p>
<ol type="1">
<li><p>A <strong>selective scan algorithm</strong>, which allows the
model to filter (ir)relevant information</p></li>
<li><p>A <strong>hardware-aware algorithm</strong> that allows for
efficient storage of (intermediate) results through <em>parallel
scan</em>, <em>kernel fusion</em>, and <em>recomputation</em>.</p></li>
</ol>
<p>Together they create the <em>selective SSM</em> or <em>S6</em> models
which can be used, like self-attention, to create <em>Mamba
blocks</em>.</p>
<p>Before exploring the two main contributions, let’s first explore why
they are necessary.</p>
<h3 id="what-problem-does-it-attempt-to-solve">What Problem does it
attempt to Solve?</h3>
<p>State Space Models, and even the S4 (Structured State Space Model),
perform poorly on certain tasks that are vital in language modeling and
generation, namely <em>the ability to focus on or ignore particular
inputs</em>.</p>
<p>We can illustrate this with two synthetic tasks, namely
<strong>selective copying</strong> and <strong>induction
heads</strong>.</p>
<p>In the <strong>selective copying</strong> task, the goal of the SSM
is to copy parts of the input and output them in order:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f1d1fb-7603-4f86-a2e4-88e0496a1f08_2120x464.png" style="zoom:50%;" /></p>
<p>However, a (recurrent/convolutional) SSM performs poorly in this task
since it is <strong><em>Linear Time Invariant</em>.</strong> As we saw
before, the matrices <em>A</em>, <em>B</em>, and <em>C</em> are the same
for every token the SSM generates.</p>
<p>As a result, an SSM cannot perform <em>content-aware reasoning</em>
since it treats each token equally as a result of the fixed A, B, and C
matrices. This is a problem as we want the SSM to reason about the input
(prompt).</p>
<p>The second task an SSM performs poorly on is <strong>induction
heads</strong> where the goal is to reproduce patterns found in the
input:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27775742-18d2-4f2c-b792-5706976e75e3_2120x744.png" style="zoom:50%;" /></p>
<p>In the above example, we are essentially performing one-shot
prompting where we attempt to “teach” the model to provide an
“<strong><em>A:</em></strong>” response after every
“<strong><em>Q:</em></strong>”. However, since SSMs are time-invariant
it cannot select which previous tokens to recall from its history.</p>
<p>Let’s illustrate this by focusing on <em>matrix B</em>. Regardless of
what the input <strong><em>x</em></strong> is, <em>matrix B</em> remains
exactly the same and is therefore independent of
<strong><em>x</em></strong>:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ee2bd7a-b99b-4871-8396-69cb7dd13cf5_1480x808.png" style="zoom:50%;" /></p>
<p>Likewise, <em>A</em> and <em>C</em> also remain fixed regardless of
the input. This demonstrates the <em>static</em> nature of the SSMs we
have seen thus far.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fea51ca-8458-4216-bf1c-c880a23504b4_1412x484.png" style="zoom:50%;" /></p>
<p>In comparison, these tasks are relatively easy for Transformers since
they <em>dynamically</em> change their attention based on the input
sequence. They can selectively “look” or “attend” at different parts of
the sequence.</p>
<p>The poor performance of SSMs on these tasks illustrates the
underlying problem with time-invariant SSMs, the static nature of
matrices <em>A</em>, <em>B</em>, and <em>C</em> results in problems with
<em>content-awareness</em>.</p>
<h3 id="selectively-retain-information">Selectively Retain
Information</h3>
<p>The recurrent representation of an SSM creates a small state that is
quite efficient as it compresses the entire history. However, compared
to a Transformer model which does no compression of the history (through
the attention matrix), it is much less powerful.</p>
<p>Mamba aims to have the best of both worlds. A small state that is as
powerful as the state of a Transformer:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84b8a71a-6310-416b-8622-e9166593171e_1540x464.png" style="zoom:50%;" /></p>
<p>As teased above, it does so by compressing data selectively into the
state. When you have an input sentence, there is often information, like
stop words, that does not have much meaning.</p>
<p>To selectively compress information, we need the parameters to be
dependent on the input. To do so, let’s first explore the dimensions of
the input and output in an SSM during training:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9376222e-b232-4458-a72c-bd741d8a031c_1436x500.png" style="zoom:50%;" /></p>
<p>In a Structured State Space Model (S4), the matrices <em>A</em>,
<em>B</em>, and <em>C</em> are independent of the input since their
dimensions <strong><em>N</em></strong> and <strong><em>D</em></strong>
are static and do not change.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e93b701-df74-4954-be5c-c0d83779d3df_1412x532.png" style="zoom:50%;" /></p>
<p>Instead, Mamba makes matrices <em>B</em> and <em>C,</em> and even the
<em>step size</em> <strong>∆</strong><em>,</em> dependent on the input
by incorporating the sequence length and batch size of the input:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdccefffd-5712-45ab-9821-c794bce65d7d_1412x596.png" style="zoom:50%;" /></p>
<p>This means that for every input token, we now have different
<em>B</em> and <em>C</em> matrices which solves the problem with
content-awareness!</p>
<blockquote>
<p><strong>NOTE</strong>: Matrix <em>A</em> remains the same since we
want the state itself to remain static but the way it is influenced
(through <em>B</em> and <em>C</em>) to be dynamic.</p>
</blockquote>
<p>Together, they <em>selectively</em> choose what to keep in the hidden
state and what to ignore since they are now dependent on the input.</p>
<p>A smaller <em>step size</em> <strong>∆</strong> results in ignoring
specific words and instead using the previous context more whilst a
larger <em>step size</em> <strong>∆</strong> focuses on the input words
more than the context:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06b21aab-aa32-450a-ae02-b976a2c9f9d8_2520x616.png" style="zoom:50%;" /></p>
<h3 id="the-scan-operation">The Scan Operation</h3>
<p>Since these matrices are now <em>dynamic</em>, they cannot be
calculated using the convolution representation since it assumes a
<em>fixed</em> kernel. We can only use the recurrent representation and
lose the parallelization the convolution provides.</p>
<p>To enable parallelization, let’s explore how we compute the output
with recurrency:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed3ad33b-7b7f-4b69-b28e-8437c7b71e2e_1540x536.png" style="zoom:50%;" /></p>
<p>Each state is the sum of the previous state (multiplied by
<em>A</em>) plus the current input (multiplied by <em>B</em>). This is
called a <em>scan operation</em> and can easily be calculated with a for
loop.</p>
<p>Parallelization, in contrast, seems impossible since each state can
only be calculated if we have the previous state. Mamba, however, makes
this possible through the <em><a
target="_blank" rel="noopener" href="https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda">parallel
scan</a></em> algorithm.</p>
<p>It assumes the order in which we do operations does not matter
through the associate property. As a result, we can calculate the
sequences in parts and iteratively combine them:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F191fdabe-6b38-4e6b-a9f0-8240feef0a9d_1640x1100.png" style="zoom:50%;" /></p>
<p>Together, dynamic matrices <em>B</em> and <em>C</em>, and the
parallel scan algorithm create the <strong><em>selective scan
algorithm</em></strong> to represent the dynamic and fast nature of
using the recurrent representation.</p>
<h3 id="hardware-aware-algorithm">Hardware-aware Algorithm</h3>
<p>A disadvantage of recent GPUs is their limited transfer (IO) speed
between their small but highly efficient SRAM and their large but
slightly less efficient DRAM. Frequently copying information between
SRAM and DRAM becomes a bottleneck.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a1d4fa3-526d-488e-8768-c7208f018eb4_1728x300.png" style="zoom:50%;" /></p>
<p>Mamba, like Flash Attention, attempts to limit the number of times we
need to go from DRAM to SRAM and vice versa. It does so through
<em>kernel fusion</em> which allows the model to prevent writing
intermediate results and continuously performing computations until it
is done.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F563b3792-701a-42b1-9b68-7b6457f5e63d_1728x344.png" style="zoom:50%;" /></p>
<p>We can view the specific instances of DRAM and SRAM allocation by
visualizing Mamba’s base architecture:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F724eceb1-4356-4ac5-b44e-f7fabce3b472_1728x580.png" style="zoom:50%;" /></p>
<p>Here, the following are fused into one kernel:</p>
<ul>
<li><p>Discretization step with <em>step size</em>
<strong>∆</strong></p></li>
<li><p>Selective scan algorithm</p></li>
<li><p>Multiplication with <em>C</em></p></li>
</ul>
<p>The last piece of the hardware-aware algorithm is
<em>recomputation</em>.</p>
<p>The intermediate states are not saved but are necessary for the
backward pass to compute the gradients. Instead, the authors recompute
those intermediate states <em>during</em> the backward pass.</p>
<p>Although this might seem inefficient, it is much less costly than
reading all those intermediate states from the relatively slow DRAM.</p>
<p>We have now covered all components of its architecture which is
depicted using the following image from its article:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc840fb8-2e24-4103-95c8-afa306ce0cfc_2409x743.png" style="zoom:50%;" /></p>
<p><strong>The Selective SSM.</strong> Retrieved from: Gu, Albert, and
Tri Dao. "Mamba: Linear-time sequence modeling with selective state
spaces." <em>arXiv preprint arXiv:2312.00752</em> (2023).</p>
<p>This architecture is often referred to as a <strong><em>selective
SSM</em></strong> or <strong><em>S6</em></strong> model since it is
essentially an S4 model computed with the selective scan algorithm.</p>
<h3 id="the-mamba-block">The Mamba Block</h3>
<p>The <em>selective SSM</em> that we have explored thus far can be
implemented as a block, the same way we can represent self-attention in
a decoder block.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb76db77-1dba-42bd-8de4-4bce240ff67e_1776x1012.png" style="zoom:50%;" /></p>
<p>Like the decoder, we can stack multiple Mamba blocks and use their
output as the input for the next Mamba block:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc94d349d-8620-45a9-8095-7c27de8b7865_1660x1356.png" style="zoom:50%;" /></p>
<p>It starts with a linear projection to expand upon the input
embeddings. Then, a convolution before the <em>Selective SSM</em> is
applied to prevent independent token calculations.</p>
<p>The <em>Selective SSM</em> has the following properties:</p>
<ul>
<li><p><em>Recurrent SSM</em> created through
<em>discretization</em></p></li>
<li><p><em>HiPPO</em> initialization on matrix <em>A</em> to capture
<em>long-range dependencies</em></p></li>
<li><p>S<em>elective scan algorithm</em> to selectively compress
information</p></li>
<li><p><em>Hardware-aware algorithm</em> to speed up
computation</p></li>
</ul>
<p>We can expand on this architecture a bit more when looking at the
code implementation and explore how an end-to-end example would look
like:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa67d7341-9a43-4c67-aa88-6e802c0902ae_1660x2040.png" style="zoom:50%;" /></p>
<p>Notice some changes, like the inclusion of normalization layers and
softmax for choosing the output token.</p>
<p>When we put everything together, we get both fast inference and
training and even unbounded context!</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe528e5fa-0dd8-4e6c-b31a-5248aaee6c68_2072x912.png" style="zoom:50%;" /></p>
<p>Using this architecture, the authors found it matches and sometimes
even exceeds the performance of Transformer models of the same size!</p>
<h2 id="conclusion"><strong>Conclusion</strong></h2>
<p>This concludes our journey in State Space Models and the incredible
Mamba architecture using a selective State Space Model. Hopefully, this
post gives you a better understanding of the potential of State Space
Models, particularly Mamba. Who knows if this is going to replace the
Transformers but for now, it is incredible to see such different
architectures getting well-deserved attention!</p>
<h2 id="resources">Resources</h2>
<p>Hopefully, this was an accessible introduction to Mamba and State
Space Models. If you want to go deeper, I would suggest the following
resources:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://srush.github.io/annotated-s4/">The Annotated S4</a>
is a JAX implementation and guide through the S4 model and is highly
advised!</li>
<li>A great <a
target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ouF-H35atOY">YouTube video</a>
introducing Mamba by building it up through foundational papers.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/state-spaces/mamba">The Mamba
repository</a> with <a
target="_blank" rel="noopener" href="https://huggingface.co/state-spaces">checkpoints on Hugging
Face</a>.</li>
<li>An amazing series of blog posts (<a
target="_blank" rel="noopener" href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-1">1</a>, <a
target="_blank" rel="noopener" href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-2">2</a>, <a
target="_blank" rel="noopener" href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-3">3</a>)
that introduces the S4 model.</li>
<li>The <a
target="_blank" rel="noopener" href="https://jameschen.io/jekyll/update/2024/02/12/mamba.html">Mamba
No. 5 (A Little Bit Of...)</a> blog post is a great next step to dive
into more technical details about Mamba but still from an amazingly
intuitive perspective.</li>
<li>And of course, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.00752">the Mamba
paper</a>! It was even used for DNA modeling and speech generation.</li>
</ul>
<hr />
<h2 id="references">References</h2>
<blockquote>
<p>[1]
原Blog链接：https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state</p>
<p>[2] 一文通透想颠覆Transformer的Mamba：从SSM、HiPPO、S4到Mamba
https://blog.csdn.net/v_JULY_v/article/details/134923301</p>
<p>[2] Gu, A., &amp; Dao, T. (2023). Mamba: Linear-time sequence
modeling with selective state spaces. <em>arXiv preprint
arXiv:2312.00752</em>.</p>
<p>[3]</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://junlei-zhou.com">坚竹韧周</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://junlei-zhou.com/2024/06/19/Mamba/">http://junlei-zhou.com/2024/06/19/Mamba/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a><a class="post-meta__tags" href="/tags/Sequence-Model/">Sequence Model</a><a class="post-meta__tags" href="/tags/State-space-model/">State space model</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq,email,copy_link"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/20/symbols/" title="symbols"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">symbols</div></div></a></div><div class="next-post pull-right"><a href="/2024/05/29/Support-Vector-Machine/" title="Support-Vector-Machine"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">Support-Vector-Machine</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">坚竹韧周</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/JunLei01"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/JunLei01" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:zjunlei1225@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#mamba"><span class="toc-number">1.</span> <span class="toc-text">Mamba</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#part-1-the-problem-with-transformers"><span class="toc-number">1.1.</span> <span class="toc-text">Part 1: The Problem with
Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-core-components-of-transformers"><span class="toc-number">1.1.1.</span> <span class="toc-text">The Core Components of
Transformers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-blessing-with-training"><span class="toc-number">1.1.2.</span> <span class="toc-text">A Blessing with Training…</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#and-the-curse-with-inference"><span class="toc-number">1.1.3.</span> <span class="toc-text">And the Curse with Inference!</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#are-rnns-a-solution"><span class="toc-number">1.1.4.</span> <span class="toc-text">Are RNNs a Solution?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#part-2-the-state-space-model-ssm"><span class="toc-number">1.2.</span> <span class="toc-text">Part 2: The State
Space Model (SSM)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#what-is-a-state-space"><span class="toc-number">1.2.1.</span> <span class="toc-text">What is a State Space?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#from-ssm-to-s4"><span class="toc-number">1.2.2.</span> <span class="toc-text">From SSM to S4</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#from-a-continuous-to-a-discrete-signal"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">From a Continuous to a
Discrete Signal</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-recurrent-representation"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">The Recurrent Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-convolution-representation"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">The Convolution
Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-three-representations"><span class="toc-number">1.2.2.4.</span> <span class="toc-text">The Three Representations</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-importance-of-matrix-a"><span class="toc-number">1.2.2.5.</span> <span class="toc-text">The Importance of Matrix
A</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#introduction-of-s4"><span class="toc-number">1.2.3.</span> <span class="toc-text">Introduction of S4</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#improving-transformer-struggles-with-handling-very-long-sequences"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">Improving
transformer struggles with handling very long sequences</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-definition-and-derivation-of-hippostate-compresses-the-history-of-input"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">The
definition and derivation of HiPPO：State compresses the history of
input</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#part-3-mamba---a-selective-ssm"><span class="toc-number">1.3.</span> <span class="toc-text">Part 3: Mamba - A Selective
SSM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#what-problem-does-it-attempt-to-solve"><span class="toc-number">1.3.1.</span> <span class="toc-text">What Problem does it
attempt to Solve?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#selectively-retain-information"><span class="toc-number">1.3.2.</span> <span class="toc-text">Selectively Retain
Information</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-scan-operation"><span class="toc-number">1.3.3.</span> <span class="toc-text">The Scan Operation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hardware-aware-algorithm"><span class="toc-number">1.3.4.</span> <span class="toc-text">Hardware-aware Algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-mamba-block"><span class="toc-number">1.3.5.</span> <span class="toc-text">The Mamba Block</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusion"><span class="toc-number">1.4.</span> <span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#resources"><span class="toc-number">1.5.</span> <span class="toc-text">Resources</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#references"><span class="toc-number">1.6.</span> <span class="toc-text">References</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/20/symbols/" title="symbols">symbols</a><time datetime="2024-06-20T06:04:02.000Z" title="Created 2024-06-20 14:04:02">2024-06-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/19/Mamba/" title="Mamba">Mamba</a><time datetime="2024-06-19T02:35:00.000Z" title="Created 2024-06-19 10:35:00">2024-06-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/05/29/Support-Vector-Machine/" title="Support-Vector-Machine">Support-Vector-Machine</a><time datetime="2024-05-29T07:56:16.000Z" title="Created 2024-05-29 15:56:16">2024-05-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/27/markov-process/" title="markov_process">markov_process</a><time datetime="2023-12-27T09:33:42.000Z" title="Created 2023-12-27 17:33:42">2023-12-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/08/hello-world/" title="Hello World">Hello World</a><time datetime="2023-12-08T12:50:07.908Z" title="Created 2023-12-08 20:50:07">2023-12-08</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 坚竹韧周</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>