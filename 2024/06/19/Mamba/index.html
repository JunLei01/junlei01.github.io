<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Mamba | Welcome to Junlei's word!</title><meta name="author" content="坚竹韧周"><meta name="copyright" content="坚竹韧周"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Mamba  The Transformer architecture has been a major component in the success of Large Language Models (LLMs). It has been used for nearly all LLMs that are being used today, from open-source mo">
<meta property="og:type" content="article">
<meta property="og:title" content="Mamba">
<meta property="og:url" content="http://junlei-zhou.com/2024/06/19/Mamba/index.html">
<meta property="og:site_name" content="Welcome to Junlei&#39;s word!">
<meta property="og:description" content="Mamba  The Transformer architecture has been a major component in the success of Large Language Models (LLMs). It has been used for nearly all LLMs that are being used today, from open-source mo">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://junlei-zhou.com/img/avatar.jpg">
<meta property="article:published_time" content="2024-06-19T02:35:00.000Z">
<meta property="article:modified_time" content="2025-02-25T07:57:29.886Z">
<meta property="article:author" content="坚竹韧周">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Sequence Model">
<meta property="article:tag" content="State space model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://junlei-zhou.com/img/avatar.jpg"><link rel="shortcut icon" href="/img/URL_LOGO.png"><link rel="canonical" href="http://junlei-zhou.com/2024/06/19/Mamba/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Mamba',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-02-25 15:57:29'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="Welcome to Junlei's word!" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-blog"></i><span> Library</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Live</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fa fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa fa-gamepad"></i><span> Game</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-paper-plane"></i><span> Comments</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Welcome to Junlei's word!"><span class="site-name">Welcome to Junlei's word!</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-blog"></i><span> Library</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Live</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fa fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa fa-gamepad"></i><span> Game</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-paper-plane"></i><span> Comments</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Mamba</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-06-19T02:35:00.000Z" title="Created 2024-06-19 10:35:00">2024-06-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-02-25T07:57:29.886Z" title="Updated 2025-02-25 15:57:29">2025-02-25</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Mamba"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><meta name="referrer" content="no-referrer">
<h1 id="mamba">Mamba</h1>
<!--本文是根据Mamba论文原作者撰写的Blog进行阅读的重点解读-->
<p>The Transformer architecture has been a major component in the
success of Large Language Models (LLMs). It has been used for nearly all
LLMs that are being used today, from open-source models like Mistral to
closed-source models like ChatGPT.</p>
<p>To further improve LLMs, new architectures are developed that might
even outperform the Transformer architecture. One of these methods is
<em>Mamba</em>, a <em>State Space Model</em>.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6bc1ca-a387-47ed-a9a6-077af838b359_1148x892.png" style="zoom:50%;"></p>
<p>Mamba was proposed in the paper <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time Sequence
Modeling with Selective State Spaces</a>.<a target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-1-141228095">1</a>
You can find its official implementation and model checkpoints in its <a target="_blank" rel="noopener" href="https://github.com/state-spaces/mamba">repository</a>.</p>
<p>In this post, I will introduce the field of State Space Models in the
context of language modeling and explore concepts one by one to develop
an intuition about the field. Then, we will cover how Mamba might
challenge the Transformers architecture.</p>
<p>As a visual guide, expect many visualizations to develop an intuition
about Mamba and State Space Models!</p>
<h2 id="part-1-the-problem-with-transformers">Part 1: The Problem with
Transformers</h2>
<p>To illustrate why Mamba is such an interesting architecture, let’s do
a short re-cap of transformers first and explore one of its
disadvantages.</p>
<p>A Transformer sees any textual input as a <em>sequence</em> that
consists of <em>tokens</em>.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b8c299a-c0c0-46fe-86cf-b22e08a91b32_1776x544.png" style="zoom:50%;"></p>
<p>A major benefit of Transformers is that whatever input it receives,
it can look back at any of the earlier tokens in the sequence to derive
its representation.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c01c75-1105-4aeb-a608-f00c85bbe5f7_1776x532.png" style="zoom:50%;"></p>
<h3 id="the-core-components-of-transformers">The Core Components of
Transformers</h3>
<p>Remember that a Transformer consists of two structures, a set of
encoder blocks for representing text and a set of decoder blocks for
generating text. Together, these structures can be used for several
tasks, including translation.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62a21d60-2e84-4c19-a6fb-d2eff501af1c_1776x952.png" style="zoom:50%;"></p>
<p>We can adopt this structure to create generative models by using only
decoders. This Transformer-based model, <em>Generative Pre-trained
Transformers</em> (GPT), uses decoder blocks to complete some input
text.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58e51959-d4ef-4fa9-a1c7-dab0e5ca4dc0_1776x1012.png" style="zoom: 50%;"></p>
<p>Let’s take a look at how that works!</p>
<h3 id="a-blessing-with-training">A Blessing with Training…</h3>
<p>A single decoder block consists of two main components, masked
self-attention followed by a feed-forward neural network.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5af9c5-5266-4c2b-b583-b20a19f19fcc_1776x464.png" style="zoom:50%;"></p>
<p>Self-attention is a major reason why these models work so well. It
enables an uncompressed view of the entire sequence with fast
training.</p>
<p>So how does it work?</p>
<p>It creates a matrix comparing each token with every token that came
before. The weights in the matrix are determined by how relevant the
token pairs are to one another.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F167cfe80-2863-47c8-a969-cb2eeedbd353_1776x860.png" style="zoom:50%;"></p>
<p>During training, this matrix is created in one go. The attention
between “<em>My</em>” and “<em>name</em>” does not need to be calculated
first before we calculate the attention between “<em>name</em>” and
“<em>is</em>”.</p>
<p>It enables <strong>parallelization</strong>, which speeds up training
tremendously!</p>
<h3 id="and-the-curse-with-inference">And the Curse with Inference!</h3>
<p>There is a flaw, however. When generating the next token, we need to
re-calculate the attention for the <em>entire sequence</em>, even if we
already generated some tokens.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb66f1965-fc44-4a61-b9c6-912c8120ecad_2420x580.png" style="zoom:50%;"></p>
<p>Generating tokens for a sequence of length <em>L</em> needs roughly
<em>L²</em> computations which can be costly if the sequence length
increases.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405074ed-aa8c-44b4-88a5-bae1dad0412e_2072x392.png" style="zoom:50%;"></p>
<p>This need to recalculate the entire sequence is a major bottleneck of
the Transformer architecture.</p>
<p>Let’s look at how a “classic” technique, Recurrent Neural Networks,
solves this problem of slow inference.</p>
<hr>
<blockquote>
<p><strong>为什么Transformer的推理速度会很慢？</strong></p>
<p>因为Transformer在推理当前的状态时，需要计算包含之前所有状态的信息，也就是说，当计算到第N个状态时，需要同时计算包含前N个状态的信息以得出当前状态的取值，这也就时为什么以Transformer为基础的模型在输入时会有长度限制的前提条件。而相比较于Transformer，RNN模型有着比较快的推理速度因为它只用重点关注前面重点的状态，根据此来推理出当前状态，故而其计算速度快，但由于只关注之前的部分状态，也导致了其推理的准确度相比较于Transformer略有不及。</p>
<p>例如：当输入的Batch Size=b, Sequence length=N,那么一个具有 <span class="math inline">\(l\)</span> 层的Transformer模型的计算量为 <span class="math inline">\(l*(24bNd^2+4bN^2d)\)</span>, <span class="math inline">\(d\)</span>
为词向量的维度或者是隐藏层的维度，详细的计算过程见<a target="_blank" rel="noopener" href="https://blog.csdn.net/v_JULY_v/article/details/133619540">原文</a>。</p>
</blockquote>
<h3 id="are-rnns-a-solution">Are RNNs a Solution?</h3>
<p>Recurrent Neural Networks (RNN) is a sequence-based network. It takes
two inputs at each time step in a sequence, namely the input at time
step <strong><em>t</em></strong> and a hidden state of the previous time
step <strong><em>t-1</em></strong>, to generate the next hidden state
and predict the output.</p>
<p>RNNs have a looping mechanism that allows them to pass information
from a previous step to the next. We can “unfold” this visualization to
make it more explicit.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddc71706-8da8-4c28-921b-675e9164c7ab_2404x872.png" style="zoom:50%;"></p>
<p>When generating the output, the RNN only needs to consider the
previous hidden state and current input. It prevents recalculating all
previous hidden states which is what a Transformer would do.</p>
<p>In other words, RNNs can do inference fast as it scales linearly with
the sequence length! In theory, it can even have an <em>infinite context
length</em>.</p>
<p>To illustrate, let’s apply the RNN to the input text we have used
before.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2484541-f7b1-4950-b04f-5a3177596fbb_2228x808.png" style="zoom:50%;"></p>
<p>Each hidden state is the aggregation of all previous hidden states
and is typically a compressed view.</p>
<p>There is a problem, however…</p>
<p>Notice that the last hidden state, when producing the name
“<em>Maarten</em>” does not contain information about the word
“<em>Hello</em>” anymore. RNNs tend to forget information over time
since they only consider one previous state.</p>
<p>This sequential nature of RNNs creates another problem. Training
cannot be done in parallel since it needs to go through each step at a
time sequentially.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819fcf9b-ea31-4954-8496-4b66c5b46dc2_2236x828.png" style="zoom:50%;"></p>
<p>The problem with RNNs, compared to Transformers, is completely the
opposite! Its inference is incredibly fast but it is not
parallelizable.</p>
<blockquote>
<p><strong>RNN存在的问题</strong></p>
<ol type="1">
<li><p>虽然在RNN中，每个状态都是先前所有隐藏状态的聚合，然而随着时间的推移，RNN会忘记掉一部分信息。</p></li>
<li><p>RNN没办法并行训练，即推理速度快但训练慢，而且因为RNN的结构，也无法进行卷积运算。</p></li>
</ol>
<p>由于RNN的每个时间 <span class="math inline">\(t\)</span>
的输出需要依赖前一个时间 <span class="math inline">\(t-1\)</span> 的
输出，导致其循环操作无法并行训练。</p>
<p>RNN中通过循环结构来实现权重共享，而CNN中通过卷积操作实现局部链接和全局共享。</p>
</blockquote>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7af6befe-e99d-4350-b555-21ce543cae53_2072x580.png" style="zoom:50%;"></p>
<p>Can we somehow find an architecture that does parallelize training
like Transformers whilst still performing inference that scales linearly
with sequence length?</p>
<p>Yes! This is what Mamba offers but before diving into its
architecture, let’s explore the world of State Space Models first.</p>
<h2 id="part-2-the-state-space-model-ssm">Part 2: The <strong>State
Space Model (SSM)</strong></h2>
<p>A State Space Model (SSM), like the Transformer and RNN, processes
sequences of information, like text but also signals. In this section,
we will go through the basics of SSMs and how they relate to textual
data.</p>
<h3 id="what-is-a-state-space">What is a State Space?</h3>
<p>A State Space contains the minimum number of variables that fully
describe a system. It is a way to mathematically represent a problem by
defining a system's possible states.</p>
<p>Let’s simplify this a bit. Imagine we are navigating through a maze.
The “<em>state space</em>” is the map of all possible locations
(states). Each point represents a unique position in the maze with
specific details, like how far you are from the exit.</p>
<p>The “<em>state space representation</em>” is a simplified description
of this map. It shows where you are (current state), where you can go
next (possible future states), and what changes take you to the next
state (going right or left).</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6480800-2449-456a-87a7-27c8a4e9e718_2520x1388.png" style="zoom:50%;"></p>
<p>Although State Space Models use equations and matrices to track this
behavior, it is simply a way to track where you are, where you can go,
and how you can get there.</p>
<p>The variables that describe a state, in our example the X and Y
coordinates, as well as the distance to the exit, can be represented as
“<em>state vectors</em>”.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c79eba-2559-4d9d-8999-bee33666f2e3_2364x736.png" style="zoom:50%;"></p>
<p>Sounds familiar? That is because embeddings or vectors in language
models are also frequently used to describe the “state” of an input
sequence. For instance, a vector of your current position (state vector)
could look a bit like this:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ff8812a-64d2-4fc6-8e54-eb86222333b0_1496x444.png" style="zoom:50%;"></p>
<p>In terms of neural networks, the “state” of a system is typically its
hidden state and in the context of Large Language Models, one of the
most important aspects of generating a new token.</p>
<p>What is a State Space Model?</p>
<p>SSMs are models used to describe these state representations and make
predictions of what their next state could be depending on some
input.</p>
<p>Traditionally, at time <strong><em>t</em></strong>, SSMs:</p>
<ul>
<li><p>map an input sequence <strong><em>x(t)</em></strong> — (e.g.,
moved left and down in the maze)</p></li>
<li><p>to a latent state representation <strong><em>h(t)</em></strong> —
(e.g., distance to exit and x/y coordinates)</p></li>
<li><p>and derive a predicted output sequence
<strong><em>y(t)</em></strong> — (e.g., move left again to reach the
exit sooner)</p>
<blockquote>
<p><strong>A:</strong> 当前状态如何影响下一状态</p>
<p><strong>B:</strong> 输入序列 <strong>x</strong>
如何影响当前的状态</p>
<p><strong>C:</strong> 当前的状态如何影响当前的输出</p>
<p><strong>D:</strong> 输入序列 <strong>x</strong>
如何影响当前的输出</p>
</blockquote></li>
</ul>
<p>However, instead of using <em>discrete</em> <em>sequences</em> (like
moving left once) it takes as input a <em>continuous</em>
<em>sequence</em> and predicts the output sequence.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5788c3e4-8794-4492-af87-3a45f7a6aa70_1992x624.png" style="zoom:50%;"></p>
<p>SSMs assume that dynamic systems, such as an object moving in 3D
space, can be predicted from its state at time
<strong><em>t</em></strong> through two equations.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32401c6d-39b6-4619-a75e-6b33d3268bca_2520x388.png" style="zoom:50%;"></p>
<blockquote>
<p>这里的 <span class="math inline">\(h&#39;(t)\)</span>
是SSM一阶微分方程中的导数，指的是连续状态下的当前状态是由上一状态得出的。</p>
<p>写成离散形态的即为 <span class="math inline">\(h_t=Ah_{t-1}+Bx_t\)</span>.</p>
<p>通过求解这两个方程，可以根据观察到的数据“输入序列 x 和先前的状态 h(t)
，即可实现对未来状态 y(t) 进行预测。</p>
<p><img src="https://imgur.la/images/2024/06/21/Mamba-Con_dis.png" alt="Mamba Con dis" border="0" style="zoom:50%;"></p>
</blockquote>
<p>By solving these equations, we assume that we can uncover the
statistical principles to predict the state of a system based on
observed data (input sequence and previous state).</p>
<p>==Its goal is to find this state representation
<strong><em>h(t)</em></strong> such that we can go from an input to an
output sequence.==</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5c7ae-3dbe-44d8-8b13-7f4dcc14a29b_2008x624.png" style="zoom:50%;"></p>
<p>These two equations are the core of the State Space Model.</p>
<p>The two equations will be referenced throughout this guide. To make
them a bit more intuitive, they are <strong>color-coded</strong> so you
can quickly reference them.</p>
<p>The <strong>state equation</strong> describes how the state changes
(through <em>matrix A</em>) based on how the input influences the state
(through <em>matrix B</em>).</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0876819d-8a46-4187-9826-14391bfd47b9_1796x624.png" style="zoom:50%;"></p>
<p>As we saw before, <strong><em>h(t)</em></strong> refers to our latent
state representation at any given time <strong><em>t</em></strong>, and
<strong><em>x(t)</em></strong> refers to some input.</p>
<p>The <strong>output equation</strong> describes how the state is
translated to the output (through <em>matrix C</em>) and how the input
influences the output (through <em>matrix D</em>).</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2e87708-9676-4a1c-b32c-d703026f64d9_1796x624.png" style="zoom:50%;"></p>
<blockquote>
<p><strong>NOTE</strong>: Matrices <em>A</em>, <em>B</em>, <em>C</em>,
and <em>D</em> are also commonly refered to as <em>parameters</em> since
they are learnable.</p>
</blockquote>
<p>Visualizing these two equations gives us the following
architecture:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc358439e-c507-49f1-ac2e-5dedaccc2a8b_1728x364.png" style="zoom:50%;"></p>
<p>Let’s go through the general technique step-by-step to understand how
these matrices influence the learning process.</p>
<p>Assume we have some input signal <strong><em>x(t)</em></strong>, this
signal first gets multiplied by <em>matrix B</em> which describes how
the inputs influence the system.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd6f8dae-2281-47af-8ba3-06bbdc594d1c_1956x360.png" style="zoom: 67%;"></p>
<p>The updated state (akin to the hidden state of a neural network) is a
latent space that contains the core “knowledge” of the environment. We
multiply the state with <em>matrix A</em> which describes how all the
internal states are connected as they represent the underlying dynamics
of the system.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cedc98a-d200-4fe4-b311-6d68dcaa50af_1956x572.png" style="zoom:50%;"></p>
<blockquote>
<p><strong>A</strong>
矩阵决定了整个状态的演化，即如何从一个状态演化到下一个状态。其本质为存储着之前所有历史信息的浓缩精华。</p>
</blockquote>
<p>As you might have noticed, <em>matrix A</em> is applied before
creating the state representations and is updated after the state
representation has been updated.</p>
<p>Then, we use <em>matrix C</em> to describe how the state can be
translated to an output.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8599487f-1023-4069-be7a-8056e63b0574_1956x572.png" style="zoom:50%;"></p>
<p>Finally, we can make use of <em>matrix D</em> to provide a direct
signal from the input to the output. This is also often referred to as a
<em>skip-connection</em>.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf79721f-5cef-44da-98c5-f63a7839ebc3_1956x756.png" style="zoom:50%;"></p>
<p>Since <em>matrix D</em> is similar to a skip-connection, the SSM is
often regarded as the following without the skip-connection.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca1d511-7d31-42a0-9220-2fb85b256efd_1956x864.png" style="zoom:50%;"></p>
<p>Going back to our simplified perspective, we can now focus on
matrices <em>A</em>, <em>B</em>, and <em>C</em> as the core of the
SSM.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e52f4f0-d7ad-453d-a741-6dfa4a998964_1728x352.png" style="zoom:50%;"></p>
<p>We can update the original equations (and add some pretty colors) to
signify the purpose of each matrix as we did before.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55df8ede-3a16-4473-8ea9-872fe199d3a1_1904x676.png" style="zoom:50%;"></p>
<p>Together, these two equations aim to predict the state of a system
from observed data. Since the input is expected to be continuous, the
main representation of the SSM is a <strong>continuous-time
representation</strong>.</p>
<h3 id="from-ssm-to-s4">From SSM to S4</h3>
<h4 id="from-a-continuous-to-a-discrete-signal">From a Continuous to a
Discrete Signal</h4>
<p>Finding the state representation <strong><em>h(t)</em></strong> is
analytically challenging if you have a continuous signal. Moreover,
since we generally have a discrete input (like a textual sequence), we
want to discretize the model.</p>
<p>To do so, we make use of the <em>Zero-order hold technique.</em> It
works as follows.</p>
<ol type="1">
<li>First, every time we receive a discrete signal, we hold its value
until we receive a new discrete signal. This process creates a
continuous signal the SSM can use:</li>
</ol>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a2ffb18-2e66-4135-9888-98e9ab88d0d8_1488x472.png" style="zoom:50%;"></p>
<ol start="2" type="1">
<li><p>How long we hold the value is represented by a new learnable
parameter, called the <em>step size</em> <span class="math inline">\(\Delta\)</span> . It represents the resolution of
the input.</p></li>
<li><p>Now that we have a continuous signal for our input, we can
generate a continuous output and only sample the values according to the
time steps of the input.</p></li>
</ol>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F042ff699-81af-4479-b99f-92e4997c4c81_1488x476.png" style="zoom:50%;"></p>
<p>These sampled values are our discretized output!</p>
<p>Mathematically, we can apply the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Zero-order_hold">Zero-order hold</a>
as follows:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6df4b59-6f76-4f13-a201-7b69e59df164_6200x1176.png" style="zoom:50%;"></p>
<p>Together, they allow us to go from a continuous SSM to a discrete SSM
represented by a formulation that instead of a
<em>function-to-function</em>, <span class="math inline">\(x(t)\rightarrow y(t)\)</span>, is now a
<em>sequence-to-sequence</em>, <span class="math inline">\(x_k\rightarrow y_k\)</span>:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc29cfbbb-ae41-4dc2-b899-9e0a81cba34d_1980x1012.png" style="zoom:50%;"></p>
<p>Here, matrices <strong><em>A</em></strong> and
<strong><em>B</em></strong> now represent discretized parameters of the
model.</p>
<p>We use <strong><em>k</em></strong> instead of
<strong><em>t</em></strong> to represent discretized timesteps and to
make it a bit more clear when we refer to a continuous versus a discrete
SSM.</p>
<blockquote>
<p><strong>NOTE:</strong> We are still saving the continuous form of
<em>Matrix A</em> and not the discretized version during training.
During training, the continuous representation is discretized.</p>
</blockquote>
<p>Now that we have a formulation of a discrete representation, let’s
explore how we can actually <em>compute</em> the model.</p>
<h4 id="the-recurrent-representation">The Recurrent Representation</h4>
<p>Our discretized SSM allows us to formulate the problem in specific
timesteps instead of continuous signals. A recurrent approach, as we saw
before with RNNs is quite useful here.</p>
<p>If we consider discrete timesteps instead of a continuous signal, we
can reformulate the problem with timesteps:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b70ba4-b068-44d1-8641-9b224d103c51_1980x548.png" style="zoom:50%;"></p>
<p>At each timestep, we calculate how the current input (<strong><span class="math inline">\(Bx_k\)</span></strong>) influences the previous
state (<strong><span class="math inline">\(Ah_{k-1}\)</span></strong>)
and then calculate the predicted output (<strong><span class="math inline">\(Ch_k\)</span></strong>).</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb4d0412-87fb-4507-bedb-4793588bd465_2116x788.png" style="zoom:50%;"></p>
<p>This representation might already seem a bit familiar! We can
approach it the same way we did with the RNN as we saw before.</p>
<blockquote>
<p>对于 <span class="math inline">\(y_2\)</span> 展开计算则有 <span class="math display">\[
\begin{align}
y_2&amp;=Ch_2 \\
&amp;=C(\bar{A}h_1+\bar{B}x_2) \\
&amp;=C(\bar{A}(\bar{A}h_0+Bx_1)+\bar{B}x_2) \\
&amp;=C(\bar{A}(\bar{A} \cdot \bar{B}x_0 +Bx_1)+\bar{B}x_2) \\
&amp;=C\cdot \bar{A}^2\cdot \bar{B}x_0+C\cdot \bar{A}\cdot
\bar{B}x_1+C\cdot \bar{B}x_2
\end{align}
\]</span> 由此可推得 <span class="math inline">\(y_k=C\cdot \bar{A}^k
\cdot \bar{B}x_0+C\cdot \bar{A}^{k-1} \cdot \bar{B}x_1+\cdots+C\cdot
\bar{A} \cdot \bar{B}x_k\)</span></p>
</blockquote>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91ca51f7-9b9b-4f17-bccb-32a5a96f3339_2184x868.png" style="zoom:50%;"></p>
<p>Which we can unfold (or unroll) as such:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1084e8a-a70d-450b-beb0-f18117ade5ed_2184x876.png" style="zoom:50%;"></p>
<p>Notice how we can use this discretized version using the underlying
methodology of an RNN.</p>
<p>This technique gives us both the advantages and disadvantages of an
RNN, namely fast inference and slow training.</p>
<h4 id="the-convolution-representation">The Convolution
Representation</h4>
<p>Another representation that we can use for SSMs is that of
convolutions. Remember from classic image recognition tasks where we
applied filters (<em>kernels</em>) to derive aggregate features:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47f05950-bfad-4013-b854-679c9a47ada9_3216x2144.png" style="zoom:50%;"></p>
<p>Since we are dealing with text and not images, we need a
1-dimensional perspective instead:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb943872f-de72-43e8-b2f1-cb8213f120a3_3216x1296.png" style="zoom:50%;"></p>
<p>The kernel that we use to represent this “filter” is derived from the
SSM formulation:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05049821-2352-4c04-8fb2-07fe15c20a9c_2620x824.png" style="zoom:50%;"></p>
<blockquote>
<p><strong>为什么可以写成卷积的形式？</strong></p>
<p>如上图，其中 <span class="math inline">\(K\)</span>
为卷积核，其中此时的 <strong>A、B、C</strong> 都是常数，可以看出卷积核
<span class="math inline">\(K\)</span>
是通过固定数值矩阵得到的，只需确定 <strong>A、B、C</strong>
即可并行运算。</p>
</blockquote>
<p>Let’s explore how this kernel works in practice. Like convolution, we
can use our SSM kernel to go over each set of tokens and calculate the
output:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9007d03b-c1c9-4b37-8c83-f27bfe8318f4_2620x1080.png" style="zoom:50%;"></p>
<p>This also illustrates the effect padding might have on the output. I
changed the order of padding to improve the visualization but we often
apply it at the end of a sentence.</p>
<p>In the next step, the kernel is moved once over to perform the next
step in the calculation:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82ed71fb-f237-4173-bb23-bd1bf02ff123_2620x1080.png" style="zoom:50%;"></p>
<p>In the final step, we can see the full effect of the kernel:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4387b79-5e92-4fe2-8b30-2abf112f5a73_2620x1080.png" style="zoom:50%;"></p>
<p>A major benefit of representing the SSM as a convolution is that it
can be trained in parallel like Convolutional Neural Networks (CNNs).
However, due to the fixed kernel size, their inference is not as fast
and unbounded as RNNs.</p>
<h4 id="the-three-representations">The Three Representations</h4>
<p>These three representations, <em>continuous</em>, <em>recurrent</em>,
and <em>convolutional</em> all have different sets of advantages and
disadvantages:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F682187d6-f402-44aa-8097-8a2e5b6179a7_2072x744.png" style="zoom:50%;"></p>
<p>Interestingly, we now have efficient inference with the recurrent SSM
and parallelizable training with the convolutional SSM.</p>
<blockquote>
<ol type="1">
<li>在训练的时候怎么使用CNN进行训练？</li>
</ol>
<p><span class="math display">\[
y=\bar{K}x
\]</span></p>
<p>​ 能够通过使用卷积的方法使其进行并行计算。</p>
<ol start="2" type="1">
<li>在推理的时候怎么进行推理？ <span class="math display">\[
\left\{ \begin{array}{lcl}
h_k=\bar{A}h_{k-1}+\bar{B}x_k \\
y_k=Ch_k+Dx_k
\end{array}\right.
\]</span></li>
</ol>
</blockquote>
<p>With these representations, there is a neat trick that we can use,
namely choose a representation depending on the task. During training,
we use the convolutional representation which can be parallelized and
during inference, we use the efficient recurrent representation:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c43c82d-9735-4d55-97bb-8ad6f504909e_1960x1008.png" style="zoom:50%;"></p>
<p>This model is referred to as the <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/05546b0e38ab9175cd905eebcc6ebb76-Abstract.html">Linear
State-Space Layer (LSSL)</a>.<a target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-2-141228095">2</a></p>
<p>These representations share an important property, namely that of
<strong><em>Linear Time Invariance</em></strong> (LTI). LTI states that
the SSMs parameters, <em>A</em>, <em>B</em>, and <em>C</em>, are fixed
for all timesteps. This means that matrices <em>A</em>, <em>B</em>, and
<em>C</em> are the same for every token the SSM generates.</p>
<p>In other words, regardless of what sequence you give the SSM, the
values of <em>A</em>, <em>B</em>, and <em>C</em> remain the same. We
have a static representation that is not content-aware.</p>
<p>Before we explore how Mamba addresses this issue, let’s explore the
final piece of the puzzle, <em>matrix A</em>.</p>
<h4 id="the-importance-of-matrix-a">The Importance of Matrix
<em>A</em></h4>
<blockquote>
<p><strong>A</strong>
矩阵决定了整个状态的演化，即如何从一个状态演化到下一个状态。其本质为存储着之前所有历史信息的浓缩精华，
<strong>A</strong> 决定了系统的动态特性，但 <strong>A</strong>
可能存在一个问题，即 <strong>A</strong>
只记住了之前的部分状态，像RNN那样忘记了部分状态，从而会导致SSM在性能上与RNN相似。</p>
<p><strong>A</strong> 设计的关键是如何在有限的空间中保留更多的记忆！</p>
</blockquote>
<p>Arguably one of the most important aspects of the SSM formulation is
<em>matrix A</em>. As we saw before with the recurrent representation,
it captures information about the <em>previous</em> state to build the
<em>new</em> state.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07542fb1-d4b6-421e-8b2a-a3f0a7790939_2028x876.png" style="zoom:50%;"></p>
<p>In essence, <em>matrix</em> <em>A</em> produces the hidden state:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47635355-6b9a-4981-af3a-7ee6a12b87b3_1412x468.png" style="zoom:50%;"></p>
<p>Creating <em>matrix A</em> can therefore be the difference between
remembering only a few previous tokens and capturing every token we have
seen thus far. Especially in the context of the Recurrent representation
since it only <em>looks back</em> <em>at the previous state</em>.</p>
<p>So how can we create <em>matrix A</em> in a way that retains a large
memory (context size)?</p>
<p>We use Hungry Hungry Hippo! Or <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2020/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html">HiPPO</a><a target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-3-141228095">3</a>
for <strong>Hi</strong>gh-order <strong>P</strong>olynomial
<strong>P</strong>rojection <strong>O</strong>perators. HiPPO attempts
to compress all input signals it has seen thus far into a vector of
coefficients.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07985a64-fc26-4ee8-9ec2-c488e4cb709a_1492x488.png" style="zoom:50%;"></p>
<p>It uses <em>matrix A</em> to build a state representation that
captures recent tokens well and decays older tokens. Its formula can be
represented as follows:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bc7c768-7f0c-4983-a21e-70a4f587e6aa_2520x628.png" style="zoom:50%;"></p>
<p>Assuming we have a square <em>matrix <strong>A</strong></em>, this
gives us:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8f5de9-6448-43d5-9878-c8cd1d938b7c_1436x708.png" style="zoom:50%;"></p>
<p>Building <em>matrix A</em> using HiPPO was shown to be much better
than initializing it as a random matrix. As a result, it more accurately
reconstructs <em>newer</em> signals (recent tokens) compared to
<em>older</em> signals (initial tokens).</p>
<p>The idea behind the HiPPO Matrix is that it produces a hidden state
that memorizes its history.</p>
<p>Mathematically, it does so by tracking the coefficients of a <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html">Legendre
polynomial</a> which allows it to approximate all of the previous
history.<a target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-4-141228095">4</a></p>
<p>HiPPO was then applied to the recurrent and convolution
representations that we saw before to handle long-range dependencies.
The result was <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.00396">Structured
State Space for Sequences (S4)</a>, a class of SSMs that can efficiently
handle long sequences.<a target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state#footnote-5-141228095">5</a></p>
<p>It consists of three parts:</p>
<ul>
<li><p>State Space Models</p></li>
<li><p>HiPPO for handling <strong>long-range
dependencies</strong></p></li>
<li><p>Discretization for creating <strong>recurrent</strong> and
<strong>convolution</strong> representations</p></li>
</ul>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb055ec5-f8c7-4862-ab88-f4fb38abf042_1892x844.png" style="zoom:50%;"></p>
<p>This class of SSMs has several benefits depending on the
representation you choose (recurrent vs. convolution). It can also
handle long sequences of text and store memory efficiently by building
upon the HiPPO matrix.</p>
<blockquote>
<p><strong>NOTE</strong>: If you want to dive into more of the technical
details on how to calculate the HiPPO matrix and build a S4 model
yourself, I would HIGHLY advise going through the <a target="_blank" rel="noopener" href="https://srush.github.io/annotated-s4/">Annotated S4</a>.</p>
<p>其中在S4中对 <strong>A</strong> 进行了改进：</p>
<p><strong>Theorem 1.</strong> All <em>HiPPO</em> matrices have a
<em>Normal Plus Low-Rank</em> (NPLR) representation <span class="math display">\[
A=VAV^*-PQ^\top=V(\Lambda-(V^*P)(V^*Q))V^*
\]</span> for unitary <span class="math inline">\(V\in
\mathbb{C}^{N\times N}\)</span>, diagonal <span class="math inline">\(\Lambda\)</span>， and low-rank factorization
<span class="math inline">\(P,Q\in\mathbb{R}^{N\times r}\)</span>. These
matrices <em>HiPPO-LegS, LegT, LagT</em> all satisfy <span class="math inline">\(r=1 \ or \ r=2\)</span>.</p>
</blockquote>
<hr>
<h3 id="introduction-of-s4">Introduction of S4</h3>
<h4 id="improving-transformer-struggles-with-handling-very-long-sequences">Improving
transformer struggles with handling very long sequences</h4>
<p>序列数据一般都是离散的数据 比如文本、图、DNA</p>
<ol type="1">
<li>但现实生活中还有很多连续的数据，比如音频、视频，对于音视频这种信号而言，其一个重要特点就是有极长的context
window</li>
<li>而在transformer长context上往往会失败，或者注意力机制在有着超长上下文长度的任务上并不擅长，而S4擅长这类任务。</li>
</ol>
<h4 id="the-definition-and-derivation-of-hippostate-compresses-the-history-of-input">The
definition and derivation of HiPPO：State compresses the history of
input</h4>
<p>我们已经知道一个RNN网络更擅长于处理这种序列数据，但是RNN网络最大的缺点是由于它的hidden
state记忆能力有限，导致其会忘记掉一些以前的特征。</p>
<p>==关键：怎样改善RNN记忆有限的问题？==</p>
<p>假设我们在 <span class="math inline">\(t_0\)</span>
时刻有接收到袁术输入信号 <span class="math inline">\(u(t)\)</span>之前的部分：</p>
<ol type="1">
<li><p>我们希望使用一个记忆单元去压缩之前这段的输入部分来学习特征，我们使用一个多项式去近似这段之前的输入
<span class="math display">\[
x(t_0)=\begin{bmatrix} 0.1 \\
-1.1 \\
3.7 \\
2.5
\end{bmatrix}
\]</span></p></li>
<li><p>当我们在接收更多的signal的时候，我们仍然希望这个记忆单元能够对已经接收到的所有的signal进行压缩，因此需要自动更新多项式的各项系数，如下图所示</p>
<p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_1.png" alt="Mamba HiPPO 1" border="0" style="zoom:50%;"></p></li>
<li><p>此时，HiPPO的关键问题就变成了一个优化问题</p>
<ul>
<li><p>如何能够找到这些最优的近似？</p></li>
<li><p>如何快速的更新这些多项式的参数？</p></li>
</ul>
<p>因此，我们需要定义一个指标去判断一个近似的好坏程度。这个指标可以解读为</p>
<p>（1）在距离当下时刻最近的拟合函数 <span class="math inline">\(x\)</span> 与原数据 <span class="math inline">\(u\)</span>
保持最大的相似度，距离当下时刻远的保留其大致状态；</p>
<p>（2）也可以是只关注距离当下时刻最近的拟合函数 <span class="math inline">\(x\)</span> 与原数据 <span class="math inline">\(u\)</span>
保持最大的相似度，而不在乎距离当下时刻远的状态；</p>
<p>（3）还可以是关注整体的拟合程度，使其总体与原数据保持最大........</p>
<p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_2.png" alt="Mamba HiPPO 2" border="0" style="zoom:50%;"></p>
<p>(Note: 添加推导过程)</p></li>
<li><p>HiPPO的定义，两个矩阵 (<strong>A</strong>
表示之前的状态怎样影响当前的状态，<strong>B</strong>当前的输入怎样影响当前的状态)与两个信号
( <span class="math inline">\(x(t)\)</span> 当前状态之前接收到的signal与
<span class="math inline">\(u(t)\)</span> 当前接收到的signal)</p>
<p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_3.png" alt="Mamba HiPPO 3" border="0" style="zoom:50%;"></p>
<blockquote>
<p>需要澄清一点的是作者在这里使用了State space
Model中状态的定义，与本文前面部分的定义有些许不同. 但事实上这里的 <span class="math inline">\(x(t)\)</span> 就等同于上文中的 <span class="math inline">\(h(t)\)</span> 这里的 <span class="math inline">\(u(t)\)</span> 就等同于上文中的 <span class="math inline">\(x(t)\)</span>
。我们更换一下这个表示即可转变为上文中的形式 <span class="math display">\[
x&#39;(t)=Ax(t)+Bu(t) \Longrightarrow h&#39;(t)=Ah(t)+Bx(t)
\]</span> 其中 <strong>A</strong> 就是HiPPO matrix.</p>
</blockquote></li>
<li><p>HiPPO就相当于把一个高维的复杂函数映射压缩成一个简单的函数，这样既保留了基本信息，又节省了空间。如下图所示，<span class="math inline">\(u(t)\)</span> 是原始输入的signal，<span class="math inline">\(x(t)\)</span> 是经过压缩后产生到的
signal，即对应上文中的 <span class="math inline">\(h(t)\)</span>.</p>
<p><img src="https://imgur.la/images/2024/06/20/Mamba-HiPPO_4.png" alt="Mamba HiPPO 4" border="0" style="zoom:50%;"></p></li>
<li><p>我们对上图进行还原处理即可得到</p>
<p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_5.png" alt="Mamba HiPPO 5" border="0" style="zoom:50%;"></p>
<p>说明：图中 <span class="math inline">\(u\)</span>
序列是一个原始的长度为10000
(横轴的长度为10000)，表示原本的信号总共有10000个1维信号，想要完全表示的话每个信号一个memory
unit，总共需要10000个memory unit。 但我们现在使用一个64个memory unit
的信号压缩器
(图中的蓝色线，其中这里为了画图清晰只画出了4条曲线，实际上应该是64条，即相当于使用了64个不同的hidden
state去表示已经接收到的signal，对应的 <strong>A</strong> 矩阵的大小为
<span class="math inline">\(\mathbb{R}^{N\times N}\)</span>)
去压缩这个10000个memory unit的信号。图像中红色的线是使用压缩后的信号
(蓝线：<span class="math inline">\(x\)</span> 序列)
还原得到的原始信号序列 (黑线：<span class="math inline">\(u\)</span>
序列)，可以看出其中在距离当下时刻最近的还原最准确，距离当下最远的时刻只保留了大致的状态。图中绿色的线是采用了EDM(Exponential
Decaying Measure) 作为评价指标来衡量逼近多项式 <span class="math inline">\(x\)</span> 的好坏。</p></li>
<li><p>事实上，HiPPO可以推广到任何指标上：</p>
<p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_6.png" alt="Mamba HiPPO 6" border="0" style="zoom:50%;"></p>
<p>无论是更关注于当前时刻的状态还是关注于总体的状态，都存在对应的 HiPPO
operator能计算下一时刻的状态。</p></li>
</ol>
<h4 id="high-order-hippo-from-state-state-transition-to-state-result-transition">High-order
HiPPO: from state-state transition to state-result transition</h4>
<p>通过上文，我们已经实现了HiPPO在低阶 (10000个Memory
unit)上的工作流程，然而维度是不断扩大的，我们的目标是将其拓展到高阶上的表示，阶数越高其对应的场景与LLM更加相似。</p>
<ol type="1">
<li><p>一个简单的思路就是继续堆叠HiPPO算子 (即 <span class="math inline">\(x\)</span>
的维度数)，但这会面临一个问题，不断地堆叠会造成维度爆炸，并且也失去了压缩的意义。</p>
<p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_7.png" alt="Mamba HiPPO 7" border="0" style="zoom:50%;"></p></li>
<li><p>这里采用了将映射的 state <span class="math inline">\(x\)</span>
进行线性组合 <span class="math inline">\(Cx\)</span>，最终得到输出 <span class="math inline">\(y\)</span> （图中红色的线），而 <span class="math inline">\(D\)</span> 是一个skip connection, 是绕开状态 <span class="math inline">\(x\)</span>，直接从输入 <span class="math inline">\(u\)</span>到输出 <span class="math inline">\(y\)</span> 的连接。</p>
<p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_8.png" alt="Mamba HiPPO 8" border="0" style="zoom:50%;"></p>
<blockquote>
<p>同样的，这里的 <span class="math inline">\(x\)</span>
就等同于上文中的 <span class="math inline">\(h(t)\)</span> 这里的 <span class="math inline">\(u\)</span> 就等同于上文中的 <span class="math inline">\(x\)</span>
。我们更换一下这个表示即可转变为上文中的形式 <span class="math display">\[
y=Cx+Du\Longrightarrow y_t=Ch(t)+Dx(t)
\]</span></p>
</blockquote></li>
<li><p>事实上，以上的两个步骤就组成了Kalman在1960年提出的State Space
Machine：</p></li>
</ol>
<p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_9.png" alt="Mamba HiPPO 9" border="0">
<span class="math display">\[
   \left\{ \begin{array}{rcl}  x&#39;=Ax+Bu \\
   y=Cx+Du
   \end{array}\right.
   \ \  \Longrightarrow \ \
   \left\{ \begin{array}{rcl} h&#39;(t)=Ah(t)+Bx(t) \\
   y(t) = Ch(t)+Dx(t)
   \end{array}\right.
   \]</span></p>
<h4 id="the-definition-and-properties-of-s4">The definition and
properties of S4</h4>
<p><strong>S4， 即Structured State Spaces。</strong> 在一个State Space
Model中，我们使用HiPPO operate 实现状态的变换，其中的 <span class="math inline">\(A\)</span> 矩阵我们使用 HiPPO Metrix。</p>
<p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_10.png" alt="Mamba HiPPO 10" border="0" style="zoom:50%;"></p>
<p>S4具有以下特点，能够应对连续序列，离散序列等场景。</p>
<p><img src="https://imgur.la/images/2024/06/21/Mamba-HiPPO_11.png" alt="Mamba HiPPO 11" border="0" style="zoom:50%;"></p>
<p>在代码实现中，SSM将这些表示作为深度学习pipeline中其中的一层，并且其中的矩阵
<span class="math inline">\(A,B,C,D\)</span>
是需要优化的变量，通过数据训练得到的。通常有 <span class="math inline">\(d\)</span>
层这样的SSM层并行存在，每个对应一个隐藏维度。</p>
<blockquote>
<p>To preserve the sequence history, HiPPO projects the history on a
basis of <strong>orthogonal polynomials</strong>, which translates to
having SSMs whose <strong>A,B</strong> matrices are initialized to some
matrices.</p>
<p>This recurrent form of SSMs allows efficient inference (i.e.,
generation) : to generate the output of the next time-step, one only
needs the state of the current time-step, not the entire input
history.</p>
</blockquote>
<p>此时的S4, 在训练的时候可以通过卷积的形式进行训练 <span class="math inline">\(y=\bar{K}u\)</span>,
这使得S4具有极快的推理速度。</p>
<blockquote>
<p>此时仍存在些许疑问，根据上文我们可以知道 <span class="math inline">\(\bar{K}=(C\bar{B},C\bar{A}\bar{B},...,C\bar{A}^k\bar{B},\dots)\)</span>。好像卷积核是随着序列长度不断增长的，这意味着卷积核
<span class="math inline">\(\bar{K}\)</span>
是无无限长的，但卷积核显然不能是无线长的，因此需要重新设计一个卷积核以适应无限长的输入序列。</p>
<p><img src="https://imgur.la/images/2024/06/21/Mamba-S4_12.png" alt="Mamba S4 12" border="0" style="zoom:50%;"></p>
<p>其中，我们不直接计算这个卷积核 <span class="math inline">\(\bar{K}\)</span> ，而是采用 truncated generating
function的方式去使用FFT的方法逼近卷积核 <span class="math inline">\(\bar{K}\)</span>.</p>
<p>此外，文中说的 <span class="math inline">\(\bar{K}\)</span> 是一个
<strong>Low-Rank</strong> 的矩阵，是因为 <strong>A</strong> 矩阵是一个
<strong>Low-Rank</strong> 的下三角矩阵，而 <span class="math inline">\(\bar{K}=(C\bar{B},C\bar{A}\bar{B},...,C\bar{A}^k\bar{B},\dots)\)</span>
因此 <span class="math inline">\(\bar{K}\)</span> 也是一个
<strong>Low-Rank</strong> 的矩阵。</p>
</blockquote>
<p>但此时的S4似乎仍存在一个问题，<strong>S4中的<span class="math inline">\(A,B,C\)</span>
矩阵是始终保持不变的（这里的保持不变是说在inference中始终都使用这三个矩阵），这也就意味着对于不同时刻的输入
<span class="math inline">\(x\)</span>
我们都使用相同的方式处理，但实际上对于一个序列输入，有些部分重要，有些部分并不重要，对所有的部分都做相同处理，这无法实现对输入的针对性处理</strong></p>
<p>因为我们在前文中一直说明的SSM模型是线性非时变系统 (Linear
time-invariant system) 的状态空间模型，实际中的SSM可能是一个线性时变系统
(Linear time-invariant system),
那如何将线性非时变系统的SSM转化称线性时变系统的SSM呢？</p>
<p><img src="https://imgur.la/images/2024/06/21/Mamba-S4_13.png" alt="Mamba S4 13" border="0" style="zoom:50%;"></p>
<p>显然，我们只需要将其中的 <span class="math inline">\(A,B,C\)</span>
矩阵变成可以跟着状态改变而改变的即可。但如果直接改变，又会引发一系列蝴蝶效应，虽然改变后的SSM模型能够"专注于"输入序列中对当前任务更重要的部分，但由于
<span class="math inline">\(\bar{K}=(C\bar{B},C\bar{A}\bar{B},...,C\bar{A}^k\bar{B},\dots)\)</span>
而导致其无法实现并行计算了。</p>
<p>在前文的介绍中，S4可以先计算卷积核 <span class="math inline">\(\bar{K}\)</span> ，保存，然后直接与输入 <span class="math inline">\(x\)</span> 相乘即可得到输出 <span class="math inline">\(y\)</span> ，然而，此时的 <span class="math inline">\(A,B,C\)</span>
是会随着输入序列的状态变化的，因而导致了我们无法提前计算卷积核 <span class="math inline">\(\bar{K}\)</span>
这也就意味着无法进行卷积运算，并行计算被PASS。</p>
<p>此时，解决此问题的关键就变成了<strong>如何找到一种无需卷积的并行运算？</strong>这正是Mamba要做的内容....</p>
<hr>
<h2 id="part-3-mamba---a-selective-ssm">Part 3: Mamba - A Selective
SSM</h2>
<p>We finally have covered all the fundamentals necessary to understand
what makes Mamba special. State Space Models can be used to model
textual sequences but still have a set of disadvantages we want to
prevent.</p>
<p>In this section, we will go through Mamba’s two main
contributions:</p>
<ol type="1">
<li><p>A <strong>selective scan algorithm</strong>, which allows the
model to filter (ir)relevant information</p></li>
<li><p>A <strong>hardware-aware algorithm</strong> that allows for
efficient storage of (intermediate) results through <em>parallel
scan</em>, <em>kernel fusion</em>, and <em>recomputation</em>.</p></li>
</ol>
<p>Together they create the <em>selective SSM</em> or <em>S6</em> models
which can be used, like self-attention, to create <em>Mamba
blocks</em>.</p>
<p>Before exploring the two main contributions, let’s first explore why
they are necessary.</p>
<blockquote>
<p>先回答前一章节留下的问题。</p>
<p>Mamba解决这一问题的办法是设计了一种简单的选择机制，通过"<strong>参数化SSM的输入</strong>"，让模型对信息有选择性的处理，以便重点关注或忽略特定的输入。因此，模型一方面能够实现对关键信息的长时间记忆，另一方面可以过滤掉与任务无关的序列信息。</p>
<p>就如前文中所提到的，Mamba每次参考前面所有内容的一个概括，越往后写对前面内容概括得越狠，丢掉细节、保留大意。</p>
<p>而Transformer每次写之前都要从头到尾重新看一遍前文；RNN则是写着忘着，只关注前面的一句。</p>
<table>
<colgroup>
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 35%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>模型</th>
<th><strong>对信息的压缩程度</strong></th>
<th><strong>训练的效率</strong></th>
<th><strong>推理的效率</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Transformer</td>
<td>transformer对每个历史记录都不压缩</td>
<td>训练消耗算力大</td>
<td>推理消耗算力大</td>
</tr>
<tr class="even">
<td>RNN</td>
<td>随着时间的推移，RNN 往往会忘记某一部分信息</td>
<td>RNN没法并行训练</td>
<td>推理时只看一个时间步 故推理高效(<em>相当于推理快但训练慢</em>)</td>
</tr>
<tr class="odd">
<td>CNN</td>
<td></td>
<td>训练效率高，可并行「<em>因为能够绕过状态计算，并实现仅包含(B, L,
D)的卷积核</em>」</td>
<td></td>
</tr>
<tr class="even">
<td>SSM</td>
<td>SSM压缩每一个历史记录</td>
<td></td>
<td>矩阵不因输入不同而不同，无法针对输入做针对性推理</td>
</tr>
<tr class="odd">
<td>Mamba</td>
<td>选择性的关注必须关注的、过滤掉可以忽略的</td>
<td>mamba每次参考前面所有内容的一个概括，兼备训练、推理的效率</td>
<td></td>
</tr>
</tbody>
</table>
<p>总之，序列模型的效率与效果的权衡点在于它们对状态的压缩程度：</p>
<ul>
<li>高效的模型必须有一个小的状态(比如RNN或S4)</li>
<li>而有效的模型必须有一个包含来自上下文的所有必要信息的状态(比如transformer)</li>
</ul>
</blockquote>
<h3 id="what-problem-does-it-attempt-to-solve">What Problem does it
attempt to Solve?</h3>
<p>State Space Models, and even the S4 (Structured State Space Model),
perform poorly on certain tasks that are vital in language modeling and
generation, namely <em>the ability to focus on or ignore particular
inputs</em>.</p>
<p>We can illustrate this with two synthetic tasks, namely
<strong>selective copying</strong> and <strong>induction
heads</strong>.</p>
<p>In the <strong>selective copying</strong> task, the goal of the SSM
is to copy parts of the input and output them in order:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3f1d1fb-7603-4f86-a2e4-88e0496a1f08_2120x464.png" style="zoom:50%;"></p>
<p>However, a (recurrent/convolutional) SSM performs poorly in this task
since it is <strong><em>Linear Time Invariant</em>.</strong> As we saw
before, the matrices <em>A</em>, <em>B</em>, and <em>C</em> are the same
for every token the SSM generates.</p>
<p>As a result, an SSM cannot perform <em>content-aware reasoning</em>
since it treats each token equally as a result of the fixed A, B, and C
matrices. This is a problem as we want the SSM to reason about the input
(prompt).</p>
<p>The second task an SSM performs poorly on is <strong>induction
heads</strong> where the goal is to reproduce patterns found in the
input:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27775742-18d2-4f2c-b792-5706976e75e3_2120x744.png" style="zoom:50%;"></p>
<p>In the above example, we are essentially performing one-shot
prompting where we attempt to “teach” the model to provide an
“<strong><em>A:</em></strong>” response after every
“<strong><em>Q:</em></strong>”. However, since SSMs are time-invariant
it cannot select which previous tokens to recall from its history.</p>
<p>Let’s illustrate this by focusing on <em>matrix B</em>. Regardless of
what the input <strong><em>x</em></strong> is, <em>matrix B</em> remains
exactly the same and is therefore independent of
<strong><em>x</em></strong>:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ee2bd7a-b99b-4871-8396-69cb7dd13cf5_1480x808.png" style="zoom:50%;"></p>
<p>Likewise, <em>A</em> and <em>C</em> also remain fixed regardless of
the input. This demonstrates the <em>static</em> nature of the SSMs we
have seen thus far.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fea51ca-8458-4216-bf1c-c880a23504b4_1412x484.png" style="zoom:50%;"></p>
<p>In comparison, these tasks are relatively easy for Transformers since
they <em>dynamically</em> change their attention based on the input
sequence. They can selectively “look” or “attend” at different parts of
the sequence.</p>
<p>The poor performance of SSMs on these tasks illustrates the
underlying problem with time-invariant SSMs, the static nature of
matrices <em>A</em>, <em>B</em>, and <em>C</em> results in problems with
<em>content-awareness</em>.</p>
<h3 id="selectively-retain-information">Selectively Retain
Information</h3>
<p>The recurrent representation of an SSM creates a small state that is
quite efficient as it compresses the entire history. However, compared
to a Transformer model which does no compression of the history (through
the attention matrix), it is much less powerful.</p>
<p>Mamba aims to have the best of both worlds. A small state that is as
powerful as the state of a Transformer:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84b8a71a-6310-416b-8622-e9166593171e_1540x464.png" style="zoom:50%;"></p>
<p>As teased above, it does so by compressing data selectively into the
state. When you have an input sentence, there is often information, like
stop words, that does not have much meaning.</p>
<p>To selectively compress information, we need the parameters to be
dependent on the input. To do so, let’s first explore the dimensions of
the input and output in an SSM during training:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9376222e-b232-4458-a72c-bd741d8a031c_1436x500.png" style="zoom:50%;"></p>
<p>In a Structured State Space Model (S4), the matrices <em>A</em>,
<em>B</em>, and <em>C</em> are independent of the input since their
dimensions <strong><em>N</em></strong> and <strong><em>D</em></strong>
are static and do not change.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e93b701-df74-4954-be5c-c0d83779d3df_1412x532.png" style="zoom:50%;"></p>
<p>Instead, Mamba makes matrices <em>B</em> and <em>C,</em> and even the
<em>step size</em> <strong>∆</strong><em>,</em> dependent on the input
by incorporating the sequence length and batch size of the input:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdccefffd-5712-45ab-9821-c794bce65d7d_1412x596.png" style="zoom:50%;"></p>
<p>This means that for every input token, we now have different
<em>B</em> and <em>C</em> matrices which solves the problem with
content-awareness!</p>
<blockquote>
<p><strong>NOTE</strong>: Matrix <em>A</em> remains the same since we
want the state itself to remain static but the way it is influenced
(through <em>B</em> and <em>C</em>) to be dynamic.</p>
</blockquote>
<p>Together, they <em>selectively</em> choose what to keep in the hidden
state and what to ignore since they are now dependent on the input.</p>
<p>A smaller <em>step size</em> <strong>∆</strong> results in ignoring
specific words and instead using the previous context more whilst a
larger <em>step size</em> <strong>∆</strong> focuses on the input words
more than the context:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06b21aab-aa32-450a-ae02-b976a2c9f9d8_2520x616.png" style="zoom:50%;"></p>
<h3 id="the-scan-operation">The Scan Operation</h3>
<p>Since these matrices are now <em>dynamic</em>, they cannot be
calculated using the convolution representation since it assumes a
<em>fixed</em> kernel. We can only use the recurrent representation and
lose the parallelization the convolution provides.</p>
<p>To enable parallelization, let’s explore how we compute the output
with recurrency:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed3ad33b-7b7f-4b69-b28e-8437c7b71e2e_1540x536.png" style="zoom:50%;"></p>
<p>Each state is the sum of the previous state (multiplied by
<em>A</em>) plus the current input (multiplied by <em>B</em>). This is
called a <em>scan operation</em> and can easily be calculated with a for
loop.</p>
<p>Parallelization, in contrast, seems impossible since each state can
only be calculated if we have the previous state. Mamba, however, makes
this possible through the <em><a target="_blank" rel="noopener" href="https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda">parallel
scan</a></em> algorithm.</p>
<p>It assumes the order in which we do operations does not matter
through the associate property. As a result, we can calculate the
sequences in parts and iteratively combine them:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F191fdabe-6b38-4e6b-a9f0-8240feef0a9d_1640x1100.png" style="zoom:50%;"></p>
<p>Together, dynamic matrices <em>B</em> and <em>C</em>, and the
parallel scan algorithm create the <strong><em>selective scan
algorithm</em></strong> to represent the dynamic and fast nature of
using the recurrent representation.</p>
<h3 id="hardware-aware-algorithm">Hardware-aware Algorithm</h3>
<p>A disadvantage of recent GPUs is their limited transfer (IO) speed
between their small but highly efficient SRAM and their large but
slightly less efficient DRAM. Frequently copying information between
SRAM and DRAM becomes a bottleneck.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a1d4fa3-526d-488e-8768-c7208f018eb4_1728x300.png" style="zoom:50%;"></p>
<p>Mamba, like Flash Attention, attempts to limit the number of times we
need to go from DRAM to SRAM and vice versa. It does so through
<em>kernel fusion</em> which allows the model to prevent writing
intermediate results and continuously performing computations until it
is done.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F563b3792-701a-42b1-9b68-7b6457f5e63d_1728x344.png" style="zoom:50%;"></p>
<p>We can view the specific instances of DRAM and SRAM allocation by
visualizing Mamba’s base architecture:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F724eceb1-4356-4ac5-b44e-f7fabce3b472_1728x580.png" style="zoom:50%;"></p>
<p>Here, the following are fused into one kernel:</p>
<ul>
<li><p>Discretization step with <em>step size</em>
<strong>∆</strong></p></li>
<li><p>Selective scan algorithm</p></li>
<li><p>Multiplication with <em>C</em></p></li>
</ul>
<p>The last piece of the hardware-aware algorithm is
<em>recomputation</em>.</p>
<p>The intermediate states are not saved but are necessary for the
backward pass to compute the gradients. Instead, the authors recompute
those intermediate states <em>during</em> the backward pass.</p>
<p>Although this might seem inefficient, it is much less costly than
reading all those intermediate states from the relatively slow DRAM.</p>
<p>We have now covered all components of its architecture which is
depicted using the following image from its article:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc840fb8-2e24-4103-95c8-afa306ce0cfc_2409x743.png" style="zoom:50%;"></p>
<p><strong>The Selective SSM.</strong> Retrieved from: Gu, Albert, and
Tri Dao. "Mamba: Linear-time sequence modeling with selective state
spaces." <em>arXiv preprint arXiv:2312.00752</em> (2023).</p>
<p>This architecture is often referred to as a <strong><em>selective
SSM</em></strong> or <strong><em>S6</em></strong> model since it is
essentially an S4 model computed with the selective scan algorithm.</p>
<h3 id="the-mamba-block">The Mamba Block</h3>
<p>The <em>selective SSM</em> that we have explored thus far can be
implemented as a block, the same way we can represent self-attention in
a decoder block.</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb76db77-1dba-42bd-8de4-4bce240ff67e_1776x1012.png" style="zoom:50%;"></p>
<p>Like the decoder, we can stack multiple Mamba blocks and use their
output as the input for the next Mamba block:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc94d349d-8620-45a9-8095-7c27de8b7865_1660x1356.png" style="zoom:50%;"></p>
<p>It starts with a linear projection to expand upon the input
embeddings. Then, a convolution before the <em>Selective SSM</em> is
applied to prevent independent token calculations.</p>
<p>The <em>Selective SSM</em> has the following properties:</p>
<ul>
<li><p><em>Recurrent SSM</em> created through
<em>discretization</em></p></li>
<li><p><em>HiPPO</em> initialization on matrix <em>A</em> to capture
<em>long-range dependencies</em></p></li>
<li><p>S<em>elective scan algorithm</em> to selectively compress
information</p></li>
<li><p><em>Hardware-aware algorithm</em> to speed up
computation</p></li>
</ul>
<p>We can expand on this architecture a bit more when looking at the
code implementation and explore how an end-to-end example would look
like:</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa67d7341-9a43-4c67-aa88-6e802c0902ae_1660x2040.png" style="zoom:50%;"></p>
<p>Notice some changes, like the inclusion of normalization layers and
softmax for choosing the output token.</p>
<p>When we put everything together, we get both fast inference and
training and even unbounded context!</p>
<p><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe528e5fa-0dd8-4e6c-b31a-5248aaee6c68_2072x912.png" style="zoom:50%;"></p>
<p>Using this architecture, the authors found it matches and sometimes
even exceeds the performance of Transformer models of the same size!</p>
<h2 id="conclusion"><strong>Conclusion</strong></h2>
<p>This concludes our journey in State Space Models and the incredible
Mamba architecture using a selective State Space Model. Hopefully, this
post gives you a better understanding of the potential of State Space
Models, particularly Mamba. Who knows if this is going to replace the
Transformers but for now, it is incredible to see such different
architectures getting well-deserved attention!</p>
<h2 id="resources">Resources</h2>
<p>Hopefully, this was an accessible introduction to Mamba and State
Space Models. If you want to go deeper, I would suggest the following
resources:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://srush.github.io/annotated-s4/">The Annotated S4</a>
is a JAX implementation and guide through the S4 model and is highly
advised!</li>
<li>A great <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ouF-H35atOY">YouTube video</a>
introducing Mamba by building it up through foundational papers.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/state-spaces/mamba">The Mamba
repository</a> with <a target="_blank" rel="noopener" href="https://huggingface.co/state-spaces">checkpoints on Hugging
Face</a>.</li>
<li>An amazing series of blog posts (<a target="_blank" rel="noopener" href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-1">1</a>, <a target="_blank" rel="noopener" href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-2">2</a>, <a target="_blank" rel="noopener" href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-3">3</a>)
that introduces the S4 model.</li>
<li>The <a target="_blank" rel="noopener" href="https://jameschen.io/jekyll/update/2024/02/12/mamba.html">Mamba
No. 5 (A Little Bit Of...)</a> blog post is a great next step to dive
into more technical details about Mamba but still from an amazingly
intuitive perspective.</li>
<li>And of course, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2312.00752">the Mamba
paper</a>! It was even used for DNA modeling and speech generation.</li>
</ul>
<hr>
<h2 id="references">References</h2>
<blockquote>
<p>[1]
原Blog链接：https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state</p>
<p>[2] 一文通透想颠覆Transformer的Mamba：从SSM、HiPPO、S4到Mamba
https://blog.csdn.net/v_JULY_v/article/details/134923301</p>
<p>[3]</p>
<p>[4] Gu, A., &amp; Dao, T. (2023). Mamba: Linear-time sequence
modeling with selective state spaces. <em>arXiv preprint
arXiv:2312.00752</em>.</p>
<p>[5]</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://junlei-zhou.com">坚竹韧周</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://junlei-zhou.com/2024/06/19/Mamba/">http://junlei-zhou.com/2024/06/19/Mamba/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a><a class="post-meta__tags" href="/tags/Sequence-Model/">Sequence Model</a><a class="post-meta__tags" href="/tags/State-space-model/">State space model</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq,email,copy_link"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/20/symbols/" title="symbols"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">symbols</div></div></a></div><div class="next-post pull-right"><a href="/2024/05/29/Support-Vector-Machine/" title="Support-Vector-Machine"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">Support-Vector-Machine</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/07/25/transformer-description/" title="transformer-description"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-25</div><div class="title">transformer-description</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">坚竹韧周</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/JunLei01"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/JunLei01" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:zjunlei1225@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#mamba"><span class="toc-number">1.</span> <span class="toc-text">Mamba</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#part-1-the-problem-with-transformers"><span class="toc-number">1.1.</span> <span class="toc-text">Part 1: The Problem with
Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-core-components-of-transformers"><span class="toc-number">1.1.1.</span> <span class="toc-text">The Core Components of
Transformers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-blessing-with-training"><span class="toc-number">1.1.2.</span> <span class="toc-text">A Blessing with Training…</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#and-the-curse-with-inference"><span class="toc-number">1.1.3.</span> <span class="toc-text">And the Curse with Inference!</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#are-rnns-a-solution"><span class="toc-number">1.1.4.</span> <span class="toc-text">Are RNNs a Solution?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#part-2-the-state-space-model-ssm"><span class="toc-number">1.2.</span> <span class="toc-text">Part 2: The State
Space Model (SSM)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#what-is-a-state-space"><span class="toc-number">1.2.1.</span> <span class="toc-text">What is a State Space?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#from-ssm-to-s4"><span class="toc-number">1.2.2.</span> <span class="toc-text">From SSM to S4</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#from-a-continuous-to-a-discrete-signal"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">From a Continuous to a
Discrete Signal</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-recurrent-representation"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">The Recurrent Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-convolution-representation"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">The Convolution
Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-three-representations"><span class="toc-number">1.2.2.4.</span> <span class="toc-text">The Three Representations</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-importance-of-matrix-a"><span class="toc-number">1.2.2.5.</span> <span class="toc-text">The Importance of Matrix
A</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#introduction-of-s4"><span class="toc-number">1.2.3.</span> <span class="toc-text">Introduction of S4</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#improving-transformer-struggles-with-handling-very-long-sequences"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">Improving
transformer struggles with handling very long sequences</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-definition-and-derivation-of-hippostate-compresses-the-history-of-input"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">The
definition and derivation of HiPPO：State compresses the history of
input</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#high-order-hippo-from-state-state-transition-to-state-result-transition"><span class="toc-number">1.2.3.3.</span> <span class="toc-text">High-order
HiPPO: from state-state transition to state-result transition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#the-definition-and-properties-of-s4"><span class="toc-number">1.2.3.4.</span> <span class="toc-text">The definition and
properties of S4</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#part-3-mamba---a-selective-ssm"><span class="toc-number">1.3.</span> <span class="toc-text">Part 3: Mamba - A Selective
SSM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#what-problem-does-it-attempt-to-solve"><span class="toc-number">1.3.1.</span> <span class="toc-text">What Problem does it
attempt to Solve?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#selectively-retain-information"><span class="toc-number">1.3.2.</span> <span class="toc-text">Selectively Retain
Information</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-scan-operation"><span class="toc-number">1.3.3.</span> <span class="toc-text">The Scan Operation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hardware-aware-algorithm"><span class="toc-number">1.3.4.</span> <span class="toc-text">Hardware-aware Algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-mamba-block"><span class="toc-number">1.3.5.</span> <span class="toc-text">The Mamba Block</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusion"><span class="toc-number">1.4.</span> <span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#resources"><span class="toc-number">1.5.</span> <span class="toc-text">Resources</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#references"><span class="toc-number">1.6.</span> <span class="toc-text">References</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/25/transformer-description/" title="transformer-description">transformer-description</a><time datetime="2024-07-25T06:22:27.000Z" title="Created 2024-07-25 14:22:27">2024-07-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/20/symbols/" title="symbols">symbols</a><time datetime="2024-06-20T06:04:02.000Z" title="Created 2024-06-20 14:04:02">2024-06-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/19/Mamba/" title="Mamba">Mamba</a><time datetime="2024-06-19T02:35:00.000Z" title="Created 2024-06-19 10:35:00">2024-06-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/05/29/Support-Vector-Machine/" title="Support-Vector-Machine">Support-Vector-Machine</a><time datetime="2024-05-29T07:56:16.000Z" title="Created 2024-05-29 15:56:16">2024-05-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/27/markov-process/" title="markov_process">markov_process</a><time datetime="2023-12-27T09:33:42.000Z" title="Created 2023-12-27 17:33:42">2023-12-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 坚竹韧周</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>