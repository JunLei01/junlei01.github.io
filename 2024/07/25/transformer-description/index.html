<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>transformer-description | Welcome to Junlei's word!</title><meta name="author" content="坚竹韧周"><meta name="copyright" content="坚竹韧周"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Transformer: Attention Is All You Need 网络结构 整体架构，Transformer由两部分构成，Encoder和Decoder。都包含了6个block   第一步：获取输入句子的每一个单词的表示向量 \(X\)，\(X\) 由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embeddin">
<meta property="og:type" content="article">
<meta property="og:title" content="transformer-description">
<meta property="og:url" content="http://junlei-zhou.com/2024/07/25/transformer-description/index.html">
<meta property="og:site_name" content="Welcome to Junlei&#39;s word!">
<meta property="og:description" content="Transformer: Attention Is All You Need 网络结构 整体架构，Transformer由两部分构成，Encoder和Decoder。都包含了6个block   第一步：获取输入句子的每一个单词的表示向量 \(X\)，\(X\) 由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embeddin">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://junlei-zhou.com/img/avatar.jpg">
<meta property="article:published_time" content="2024-07-25T06:22:27.000Z">
<meta property="article:modified_time" content="2024-07-25T08:12:10.395Z">
<meta property="article:author" content="坚竹韧周">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Sequence Model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://junlei-zhou.com/img/avatar.jpg"><link rel="shortcut icon" href="/img/URL_LOGO.png"><link rel="canonical" href="http://junlei-zhou.com/2024/07/25/transformer-description/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'transformer-description',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-07-25 16:12:10'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="Welcome to Junlei's word!" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-blog"></i><span> Library</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Live</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fa fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa fa-gamepad"></i><span> Game</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-paper-plane"></i><span> Comments</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Welcome to Junlei's word!"><span class="site-name">Welcome to Junlei's word!</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-blog"></i><span> Library</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Live</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/movies/"><i class="fa-fw fa fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fa fa-gamepad"></i><span> Game</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-paper-plane"></i><span> Comments</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">transformer-description</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-07-25T06:22:27.000Z" title="Created 2024-07-25 14:22:27">2024-07-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-07-25T08:12:10.395Z" title="Updated 2024-07-25 16:12:10">2024-07-25</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="transformer-description"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="transformer-attention-is-all-you-need">Transformer: Attention Is
All You Need</h1>
<h2 id="网络结构">网络结构</h2>
<p>整体架构，Transformer由两部分构成，Encoder和Decoder。都包含了6个<strong><em><span
style="color: #ff7700">block</span></em></strong></p>
<p><img src="https://imgur.la/images/2024/07/25/PPKV7RHB.png" alt="PPKV7RHB" border="0"></p>
<h2 id="section"></h2>
<h2
id="第一步获取输入句子的每一个单词的表示向量-xx-由单词的-embeddingembedding就是从原始数据提取出来的feature-和单词位置的-embedding-相加得到">第一步：获取输入句子的每一个单词的表示向量
<span class="math inline">\(X\)</span><strong><span
style="color: rgb(18, 18, 18)"><span
style="background-color: rgb(255, 255, 255)">，</span></span></strong><span
class="math inline">\(X\)</span> 由单词的
Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的
Embedding 相加得到。</h2>
<p><img src="https://imgur.la/images/2024/07/25/YR9I534X.png" alt="YR9I534X" border="0"></p>
<center>
<p>
Transformer的输入表示
</p>
</center>
<ul>
<li><p>词Embedding方法：<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1301.3781">Word2Vec</a> 这种方式在 2018
年之前比较主流，但是随着 BERT、GPT2.0
的出现，这种方式已经不算效果最好的方法了), <a
target="_blank" rel="noopener" href="https://aclanthology.org/D14-1162/">Glove</a>
等算法预训练得到，也可以在 Transformer 中训练得到。</p></li>
<li><p>位置Embedding方法：Transformer与RNN不同，Transformer是统观全局将所有单词的位置信息也进行编码作为输入，而RNN则是使用单词的顺序信息，通过使用前一步的输出作为当前步的输入来获取位置信息。</p>
<p>通常，位置 Embedding 用 <strong>PE</strong>
<strong>PE</strong> 的维度与单词 Embedding 是一样的。PE
可以通过训练得到，也可以使用某种公式计算得到。在 Transformer
中采用了后者，计算公式如下：</p></li>
</ul>
<p><span class="math display">\[
PE_{(pos,2i)}=sin(pos/10000^{2i/d}) \\
PE_{(pos,2i+1)}=cos(pos/10000^{2i/d})
\]</span></p>
<p>其中，pos 表示单词在句子中的位置，<span
class="math inline">\(d\)</span> 表示 <strong>PE</strong> 的维度 (与词
Embedding 一样)，<span class="math inline">\(2i\)</span>
表示偶数的维度，<span class="math inline">\(2i+1\)</span> 表示奇数维度
(即 <span class="math inline">\(2i≤d, 2i+1≤d\)</span>)。使用这种公式计算
<strong>PE</strong> 有以下的好处：</p>
<ul>
<li><p>使 PE
能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20
个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第
21 位的 Embedding。</p></li>
<li><p>可以让模型容易地计算出相对位置，对于固定长度的间距 <span
class="math inline">\(k\)</span>，<strong>PE(pos+k)</strong>可以用 <strong>PE(pos)</strong>
计算得到。因为 <span class="math inline">\(Sin(A+B) = Sin(A)Cos(B) +
Cos(A)Sin(B),\\ Cos(A+B) = Cos(A)Cos(B) -
Sin(A)Sin(B)\)</span>。</p></li>
</ul>
<p><span style="color: rgb(18, 18, 18)"><span
style="background-color: rgb(255, 255, 255)">将单词的词 Embedding 和位置
Embedding 相加，就可以得到单词的表示向量 </span></span><strong><span
style="color: rgb(18, 18, 18)"><span
style="background-color: rgb(255, 255, 255)">x</span></span></strong><span
style="color: rgb(18, 18, 18)"><span
style="background-color: rgb(255, 255, 255)">，</span></span><strong><span
style="color: rgb(18, 18, 18)"><span
style="background-color: rgb(255, 255, 255)">x </span></span></strong><span
style="color: rgb(18, 18, 18)"><span
style="background-color: rgb(255, 255, 255)">就是 Transformer
的输入。</span></span></p>
<h2
id="第二步将得到的单词表示向量矩阵-如上图所示每一行是一个单词的表示-x-传入-encoder-中经过-6-个-encoder-block-后可以得到句子所有单词的编码信息矩阵-c">第二步：将得到的单词表示向量矩阵
(如上图所示，每一行是一个单词的表示 x) 传入 Encoder 中，经过 6 个
Encoder block 后可以得到句子所有单词的编码信息矩阵 C</h2>
<p>如下图。单词向量矩阵用 <span class="math inline">\(X_{n×d}\)</span>
表示， <span class="math inline">\(n\)</span> 是句子中单词个数，<span
class="math inline">\(d\)</span> 是表示向量的维度 (论文中 <span
class="math inline">\(d=512\)</span>)。每一个 Encoder block
输出的矩阵维度与输入完全一致。</p>
<p><img src="https://imgur.la/images/2024/07/25/BWNSQFCB.png" alt="BWNSQFCB" border="0" style="zoom:50%;" ></p>
<center>
<p>
Transformer Encoder 编码句子信息
</p>
</center>
<h3 id="encoder-block">Encoder  block</h3>
<p>       
 <img src="https://imgur.la/images/2024/07/25/architecture.png" alt="architecture" border="0" style="zoom:50%;" ></p>
<p>左侧为 <strong>Encoder block</strong>，右侧为 <strong>Decoder
block</strong>。<strong>Multi-Head
Attention</strong>，是由多个 <strong>Self-Attention</strong>
组成的，可以看到 <strong>Encoder block</strong> 包含一个
<strong>Multi-Head Attention</strong>，而 <strong>Decoder block</strong>
包含两个 <strong>Multi-Head Attention</strong> (其中有一个用到
Masked)。<strong>Add &amp; Norm</strong> 层，Add 表示残差连接 (Residual
Connection) 用于防止网络退化，<strong>Norm</strong> 表示 Layer
Normalization，用于对每一层的激活值进行归一化。</p>
<h4 id="self-attention-结构"><strong>Self-Attention 结构</strong></h4>
<p><img src="https://imgur.la/images/2024/07/25/3FSEDRY4.png" alt="3FSEDRY4" border="0" style="zoom:50%;" ></p>
<h5
id="qquerykkeyvvalue"><strong>Q(Query)、K(Key)、V(Value)</strong></h5>
<p>计算自注意力的第一步是从编码器的每个输入向量（在本例中为每个单词的嵌入）创建三个向量。因此，对于每个单词，我们创建一个<strong>Q</strong>、<strong>K</strong>、<strong>V</strong>。
<strong>WQ</strong>、<strong>WK</strong>、<strong>WV</strong>
三个矩阵是在训练的时候创建的。</p>
<p><img src="https://imgur.la/images/2024/07/25/V97S38NR.png" alt="V97S38NR" border="0" style="zoom:50%;" ></p>
<h5
id="self-attention计算输出即计算self-attention的分数"><strong>Self-Attention计算输出(即计算self-Attention的分数）</strong></h5>
<p>假设我们正在计算这个例子中第一个词“我”的自我注意力。我们需要根据这个单词对输入句子的每个单词进行评分。分数决定了当我们在某个位置对单词进行编码时，对输入句子的其他部分的关注程度。</p>
<p><span class="math display">\[
Attention（Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}}V)
\]</span></p>
<p>公式中计算矩阵 <span class="math inline">\(Q\)</span> 和 <span
class="math inline">\(K\)</span>
每一行向量的内积，为了防止内积过大，因此除以 <span
class="math inline">\(d_k\)</span> 的平方根。<span
class="math inline">\(Q\)</span> 乘以 <span
class="math inline">\(K\)</span> 的转置后，得到的矩阵行列数都为 <span
class="math inline">\(n\)</span>，<span class="math inline">\(n\)</span>
为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为 <span
class="math inline">\(Q\)</span> 乘以<span
class="math inline">\(K^T\)</span>，1234 表示的是句子中的单词。</p>
<p><img src="https://imgur.la/images/2024/07/25/BY3PA8IB.png" alt="BY3PA8IB" border="0"></p>
<p>得到 <span class="math inline">\(QK^T\)</span>之后，使用 Softmax
计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax
是对矩阵的每一行进行 Softmax，即每一行的和都变为 1. softmax
分数决定了每个单词在此位置的表达量。显然，这个位置的单词将具有最高的softmax分数，但有时关注与当前单词相关的另一个单词是有用的。</p>
<p><img src="https://imgur.la/images/2024/07/25/YAAYRBU9.png" alt="YAAYRBU9" border="0"></p>
<p>得到 Softmax 矩阵之后可以和 <span class="math inline">\(V\)</span>
相乘，得到最终的输出 <span class="math inline">\(Z\)</span>.</p>
<p><img src="https://imgur.la/images/2024/07/25/IEIZUYE5.png" alt="IEIZUYE5" border="0"></p>
<p>Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention
系数，最终单词 1 的输出 <span
class="math inline">\(V_i\)</span> 等于所有单词 i 的值 <span
class="math inline">\(V_i\)</span> 根据 attention
系数的比例加在一起得到，如下图所示：</p>
<p><img src="https://imgur.la/images/2024/07/25/B6LPEG5N.png" alt="B6LPEG5N" border="0"></p>
<h4 id="multi-head-attention">Multi-Head Attention</h4>
<p>Multi-Head Attention 包含多个 Self-Attention 层，首先将输入 <span
class="math inline">\(X\)</span> 分别传递到 <span
class="math inline">\(h\)</span> 个不同的 Self-Attention 中，计算得到
<span class="math inline">\(h\)</span> 个输出矩阵 <span
class="math inline">\(Z\)</span>。下图是 <span
class="math inline">\(h=8\)</span> 时候的情况，此时会得到 8 个输出矩阵
<span class="math inline">\(Z\)</span>.</p>
<p><img src="https://imgur.la/images/2024/07/25/ILJMUQNZ.png" alt="ILJMUQNZ" border="0" style="zoom:50%;" ></p>
<p>得到 8 个输出矩阵 <span class="math inline">\(Z1\)</span> 到 <span
class="math inline">\(Z8\)</span> 之后，Multi-Head Attention
将它们拼接在一起 (Concat)，然后传入一个<strong>Linear</strong> 层，得到
Multi-Head Attention 最终的输出 <span
class="math inline">\(Z\)</span>。Multi-Head Attention 输出的矩阵 <span
class="math inline">\(Z\)</span> 与其输入的矩阵 <span
class="math inline">\(X\)</span> 的维度是一样的。</p>
<p><img src="https://imgur.la/images/2024/07/25/T7JWQ9UY.png" alt="T7JWQ9UY" border="0"></p>
<h5 id="add-norm">Add &amp; Norm</h5>
<p>Add &amp; Norm 层由 Add 和 Norm 两部分组成,</p>
<p><span class="math display">\[
LayerNorm(X+MultiHeadAttention(X)) \\
LayerNorm(X+FeedForward(X))
\]</span></p>
<p>其中 <span class="math inline">\(X\)</span> 表示 Multi-Head Attention
或者 Feed Forward 的输入，MultiHeadAttention(<span
class="math inline">\(X\)</span>) 和 FeedForward(<span
class="math inline">\(X\)</span>) 表示输出 (输出与输入 <span
class="math inline">\(X\)</span>
维度是一样的，所以可以相加)。<strong>Add</strong> 指 <span
class="math inline">\(X\)</span>+MultiHeadAttention(<span
class="math inline">\(X\)</span>)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分：</p>
<p><img src="https://imgur.la/images/2024/07/25/E5CVMKZS.png" alt="E5CVMKZS" border="0"></p>
<p><strong>Norm</strong> 指 Layer Normalization，通常用于 RNN
结构，Layer Normalization
会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。</p>
<h5 id="feed-forward">Feed forward</h5>
<p><strong>Feed Forward</strong>
是一个两层的全连接层，第一层的激活函数为
Relu，第二层不使用激活函数，对应的公式如下: <span
class="math display">\[
FFN(x)=max(0,xW_1+b_1)W_2+b_2
\]</span></p>
<p>通过 <strong>Multi-Head Attention, Add &amp; Norm, Feed
forward</strong> 三个部分可以构成一个<strong>Encoder Block,Encoder
Block</strong> 通过接收输入 <span
class="math inline">\(X_{(n×d)}\)</span>,并通过 <strong>Encoder
Block</strong> 得到输出 <span
class="math inline">\(O_{(n×d)}\)</span>，通过多个 <strong>Encoder
block</strong> 叠加就可以组成 <strong>Encoder</strong>。经过
<strong>Encoder</strong> 部分我们可以得到信息编码矩阵 <span
class="math inline">\(C\)</span>。</p>
<h2
id="第三步将-encoder-输出的编码信息矩阵-c传递到-decoder-中decoder-依次会根据当前翻译过的单词-1-i-翻译下一个单词-i1如下图所示在使用的过程中翻译到单词-i1-的时候需要通过-mask-操作遮盖住-i1-之后的单词"><strong><span
style="color: rgb(18, 18, 18)"><span
style="background-color: rgb(255, 255, 255)">第三步</span></span></strong><span
style="color: rgb(18, 18, 18)"><span
style="background-color: rgb(255, 255, 255)">：将 Encoder
输出的编码信息矩阵 </span></span><strong><span
style="color: rgb(18, 18, 18)"><span
style="background-color: rgb(255, 255, 255)">C</span></span></strong><span
style="color: rgb(18, 18, 18)"><span
style="background-color: rgb(255, 255, 255)">传递到 Decoder 中，Decoder
依次会根据当前翻译过的单词 1~ i 翻译下一个单词
i+1，如下图所示。在使用的过程中，翻译到单词 i+1
的时候需要通过 </span></span><strong><span
style="color: rgb(18, 18, 18)"><span
style="background-color: rgb(255, 255, 255)">Mask
</span></span></strong><span style="color: rgb(18, 18, 18)"><span
style="background-color: rgb(255, 255, 255)">操作遮盖住 i+1
之后的单词。</span></span></h2>
<h3 id="decoder-block">Decoder  block</h3>
<p><img src="https://imgur.la/images/2024/07/25/architecture.png" alt="architecture" border="0" style="zoom:50%;" ></p>
<center>
<p>
Decoder block 包含两个 Multi-Head Attention 层。
</p>
</center>
<p>第一个 Multi-Head Attention 层采用了 Masked
操作。(在Transformer模型中，使用Masked可以避免在预测当前位置是使用到后续序列中的信息。在预测当前位置时使用未来位置的信息会导致信息泄露和模型过拟合的原因是因为这会造成未来信息的“泄漏”。在自注意力机制中，每个位置都可以与其他位置进行关联，因此如果模型在预测当前位置时使用了未来位置的信息，那么模型实际上是“知道”了未来的信息，这就相当于信息泄漏。信息泄漏会导致模型在训练时过度依赖未来位置的信息，从而导致模型过拟合训练数据，失去了对真实数据的泛化能力。过拟合的模型在未见过的数据上表现不佳，因为它们过度依赖于训练数据中的特定模式和关联，而无法很好地推广到新的数据。因此，在Transformer中，为了避免信息泄漏和过拟合，需要使用masked来确保模型只能使用当前位置之前的信息，这样可以更好地捕捉序列中的依赖关系和上下文信息，同时保持模型的泛化能力)</p>
<h4 id="第一个multi-head-attentionmasked">第一个Multi-Head
Attention（Masked）：</h4>
<p>Decoder block 的第一个 Multi-Head Attention 采用了 Masked
操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第
i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1
个单词之后的信息。以 "我有一只猫" 翻译成 "I have a cat" 为例，在 Decoder
的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入
"<Begin>" 预测出第一个单词为 "I"，然后根据输入 "<Begin> I"
预测下一个单词 "have"。</p>
<p><img src="https://imgur.la/images/2024/07/25/3C7JYEUH.png" alt="3C7JYEUH" border="0"></p>
<p>Decoder 可以在训练的过程中使用 Teacher Forcing
并且并行化训练，即将正确的单词序列 (&lt;Begin&gt; I have a cat)
和对应输出 (I have a cat &lt;end&gt;) 传递到 Decoder。那么在预测第 i
个输出时，就要将第 i+1 之后的单词掩盖住，注意 Mask 操作是在
Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示
"&lt;Begin&gt; I have a cat &lt;end&gt;"。</p>
<p><strong>第一步：</strong>是 Decoder 的输入矩阵和
<strong>Mask</strong> 矩阵，输入矩阵包含 "<Begin> I have a cat" (0, 1,
2, 3, 4) 五个单词的表示向量，<strong>Mask</strong> 是一个 5×5 的矩阵。在
<strong>Mask</strong> 可以发现单词 0 只能使用单词 0 的信息，而单词 1
可以使用单词 0, 1 的信息，即只能使用之前的信息。</p>
<p><img src="https://imgur.la/images/2024/07/25/IYX8T5F3.png" alt="IYX8T5F3" border="0"></p>
<center>
<p>
输入矩阵与 Mask 矩阵
</p>
</center>
<p><strong>第二步：</strong>接下来的操作和之前的 Self-Attention
一样，通过输入矩阵 <span class="math inline">\(X\)</span> 计算得到 <span
class="math inline">\(Q\)</span>, <span
class="math inline">\(K\)</span>, <span class="math inline">\(V\)</span>
矩阵。然后计算 <span class="math inline">\(Q\)</span> 和 <span
class="math inline">\(K^T\)</span> 的乘积 <span
class="math inline">\(QK^T\)</span> 。</p>
<p><img src="https://imgur.la/images/2024/07/25/83YBXRLY.png" alt="83YBXRLY" border="0"></p>
<center>
<p>
Q乘以K的转置
</p>
</center>
<p><strong>第三步：</strong>在得到 <span
class="math inline">\(QK^T\)</span> 之后需要进行 Softmax，计算 attention
score，我们在 Softmax
之前需要使用<strong>Mask</strong>矩阵遮挡住每一个单词之后的信息，遮挡操作如下：</p>
<p><img src="https://imgur.la/images/2024/07/25/YU3MAKSS.png" alt="YU3MAKSS" border="0"></p>
<center>
<p>
Softmax 之前 Mask
</p>
</center>
<p>得到 <strong>Mask</strong> <span class="math inline">\(QK^T\)</span>
之后在 <strong>Mask</strong> <span
class="math inline">\(QK^T\)</span>上进行 Softmax，每一行的和都为
1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p>
<p><strong>第四步：</strong>使用 <strong>Mask</strong> <span
class="math inline">\(QK^T\)</span> 与矩阵 <span
class="math inline">\(V\)</span> 相乘，得到输出 <span
class="math inline">\(Z\)</span>，则单词 1 的输出向量 <span
class="math inline">\(Z_1\)</span> 是只包含单词 1 信息的。</p>
<p><img src="https://imgur.la/images/2024/07/25/2JJ24Y9A.png" alt="2JJ24Y9A" border="0"></p>
<center>
<p>
Mask 之后的输出
</p>
</center>
<p><strong>第五步：</strong>通过上述步骤就可以得到一个 Mask
Self-Attention 的输出矩阵 <strong>Zi</strong>，然后和 Encoder 类似，通过
Multi-Head Attention 拼接多个输出<strong>Zi</strong> 然后计算得到第一个
Multi-Head Attention
的输出<strong>Z</strong>，<strong>Z</strong>与输入<strong>X</strong>维度一样。</p>
<h4 id="第二个-multi-head-attention">第二个 Multi-Head Attention</h4>
<p>第二个 Multi-Head Attention 层的<strong>K, V</strong>矩阵使用 Encoder
的<strong>编码信息矩阵C</strong>进行计算，而<strong>Q</strong>使用上一个
Decoder block 的输出计算。根据 Encoder 的输出 <strong>C</strong>计算得到
<strong>K, V</strong>，根据上一个 Decoder block 的输出
<strong>Z</strong> 计算 <strong>Q</strong> (如果是第一个 Decoder block
则使用输入矩阵 <strong>X</strong>
进行计算)，后续的计算方法与之前描述的一致。这样做的好处是在 Decoder
的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需
<strong>Mask</strong>)。</p>
<p><img src="https://imgur.la/images/2024/07/25/8ARNRACC.png" alt="8ARNRACC" border="0"></p>
<h4 id="最后的-softmax-层">最后的 Softmax 层</h4>
<p>计算下一个翻译单词的概率。</p>
<p>Decoder block 最后的部分是利用 Softmax
预测下一个单词，在之前的网络层我们得到一个最终的输出
<strong>Z</strong>，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0
的信息，如下：</p>
<p><img src="https://imgur.la/images/2024/07/25/H7ZPFVIX.png" alt="H7ZPFVIX" border="0"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://junlei-zhou.com">坚竹韧周</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://junlei-zhou.com/2024/07/25/transformer-description/">http://junlei-zhou.com/2024/07/25/transformer-description/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a><a class="post-meta__tags" href="/tags/Sequence-Model/">Sequence Model</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq,email,copy_link"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/06/20/symbols/" title="symbols"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">symbols</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/06/19/Mamba/" title="Mamba"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-19</div><div class="title">Mamba</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">坚竹韧周</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/JunLei01"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/JunLei01" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:zjunlei1225@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer-attention-is-all-you-need"><span class="toc-number">1.</span> <span class="toc-text">Transformer: Attention Is
All You Need</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">1.1.</span> <span class="toc-text">网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#section"><span class="toc-number">1.2.</span> <span class="toc-text"></span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%E8%8E%B7%E5%8F%96%E8%BE%93%E5%85%A5%E5%8F%A5%E5%AD%90%E7%9A%84%E6%AF%8F%E4%B8%80%E4%B8%AA%E5%8D%95%E8%AF%8D%E7%9A%84%E8%A1%A8%E7%A4%BA%E5%90%91%E9%87%8F-xx-%E7%94%B1%E5%8D%95%E8%AF%8D%E7%9A%84-embeddingembedding%E5%B0%B1%E6%98%AF%E4%BB%8E%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%E5%87%BA%E6%9D%A5%E7%9A%84feature-%E5%92%8C%E5%8D%95%E8%AF%8D%E4%BD%8D%E7%BD%AE%E7%9A%84-embedding-%E7%9B%B8%E5%8A%A0%E5%BE%97%E5%88%B0"><span class="toc-number">1.3.</span> <span class="toc-text">第一步：获取输入句子的每一个单词的表示向量
\(X\)，\(X\) 由单词的
Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的
Embedding 相加得到。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%E5%B0%86%E5%BE%97%E5%88%B0%E7%9A%84%E5%8D%95%E8%AF%8D%E8%A1%A8%E7%A4%BA%E5%90%91%E9%87%8F%E7%9F%A9%E9%98%B5-%E5%A6%82%E4%B8%8A%E5%9B%BE%E6%89%80%E7%A4%BA%E6%AF%8F%E4%B8%80%E8%A1%8C%E6%98%AF%E4%B8%80%E4%B8%AA%E5%8D%95%E8%AF%8D%E7%9A%84%E8%A1%A8%E7%A4%BA-x-%E4%BC%A0%E5%85%A5-encoder-%E4%B8%AD%E7%BB%8F%E8%BF%87-6-%E4%B8%AA-encoder-block-%E5%90%8E%E5%8F%AF%E4%BB%A5%E5%BE%97%E5%88%B0%E5%8F%A5%E5%AD%90%E6%89%80%E6%9C%89%E5%8D%95%E8%AF%8D%E7%9A%84%E7%BC%96%E7%A0%81%E4%BF%A1%E6%81%AF%E7%9F%A9%E9%98%B5-c"><span class="toc-number">1.4.</span> <span class="toc-text">第二步：将得到的单词表示向量矩阵
(如上图所示，每一行是一个单词的表示 x) 传入 Encoder 中，经过 6 个
Encoder block 后可以得到句子所有单词的编码信息矩阵 C</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder-block"><span class="toc-number">1.4.1.</span> <span class="toc-text">Encoder  block</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#self-attention-%E7%BB%93%E6%9E%84"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">Self-Attention 结构</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#qquerykkeyvvalue"><span class="toc-number">1.4.1.1.1.</span> <span class="toc-text">Q(Query)、K(Key)、V(Value)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#self-attention%E8%AE%A1%E7%AE%97%E8%BE%93%E5%87%BA%E5%8D%B3%E8%AE%A1%E7%AE%97self-attention%E7%9A%84%E5%88%86%E6%95%B0"><span class="toc-number">1.4.1.1.2.</span> <span class="toc-text">Self-Attention计算输出(即计算self-Attention的分数）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multi-head-attention"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">Multi-Head Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#add-norm"><span class="toc-number">1.4.1.2.1.</span> <span class="toc-text">Add &amp; Norm</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#feed-forward"><span class="toc-number">1.4.1.2.2.</span> <span class="toc-text">Feed forward</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%E5%B0%86-encoder-%E8%BE%93%E5%87%BA%E7%9A%84%E7%BC%96%E7%A0%81%E4%BF%A1%E6%81%AF%E7%9F%A9%E9%98%B5-c%E4%BC%A0%E9%80%92%E5%88%B0-decoder-%E4%B8%ADdecoder-%E4%BE%9D%E6%AC%A1%E4%BC%9A%E6%A0%B9%E6%8D%AE%E5%BD%93%E5%89%8D%E7%BF%BB%E8%AF%91%E8%BF%87%E7%9A%84%E5%8D%95%E8%AF%8D-1-i-%E7%BF%BB%E8%AF%91%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%8D%95%E8%AF%8D-i1%E5%A6%82%E4%B8%8B%E5%9B%BE%E6%89%80%E7%A4%BA%E5%9C%A8%E4%BD%BF%E7%94%A8%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%BF%BB%E8%AF%91%E5%88%B0%E5%8D%95%E8%AF%8D-i1-%E7%9A%84%E6%97%B6%E5%80%99%E9%9C%80%E8%A6%81%E9%80%9A%E8%BF%87-mask-%E6%93%8D%E4%BD%9C%E9%81%AE%E7%9B%96%E4%BD%8F-i1-%E4%B9%8B%E5%90%8E%E7%9A%84%E5%8D%95%E8%AF%8D"><span class="toc-number">1.5.</span> <span class="toc-text">第三步：将 Encoder
输出的编码信息矩阵 C传递到 Decoder 中，Decoder
依次会根据当前翻译过的单词 1~ i 翻译下一个单词
i+1，如下图所示。在使用的过程中，翻译到单词 i+1
的时候需要通过 Mask
操作遮盖住 i+1
之后的单词。</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#decoder-block"><span class="toc-number">1.5.1.</span> <span class="toc-text">Decoder  block</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E4%B8%AAmulti-head-attentionmasked"><span class="toc-number">1.5.1.1.</span> <span class="toc-text">第一个Multi-Head
Attention（Masked）：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E4%B8%AA-multi-head-attention"><span class="toc-number">1.5.1.2.</span> <span class="toc-text">第二个 Multi-Head Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E5%90%8E%E7%9A%84-softmax-%E5%B1%82"><span class="toc-number">1.5.1.3.</span> <span class="toc-text">最后的 Softmax 层</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/25/transformer-description/" title="transformer-description">transformer-description</a><time datetime="2024-07-25T06:22:27.000Z" title="Created 2024-07-25 14:22:27">2024-07-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/20/symbols/" title="symbols">symbols</a><time datetime="2024-06-20T06:04:02.000Z" title="Created 2024-06-20 14:04:02">2024-06-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/19/Mamba/" title="Mamba">Mamba</a><time datetime="2024-06-19T02:35:00.000Z" title="Created 2024-06-19 10:35:00">2024-06-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/05/29/Support-Vector-Machine/" title="Support-Vector-Machine">Support-Vector-Machine</a><time datetime="2024-05-29T07:56:16.000Z" title="Created 2024-05-29 15:56:16">2024-05-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/27/markov-process/" title="markov_process">markov_process</a><time datetime="2023-12-27T09:33:42.000Z" title="Created 2023-12-27 17:33:42">2023-12-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 坚竹韧周</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>